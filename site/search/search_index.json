{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_helm/","text":"Restore Digital Asset Management image to previous version This section shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. Restore DAM image using Helm This procedure restores your DAM deployment to a previous version, in case the database migration fails repeatedly. See Update deployment to a later version for more information on updating your Helm deployment. For the latest HCL DX 9.5 images, see Docker image list . To restore, get the custom-values.yaml file and modify the digitalAssetManagement property value and replace it with the previous image version tag. In this example, digitalAssetManagement is updated with the CF196 DAM image version tag: # Ensure to use the correct image version tags digitalAssetManagement: \"95_CF196_20210625-2013\" Parent topic: Update deployment to a later version","title":"Restore Digital Asset Management image to previous version"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_helm/#restore-digital-asset-management-image-to-previous-version","text":"This section shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version.","title":"Restore Digital Asset Management image to previous version"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_helm/#restore-dam-image-using-helm","text":"This procedure restores your DAM deployment to a previous version, in case the database migration fails repeatedly. See Update deployment to a later version for more information on updating your Helm deployment. For the latest HCL DX 9.5 images, see Docker image list . To restore, get the custom-values.yaml file and modify the digitalAssetManagement property value and replace it with the previous image version tag. In this example, digitalAssetManagement is updated with the CF196 DAM image version tag: # Ensure to use the correct image version tags digitalAssetManagement: \"95_CF196_20210625-2013\" Parent topic: Update deployment to a later version","title":"Restore DAM image using Helm"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_operator/","text":"Restore Digital Asset Management image to previous version This shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. You can restore your DAM deployment to a previous version, in case the database migration fails repeatedly. See Updating a deployment for more information on updating your deployment using dxctl. For the latest HCL DX 9.5 images, see Docker image list . To restore, get the properties file and modify the dam.tag property value under ## DAM configuration and replace it with the previous image version tag. In this example, dam.tag is updated with the CF196 DAM image version tag: ``` Ensure to use the correct image version tags dam.tag: \"95_CF196_20210625-2013\" ``` Parent topic: Backup and recovery procedures Containerization","title":"Restore Digital Asset Management image to previous version"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_operator/#restore-digital-asset-management-image-to-previous-version","text":"This shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. You can restore your DAM deployment to a previous version, in case the database migration fails repeatedly. See Updating a deployment for more information on updating your deployment using dxctl. For the latest HCL DX 9.5 images, see Docker image list . To restore, get the properties file and modify the dam.tag property value under ## DAM configuration and replace it with the previous image version tag. In this example, dam.tag is updated with the CF196 DAM image version tag: ```","title":"Restore Digital Asset Management image to previous version"},{"location":"allnewfeatures/digital_asset_mgmt/dam_restore_image_operator/#ensure-to-use-the-correct-image-version-tags","text":"dam.tag: \"95_CF196_20210625-2013\" ``` Parent topic: Backup and recovery procedures Containerization","title":"Ensure to use the correct image version tags"},{"location":"allnewfeatures/digital_asset_mgmt/digital_asset_mgmt_overview/","text":"HCL Digital Asset Management HCL Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the HCL Digital Asset Management features in HCL Digital Experience 9.5 Container Update Release CF181 and higher releases. Follow the instructions below to install, configure, and use the HCL Digital Experience 9.5 Digital Asset Management features. Notes: The following limitations exist in HCL Digital Asset Management: Content created using Content Composer and Digital Asset Management cannot be used with the HCL Digital Experience Projects capabilities. Content Composer and Digital Asset Management may not be used with HCL DX 9.5 container deployments that set an alternate context root. Renditions for graphics interchange format (GIF) images are not yet supported. The use of Content Composer features in a Virtual Portal deployment pattern is not yet supported. When planning to use the Content Composer features in an authoring environment, with the Container Update CF181 release, it is only supported when running with one Digital Experience 9.5 core container (pod). This limitation does not apply when using the Web Content Management Authoring Portlet or when just using the new Digital Asset Management feature. It also does not apply to environments used for rendering. When using Kaltura with Digital Asset Management, all uploaded videos are anonymously accessible for everyone who knows the URL, regardless of the access control setting on the collection containing the video. Do not use Kaltura integration if you have sensitive videos that should not be available without authentication. If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your CDN configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' .","title":"HCL Digital Asset Management"},{"location":"allnewfeatures/digital_asset_mgmt/digital_asset_mgmt_overview/#hcl-digital-asset-management","text":"HCL Digital Asset Management (DAM) delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the HCL Digital Asset Management features in HCL Digital Experience 9.5 Container Update Release CF181 and higher releases. Follow the instructions below to install, configure, and use the HCL Digital Experience 9.5 Digital Asset Management features. Notes: The following limitations exist in HCL Digital Asset Management: Content created using Content Composer and Digital Asset Management cannot be used with the HCL Digital Experience Projects capabilities. Content Composer and Digital Asset Management may not be used with HCL DX 9.5 container deployments that set an alternate context root. Renditions for graphics interchange format (GIF) images are not yet supported. The use of Content Composer features in a Virtual Portal deployment pattern is not yet supported. When planning to use the Content Composer features in an authoring environment, with the Container Update CF181 release, it is only supported when running with one Digital Experience 9.5 core container (pod). This limitation does not apply when using the Web Content Management Authoring Portlet or when just using the new Digital Asset Management feature. It also does not apply to environments used for rendering. When using Kaltura with Digital Asset Management, all uploaded videos are anonymously accessible for everyone who knows the URL, regardless of the access control setting on the collection containing the video. Do not use Kaltura integration if you have sensitive videos that should not be available without authentication. If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your CDN configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' .","title":"HCL Digital Asset Management"},{"location":"allnewfeatures/digital_asset_mgmt/helm_dam_backup_restore_image/","text":"Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed. Back up your database Verify that persistence (read-write) and DAM pods are up and running: kubectl -n <namespace> get all Example: kubectl -n dxns get all You may see more than one persistence pods running: pod/dx-deployment-persistence-node-0 2/2 Running 0 3h49m pod/dx-deployment-persistence-node-1 2/2 Running 0 3h48m pod/dx-deployment-persistence-node-2 2/2 Running 0 3h48m The number of these pods configured as stateful sets is listed at the end of the report. Take note of this number as you will need this for step 5. NAME READY AGE statefulset.apps/dx-deployment-core 1/1 4h1m statefulset.apps/dx-deployment-digital-asset-management 1/1 4h1m statefulset.apps/dx-deployment-open-ldap 1/1 4h1m statefulset.apps/dx-deployment-persistence-node 3/3 4h1m statefulset.apps/dx-deployment-remote-search 1/1 4h1m Scale down the number persistence pods to 1. kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=1 Example: kubectl scale statefulsets dx-deployment-persistence-node -d dxns --replicas=1 Note: Verify that only 1 persistence pod remains in the deployment. Connect with the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Dump the current database: pg_dump dxmediadb > /tmp/dxmediadb.dmp Close the shell in the persistence pod: exit Download the database dump to the local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Set the number of persistence pods back to the number you noted from step 1: kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=<new-replicas> Example: kubectl scale statefulsets dx-deployment-persistence-node -n dxns --replicas=3 Back up your DAM binary Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located under /opt/app/upload directory: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Close the shell in the DAM pod: exit Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your DAM binary Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-dam-0:/tmp/backupml.tar.gz Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Close the shell in the DAM pod: exit Restore your database Verify that persistence (read-write) and DAM pods are running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Scale down the number persistence pods to 1. kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=1 Example: kubectl scale statefulsets dx-deployment-persistence-node -d dxns --replicas=1 Note: Verify that only 1 persistence pod remains in the deployment. Copy the database dump file to the persistence pod: kubectl cp <target-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp dxmediadb.dmp dxns/dx-deployment-persistence:/tmp/dxmediadb.dmp Connect to the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl -n dxns exec --stdin --tty pod/dx-deployment-persistence -d dxns -- /bin/bash Run the following commands in order: Set the database connection limit to 0 for dxmediadb : psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : dropdb dxmediadb Note: If you are getting the following error, run the two commands from this step again until it completes without the error occurring. shell dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database. createdb -O dxuser dxmediadb Restore the database. psql dxmediadb < dxmediadb.dmp Restore the database connection limit: psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Close the shell in the persistence pod (read-write): exit Set the number of persistence pods back to the number you noted from step 1 from Back up your database: kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=<new-replicas> Example: kubectl scale statefulsets dx-deployment-persistence-node -n dxns --replicas=3 Additional step to restore your database The DAM pods start working in a few minutes. If not, you may delete the DAM pod: kubectl delete pod <dam-pod-name> -n <namespace> Example: kubectl delete pod dx-deployment-dam-0 -n dxns Parent topic: Backup and recovery procedures for Helm Containerization","title":"Back up and restore a DAM image"},{"location":"allnewfeatures/digital_asset_mgmt/helm_dam_backup_restore_image/#back-up-and-restore-a-dam-image","text":"This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed. Back up your database Verify that persistence (read-write) and DAM pods are up and running: kubectl -n <namespace> get all Example: kubectl -n dxns get all You may see more than one persistence pods running: pod/dx-deployment-persistence-node-0 2/2 Running 0 3h49m pod/dx-deployment-persistence-node-1 2/2 Running 0 3h48m pod/dx-deployment-persistence-node-2 2/2 Running 0 3h48m The number of these pods configured as stateful sets is listed at the end of the report. Take note of this number as you will need this for step 5. NAME READY AGE statefulset.apps/dx-deployment-core 1/1 4h1m statefulset.apps/dx-deployment-digital-asset-management 1/1 4h1m statefulset.apps/dx-deployment-open-ldap 1/1 4h1m statefulset.apps/dx-deployment-persistence-node 3/3 4h1m statefulset.apps/dx-deployment-remote-search 1/1 4h1m Scale down the number persistence pods to 1. kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=1 Example: kubectl scale statefulsets dx-deployment-persistence-node -d dxns --replicas=1 Note: Verify that only 1 persistence pod remains in the deployment. Connect with the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Dump the current database: pg_dump dxmediadb > /tmp/dxmediadb.dmp Close the shell in the persistence pod: exit Download the database dump to the local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Set the number of persistence pods back to the number you noted from step 1: kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=<new-replicas> Example: kubectl scale statefulsets dx-deployment-persistence-node -n dxns --replicas=3 Back up your DAM binary Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located under /opt/app/upload directory: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Close the shell in the DAM pod: exit Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your DAM binary Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-dam-0:/tmp/backupml.tar.gz Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Close the shell in the DAM pod: exit Restore your database Verify that persistence (read-write) and DAM pods are running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Scale down the number persistence pods to 1. kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=1 Example: kubectl scale statefulsets dx-deployment-persistence-node -d dxns --replicas=1 Note: Verify that only 1 persistence pod remains in the deployment. Copy the database dump file to the persistence pod: kubectl cp <target-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp dxmediadb.dmp dxns/dx-deployment-persistence:/tmp/dxmediadb.dmp Connect to the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl -n dxns exec --stdin --tty pod/dx-deployment-persistence -d dxns -- /bin/bash Run the following commands in order: Set the database connection limit to 0 for dxmediadb : psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : dropdb dxmediadb Note: If you are getting the following error, run the two commands from this step again until it completes without the error occurring. shell dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database. createdb -O dxuser dxmediadb Restore the database. psql dxmediadb < dxmediadb.dmp Restore the database connection limit: psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Close the shell in the persistence pod (read-write): exit Set the number of persistence pods back to the number you noted from step 1 from Back up your database: kubectl scale statefulsets <stateful-set-name> -n <namespace> --replicas=<new-replicas> Example: kubectl scale statefulsets dx-deployment-persistence-node -n dxns --replicas=3 Additional step to restore your database The DAM pods start working in a few minutes. If not, you may delete the DAM pod: kubectl delete pod <dam-pod-name> -n <namespace> Example: kubectl delete pod dx-deployment-dam-0 -n dxns Parent topic: Backup and recovery procedures for Helm Containerization","title":"Back up and restore a DAM image"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/","text":"Manage collections Learn more about creating, configuring, and deleting media asset collections using Digital Asset Management. Prerequisite HCL Digital Asset Management should be installed and configured to HCL Digital Experience 9.5 Container Release Update CF181 and higher. See the Install the HCL Digital Experience 9.5 Container components topic for instructions. Browse and search media assets in a collection Follow these steps to work with Collections in HCL Digital Experience 9.5 Digital Asset Management. From the HCL Digital Experience 9.5 Digital Asset Management interface, click Collections . The HCL Digital Experience 9.5 Digital Asset Management interface allows you to see previews of media assets for a specified collection. For example, a curated set of media assets are grouped in a collection named Furniture Gallery as shown below. This Collections interface presents you with the following options: Create Collection - Create a new collection or a new nested collection (when used inside an existing collection) View information - opens the Information panel and shows Access Control Information - Allows you to update the Collection name and description. Access Control - Allows you to set who can access the media assets in the Collection. Delete Collection - Deletes the selected Collection. Select the Filter option to filter media asset results by Favorites , by Asset types , or Asset size . Favorites - Filters media assets added to your favorites. Asset types - Filters media assets by type. You can further refine search results by selecting a specific file type. Asset size - Filters media assets by size. Note: Asset size filter is available in HCL DX CF192 and later. On the top right, click the Grid or List icon ( Open view options ) to change how the media assets or collections are displayed: View as - Lists options to view media assets or collections. Grid - Shows media assets in an equal-sized thumbnail view. This is the default display view. List - Shows media assets in a standard list. Use this view to access a media asset when you can recognize the asset by its attributes, such as its file name. Sort by - Lists options to sort media assets or collections: Date Name Type Size Order - Lists options to sort order of media assets or collections depending on the selected Sort by option: Date Order by file name by Older to newer date Order by file name by Newer to older date - Name Order by file name from A-Z Order by file name Z-A - Type Ascending Descending - Size Smaller to larger Larger to smaller Modify information metadata of a media asset collection From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click the Information icon (small i). On the Information panel, edit the Collection name and Description as applicable. Click Save to save changes. Manage user access permissions of a media asset collection Video : Configuring user access permissions to Digital Asset Management assets From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click Information (small i). Click Access . Assign levels for other users to access media items stored within the collection by clicking Add user . When you click Add User , a pop-up displays options to add these user access levels for individuals and groups as defined by the user directory integrated to your Digital Experience platform. Select the appropriate member. In this example, click Administrator . Select the appropriate Access rights . For this example, click All authenticated DX users . Click Add users and groups to save changes. Once added, you can specify additional users for the selected access right. Select the access right to add a member or groups. For this example, click Add Administrator . Once done, click Add users and groups to save changes. Delete a collection From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click Delete . A pop-up message displays to confirm your action. Once a collection is deleted, you can no longer retrieve it, including the media assets, renditions, and versions you have used as your web content. Click Delete to proceed with deleting the collection. HCL Digital Experience Solution Feedback HCL Digital Experience is interested in your experience and feedback working with HCL Digital Experience 9.5 release software. To offer comments or issues on your findings, please access the HCL Digital Experience 9.5 Feedback Reporting application .","title":"Manage collections"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#manage-collections","text":"Learn more about creating, configuring, and deleting media asset collections using Digital Asset Management.","title":"Manage collections"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#prerequisite","text":"HCL Digital Asset Management should be installed and configured to HCL Digital Experience 9.5 Container Release Update CF181 and higher. See the Install the HCL Digital Experience 9.5 Container components topic for instructions.","title":"Prerequisite"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#browse-and-search-media-assets-in-a-collection","text":"Follow these steps to work with Collections in HCL Digital Experience 9.5 Digital Asset Management. From the HCL Digital Experience 9.5 Digital Asset Management interface, click Collections . The HCL Digital Experience 9.5 Digital Asset Management interface allows you to see previews of media assets for a specified collection. For example, a curated set of media assets are grouped in a collection named Furniture Gallery as shown below. This Collections interface presents you with the following options: Create Collection - Create a new collection or a new nested collection (when used inside an existing collection) View information - opens the Information panel and shows Access Control Information - Allows you to update the Collection name and description. Access Control - Allows you to set who can access the media assets in the Collection. Delete Collection - Deletes the selected Collection. Select the Filter option to filter media asset results by Favorites , by Asset types , or Asset size . Favorites - Filters media assets added to your favorites. Asset types - Filters media assets by type. You can further refine search results by selecting a specific file type. Asset size - Filters media assets by size. Note: Asset size filter is available in HCL DX CF192 and later. On the top right, click the Grid or List icon ( Open view options ) to change how the media assets or collections are displayed: View as - Lists options to view media assets or collections. Grid - Shows media assets in an equal-sized thumbnail view. This is the default display view. List - Shows media assets in a standard list. Use this view to access a media asset when you can recognize the asset by its attributes, such as its file name. Sort by - Lists options to sort media assets or collections: Date Name Type Size Order - Lists options to sort order of media assets or collections depending on the selected Sort by option: Date Order by file name by Older to newer date Order by file name by Newer to older date - Name Order by file name from A-Z Order by file name Z-A - Type Ascending Descending - Size Smaller to larger Larger to smaller","title":"Browse and search media assets in a collection"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#modify-information-metadata-of-a-media-asset-collection","text":"From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click the Information icon (small i). On the Information panel, edit the Collection name and Description as applicable. Click Save to save changes.","title":"Modify information metadata of a media asset collection"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#manage-user-access-permissions-of-a-media-asset-collection","text":"Video : Configuring user access permissions to Digital Asset Management assets From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click Information (small i). Click Access . Assign levels for other users to access media items stored within the collection by clicking Add user . When you click Add User , a pop-up displays options to add these user access levels for individuals and groups as defined by the user directory integrated to your Digital Experience platform. Select the appropriate member. In this example, click Administrator . Select the appropriate Access rights . For this example, click All authenticated DX users . Click Add users and groups to save changes. Once added, you can specify additional users for the selected access right. Select the access right to add a member or groups. For this example, click Add Administrator . Once done, click Add users and groups to save changes.","title":"Manage user access permissions of a media asset collection"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#delete-a-collection","text":"From the HCL Digital Experience 9.5 Digital Asset Management interface, hover over a collection and click Delete . A pop-up message displays to confirm your action. Once a collection is deleted, you can no longer retrieve it, including the media assets, renditions, and versions you have used as your web content. Click Delete to proceed with deleting the collection.","title":"Delete a collection"},{"location":"allnewfeatures/digital_asset_mgmt/manage_collections/#hcl-digital-experience-solution-feedback","text":"HCL Digital Experience is interested in your experience and feedback working with HCL Digital Experience 9.5 release software. To offer comments or issues on your findings, please access the HCL Digital Experience 9.5 Feedback Reporting application .","title":"HCL Digital Experience Solution Feedback"},{"location":"allnewfeatures/digital_asset_mgmt/operator_dam_backup_restore_image/","text":"Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in an Operator-based deployment using dxctl . This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed. Note: The steps in this section are supported for HCL DX 9.5 Container Update CF195 or later deployments. Please contact HCL Support if you need to perform backup/restore for container deployments at earlier levels. Back up your database Verify that persistence (read-write) and DAM pods are up and running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Disable the persistence read-only pods by setting the following properties in the deployment properties file: persist.minreplicas: 1 persist.force-read: false Then run the following command: dxctl --update -p deployment.properties Note: Verify that there are no read-only pods in the deployment. Connect with the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database: pg_dump dxmediadb > /tmp/dxmediadb.dmp Close the shell in the persistence pod: exit Download the database dump to the local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Back up your DAM binary Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located under /opt/app/upload directory: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Close the shell in the DAM pod: exit Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your DAM binary Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-dam-0:/tmp/backupml.tar.gz Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Close the shell in the DAM pod: exit Restore your database Verify that persistence (read-write) and DAM pods are running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Copy the database dump file to the persistence pod: kubectl cp <target-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp dxmediadb.dmp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp Connect to the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Run the following commands in order: Set the database connection limit to 0 for dxmediadb : psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : dropdb dxmediadb Note: If you are getting the following error, run the two commands from this step again until it completes without the error occurring. dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database. createdb -O dxuser dxmediadb Restore the database. psql dxmediadb < dxmediadb.dmp Restore the database connection limit: psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Close the shell in the persistence pod (read-write): exit Additional steps to restore your database The DAM pods start working in a few minutes. If not, you may delete the DAM pod: kubectl delete pod <dam-pod-name> -n <namespace> Example: kubectl delete pod dx-deployment-dam-0 -n dxns You must reset the read-only pod. Do the following steps: Get all the PersistentVolumeClaim ( PVC ) from the deployment. kubectl get pvc -n <name space> Example: kubectl get pvc -n dxns Delete the PVC for the read-only pods. kubectl delete pvc <pvc-name-for-read-only-pod> -n <namespace> Example: kubectl delete pvc dam-persistence-dx-deployment-persistence-ro-0 -n dxns Enable the read-only pods. To enable the read-only pods, set persist.force-read to true in deployment properties file. You may also update the persist.minreplicas: with the value you configured earlier. persist.minreplicas: 1 persist.force-read: true Then run the following command: dxctl --update -p deployment.properties Parent topic: Backup and recovery procedures Containerization","title":"Back up and restore a DAM image"},{"location":"allnewfeatures/digital_asset_mgmt/operator_dam_backup_restore_image/#back-up-and-restore-a-dam-image","text":"This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in an Operator-based deployment using dxctl . This procedure is not meant for moving DAM data to another deployment. The backup data is valid only on the deployment where the backup is performed. Note: The steps in this section are supported for HCL DX 9.5 Container Update CF195 or later deployments. Please contact HCL Support if you need to perform backup/restore for container deployments at earlier levels. Back up your database Verify that persistence (read-write) and DAM pods are up and running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Disable the persistence read-only pods by setting the following properties in the deployment properties file: persist.minreplicas: 1 persist.force-read: false Then run the following command: dxctl --update -p deployment.properties Note: Verify that there are no read-only pods in the deployment. Connect with the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database: pg_dump dxmediadb > /tmp/dxmediadb.dmp Close the shell in the persistence pod: exit Download the database dump to the local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Back up your DAM binary Connect to the DAM pod. Open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located under /opt/app/upload directory: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Close the shell in the DAM pod: exit Download the compressed binaries to the local system. From a local system, you can now download the backup DAM binaries from the DAM pod: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your DAM binary Upload the backup binary to the DAM pod. You can now transfer the backup database to the remote DAM pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/backupml.tar.gz dxns/dx-deployment-dam-0:/tmp/backupml.tar.gz Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Close the shell in the DAM pod: exit Restore your database Verify that persistence (read-write) and DAM pods are running: kubectl -n <namespace> get all Example: kubectl -n dxns get all Copy the database dump file to the persistence pod: kubectl cp <target-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp dxmediadb.dmp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp Connect to the persistence pod (read-write). Open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Run the following commands in order: Set the database connection limit to 0 for dxmediadb : psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 0;\" Terminate all the existing connections to the database, if any: psql -c \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'dxmediadb' AND pid <> pg_backend_pid();\" Drop the database dxmediadb : dropdb dxmediadb Note: If you are getting the following error, run the two commands from this step again until it completes without the error occurring. dropdb: database removal failed: ERROR: database \"dxmediadb\" is being accessed by other users Create the database. createdb -O dxuser dxmediadb Restore the database. psql dxmediadb < dxmediadb.dmp Restore the database connection limit: psql -c \"ALTER DATABASE dxmediadb CONNECTION LIMIT 500;\" Close the shell in the persistence pod (read-write): exit Additional steps to restore your database The DAM pods start working in a few minutes. If not, you may delete the DAM pod: kubectl delete pod <dam-pod-name> -n <namespace> Example: kubectl delete pod dx-deployment-dam-0 -n dxns You must reset the read-only pod. Do the following steps: Get all the PersistentVolumeClaim ( PVC ) from the deployment. kubectl get pvc -n <name space> Example: kubectl get pvc -n dxns Delete the PVC for the read-only pods. kubectl delete pvc <pvc-name-for-read-only-pod> -n <namespace> Example: kubectl delete pvc dam-persistence-dx-deployment-persistence-ro-0 -n dxns Enable the read-only pods. To enable the read-only pods, set persist.force-read to true in deployment properties file. You may also update the persist.minreplicas: with the value you configured earlier. persist.minreplicas: 1 persist.force-read: true Then run the following command: dxctl --update -p deployment.properties Parent topic: Backup and recovery procedures Containerization","title":"Back up and restore a DAM image"},{"location":"allnewfeatures/practitioner_studio/enable_prac_studio/","text":"How to enable Practitioner Studio This section outlines how to enable Practitioner Studio and Woodburn Studio in both base portal and virtual portal environments. Enabling Practitioner Studio and Woodburn Studio in base portal Note: It is not necessary to stop or restart Portal when running these configuration tasks. Please note that in order for you to deploy Practitioner Studio and Woodburn Studio to a virtual portal, you must first enable the 95 UI features in base Portal. Open a command line. Change to the wp_profile-root/ConfigEngine directory. Run the enable-v95-UI-features config task. AIX: ./ConfigEngine.sh **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Linux: ./ConfigEngine.sh **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Windows: ConfigEngine.bat **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Note: Any actions taken when using the Preview As User feature in DAM and CC shows the site as the original user, not the one that you are using to preview. Enabling Practitioner Studio and Woodburn Studio in an existing virtual portal Open a command line. Change to the wp_profile-root/ConfigEngine directory. Run the enable-v95-UI-features-virtual-portal config task. AIX: ./ConfigEngine.sh **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Linux: ./ConfigEngine.sh **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Windows: ConfigEngine.bat **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Notes: To configure the virtual Portal Manager so that Practitioner Studio is deployed for the newly created virtual portals, see How to configure Practitioner Studio . To enable Practitioner Studio and Woodburn Studio on all available Virtual Portals, you can use the following parameter: -DUpdateVPs=true If -DVirtualPortalContext= has a space in the context name, please add quotes around the name of the context. Enabling HCL DX site to be seen in SiteMap Enabling Practitioner Studio via the enable task has the following effect to your previous view: The Administration pages will be disabled, the Home pages will be excluded from the SiteMap, and first-level navigation drop downs in the toolbar and Practitioner Studio. Follow the steps to enable your site to be seen in SiteMap: Navigate to Administration > Managed Pages . Click Edit Page Properties for your home page. Expand Advanced Options and then click I want to set parameters . Create a new parameter com.ibm.portal.Hidden with value false. Save, then test.","title":"How to enable Practitioner Studio"},{"location":"allnewfeatures/practitioner_studio/enable_prac_studio/#how-to-enable-practitioner-studio","text":"This section outlines how to enable Practitioner Studio and Woodburn Studio in both base portal and virtual portal environments.","title":"How to enable Practitioner Studio"},{"location":"allnewfeatures/practitioner_studio/enable_prac_studio/#enabling-practitioner-studio-and-woodburn-studio-in-base-portal","text":"Note: It is not necessary to stop or restart Portal when running these configuration tasks. Please note that in order for you to deploy Practitioner Studio and Woodburn Studio to a virtual portal, you must first enable the 95 UI features in base Portal. Open a command line. Change to the wp_profile-root/ConfigEngine directory. Run the enable-v95-UI-features config task. AIX: ./ConfigEngine.sh **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Linux: ./ConfigEngine.sh **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Windows: ConfigEngine.bat **enable-v95-UI-features** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> Note: Any actions taken when using the Preview As User feature in DAM and CC shows the site as the original user, not the one that you are using to preview.","title":"Enabling Practitioner Studio and Woodburn Studio in base portal"},{"location":"allnewfeatures/practitioner_studio/enable_prac_studio/#enabling-practitioner-studio-and-woodburn-studio-in-an-existing-virtual-portal","text":"Open a command line. Change to the wp_profile-root/ConfigEngine directory. Run the enable-v95-UI-features-virtual-portal config task. AIX: ./ConfigEngine.sh **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Linux: ./ConfigEngine.sh **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Windows: ConfigEngine.bat **enable-v95-UI-features-virtual-portal** -DWasPassword=<WAS admin password> -DPortalAdminPwd=<Portal admin password> **-DVirtualPortalContext=** Notes: To configure the virtual Portal Manager so that Practitioner Studio is deployed for the newly created virtual portals, see How to configure Practitioner Studio . To enable Practitioner Studio and Woodburn Studio on all available Virtual Portals, you can use the following parameter: -DUpdateVPs=true If -DVirtualPortalContext= has a space in the context name, please add quotes around the name of the context.","title":"Enabling Practitioner Studio and Woodburn Studio in an existing virtual portal"},{"location":"allnewfeatures/practitioner_studio/enable_prac_studio/#enabling-hcl-dx-site-to-be-seen-in-sitemap","text":"Enabling Practitioner Studio via the enable task has the following effect to your previous view: The Administration pages will be disabled, the Home pages will be excluded from the SiteMap, and first-level navigation drop downs in the toolbar and Practitioner Studio. Follow the steps to enable your site to be seen in SiteMap: Navigate to Administration > Managed Pages . Click Edit Page Properties for your home page. Expand Advanced Options and then click I want to set parameters . Create a new parameter com.ibm.portal.Hidden with value false. Save, then test.","title":"Enabling HCL DX site to be seen in SiteMap"},{"location":"allnewfeatures/practitioner_studio/practitionerstudio_overview/","text":"Practitioner Studio Practitioner Studio is a newly designed user experience for HCL Digital Experience. Please see the following pages to understand how the new navigation is organized. Note: With HCL Digital Experience 9.5 Container Update CF192 and later, the following issues are experienced with use of the Safari web browser (Version 13.0.5 (15608.5.11). These issues, which were found in Web Content from the Practitioner Studio side navigator, will be resolved in a later software update: List navigation is not accessible using keyboard Incorrect focus behavior in Open Search and Close sidebar buttons Remnant of highlight outline of Search icon remains after back-tabbing to Back link Overview Practitioner Studio is a newly designed user experience for HCL Digital Experience. There are several advantages to this new look. The consolidated navigation has a consistent look and feel for most administration tasks so that commonly used tasks are found together. User assistance is easily found and is provided for pages that are in the navigation. In time, this interface also serves as the launching point for newly developed features. Enabling Practitioner Studio deactivates the legacy Administration and Applications pages. However, all of these functions are still available.","title":"Practitioner Studio"},{"location":"allnewfeatures/practitioner_studio/practitionerstudio_overview/#practitioner-studio","text":"Practitioner Studio is a newly designed user experience for HCL Digital Experience. Please see the following pages to understand how the new navigation is organized. Note: With HCL Digital Experience 9.5 Container Update CF192 and later, the following issues are experienced with use of the Safari web browser (Version 13.0.5 (15608.5.11). These issues, which were found in Web Content from the Practitioner Studio side navigator, will be resolved in a later software update: List navigation is not accessible using keyboard Incorrect focus behavior in Open Search and Close sidebar buttons Remnant of highlight outline of Search icon remains after back-tabbing to Back link","title":"Practitioner Studio"},{"location":"allnewfeatures/practitioner_studio/practitionerstudio_overview/#overview","text":"Practitioner Studio is a newly designed user experience for HCL Digital Experience. There are several advantages to this new look. The consolidated navigation has a consistent look and feel for most administration tasks so that commonly used tasks are found together. User assistance is easily found and is provided for pages that are in the navigation. In time, this interface also serves as the launching point for newly developed features. Enabling Practitioner Studio deactivates the legacy Administration and Applications pages. However, all of these functions are still available.","title":"Overview"},{"location":"allnewfeatures/practitioner_studio/working_prac_studio/","text":"Working with Practitioner Studio Learn how to enable, configure and disable Practitioner Studio on the latest version of HCL Digital Experience. If you are using the on-premise installation of HCL Digital Experience 9.5, you will need to perform deployment and configuration steps in order to work with Practitioner Studio. If you choose not to enable Practitioner Studio, you can still do all of the Administrator services that were available in previous releases.","title":"Working with Practitioner Studio"},{"location":"allnewfeatures/practitioner_studio/working_prac_studio/#working-with-practitioner-studio","text":"Learn how to enable, configure and disable Practitioner Studio on the latest version of HCL Digital Experience. If you are using the on-premise installation of HCL Digital Experience 9.5, you will need to perform deployment and configuration steps in order to work with Practitioner Studio. If you choose not to enable Practitioner Studio, you can still do all of the Administrator services that were available in previous releases.","title":"Working with Practitioner Studio"},{"location":"allnewfeatures/woodburn_studio/woodburn_studio/","text":"The Woodburn Studio demo site The Woodburn Studio is a website that demonstrates the use of some of the popular HCL Digital Experience features. Included in HCL Digital Experience 9.5, Woodburn Studio is a demo site for a fictional organization. Its intent is to show some of the themes that exist within HCL Digital Experience. Woodburn Studio leverages in-line editing and site management tools, as well as role and device-based controls on some parts of the site. It is not recommended to use Woodburn Studio for anything but a demo site. Woodburn Studio currently does not have E-commerce functionality. The Woodburn Studio demo site has a home page and several other microsites with one or more pages. Many of the elements are reused across the demo site, which is styled for the page on which they appear. Working with Woodburn Studio Woodburn Studio is not deployed by default. You can deploy, configure, and disable Woodburn Studio the same way you deploy Practitioner Studio. Go to Working with Practitioner Studio for more information. Note: Upon initial deployment, the HCL Digital Experience search indexes are not yet built. Therefore, search will not yield results for pages and content in the Woodburn Studio demonstration site until the search indexing services have run the first time. This will automatically occur with 24 hours of the initial deployment of DX. However, one can immediately force the indexer to run manually from the Search Collections screen in Administration > Search > Search Collections from the Practitioner Studio interface: From your HCL Digital Experience 9.5 Practitioner Studio interface, click Administration from the navigation menu as shown below. Click Search , then Search Collections . Click Default Search Collection from the Search Collection list. To start the crawler for the WCM Content Source, click the Play icon. To start the crawler for the Portal Content Source, click the Play icon. Go to Configuring a crawler to search your local portal site for more information about configuring and running a search crawler on your local portal site.","title":"The Woodburn Studio demo site"},{"location":"allnewfeatures/woodburn_studio/woodburn_studio/#the-woodburn-studio-demo-site","text":"The Woodburn Studio is a website that demonstrates the use of some of the popular HCL Digital Experience features. Included in HCL Digital Experience 9.5, Woodburn Studio is a demo site for a fictional organization. Its intent is to show some of the themes that exist within HCL Digital Experience. Woodburn Studio leverages in-line editing and site management tools, as well as role and device-based controls on some parts of the site. It is not recommended to use Woodburn Studio for anything but a demo site. Woodburn Studio currently does not have E-commerce functionality. The Woodburn Studio demo site has a home page and several other microsites with one or more pages. Many of the elements are reused across the demo site, which is styled for the page on which they appear.","title":"The Woodburn Studio demo site"},{"location":"allnewfeatures/woodburn_studio/woodburn_studio/#working-with-woodburn-studio","text":"Woodburn Studio is not deployed by default. You can deploy, configure, and disable Woodburn Studio the same way you deploy Practitioner Studio. Go to Working with Practitioner Studio for more information. Note: Upon initial deployment, the HCL Digital Experience search indexes are not yet built. Therefore, search will not yield results for pages and content in the Woodburn Studio demonstration site until the search indexing services have run the first time. This will automatically occur with 24 hours of the initial deployment of DX. However, one can immediately force the indexer to run manually from the Search Collections screen in Administration > Search > Search Collections from the Practitioner Studio interface: From your HCL Digital Experience 9.5 Practitioner Studio interface, click Administration from the navigation menu as shown below. Click Search , then Search Collections . Click Default Search Collection from the Search Collection list. To start the crawler for the WCM Content Source, click the Play icon. To start the crawler for the Portal Content Source, click the Play icon. Go to Configuring a crawler to search your local portal site for more information about configuring and running a search crawler on your local portal site.","title":"Working with Woodburn Studio"},{"location":"containerization/REST_APIs_remote_search/","text":"Configure Remote Search using REST APIs This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs. Introduction REST APIs are available to allow listing, deleting, modifying, and defining Portal Search Services (and their parameters). REST APIs may also be used to list, delete, and define Portal Search collections and Portal Search Content Providers (and their parameters). Reference the HCL DX 9.5 Help Center topic REST APIs for Search for additional information. Configuring Remote Search for DX Portal requires changes to the WebSphere configuration of both the remote search server, as well as the DX Portal server. In addition, Portal changes need to be made on the Portal server itself. Traditionally, these changes occurred via the Search Admin GUI on DX Portal. Starting HCL Digital Experience 9.5 CF199 and higher deployments, a new set of REST services also enables users to configure Remote Search. A REST service is implemented, and may be used to perform many of the same Remote Search configuration tasks in a selected environment. The environment can be a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment, a set of Docker images, or a set of DX Kubernetes PODs. Prerequisites In general, at least one (1) DX Portal Server and exactly one (1) DX Remote Search Server instances must be running. This can be in Docker, in Kubernetes, or a cluster. The Portal Servers must have addressability to the Remote Search Server and vice-versa. Optimally, this is handled through a DNS server so each of the servers has an IP address statically assigned and resolvable via DNS. Note: Some services typically assign IP addresses dynamically and are NOT available in DNS. This is true (by default) for Docker. To resolve this issue in Docker, add parameters to the docker run command. Docker images can be started like it follows both - to statistically assign a DNS address, and for that DNS address to be in the /etc/hosts file on the servers: ``` !/bin/bash PORTALIP=\"172.18.0.10\" REMOTESEARCHIP=\"172.18.0.11\" DOCKERHOST=\"172.19.0.1\" PORTAL_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output/core/dxen:v95_CF192_20210206-022427_rohan_DXQ-14209_on_develop_601eaed4\" REMOTE_SEARCH_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dxrs:v95_CF192_20210208-055522_rohan_develop_60215986\" echo \"Starting portal docker image with tag\" $PORTAL_DOCKER_IMAGE echo \"Starting remote search docker image with tag\" $REMOTE_SEARCH_DOCKER_IMAGE Start the two docker images Portal docker run -d --name portaldocker --net aDockerNetwork --ip=\"$PORTALIP\" -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200-10205:10200-10205 -p 7777:7777 --add-host=\"remotesearch:$REMOTESEARCHIP\" --add-host=\"remotesearch:$REMOTESEARCHIP2\" --add-host=\"dockerHost:$DOCKERHOST\" $PORTAL_DOCKER_IMAGE Remote Search docker run -d --name remotesearch --net aDockerNetwork --ip=\"$REMOTESEARCHIP\" -p 8880:8880 -p 2809:2809 -p 9043:9043 -p 9060:9060 -p 9080:9080 -p 9403:9403 --add-host=\"portaldocker:$PORTALIP\" --add-host=\"dockerHost:$DOCKERHOST\" $REMOTE_SEARCH_DOCKER_IMAGE ``` In order to use statically assigned IP addresses like in the example above, a private Docker subnet is created using the following command: docker network create --internal --subnet 172.18.0.0/16 aDockerNetwork The default network used in the example is 172.19.0.0/16. This is the address range used by the Docker host. The example will result to a new set of ConfigEngine tasks to exist for the WebSphere configuration portion. A new set of REST APIs are also now in place to support the command-line configuration of Portal Search. Note: The configuration commands used in the example configures remote search in a DX environment. However, the collections are empty even though they are defined. To populate the collections, the crawlers must be started. This can either be achieved by manually starting them, putting them on a schedule, or a combination of both. Access Rights For any attempted operation, the user that makes the request must first log into the Portal. The logged-in user is then checked for sufficient privileges before the requested action to any subsequent Remote Search REST API request is executed. If the logged-in user has no sufficient privileges, the Remote Search REST API request is rejected, and an appropriate response is returned. New ConfigEngine tasks ConfigEngine tasks on the DX Portal Server Complete configuration of WebSphere on the DX Portal Server is accomplished by executing the following command: ./ConfigEngine.sh configure-portal-for-remote-search -DWasPassword={Was Password} Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dremote.search.host.port default=\"9043\" -Dremote.search.cert.alias default=\"remotesearchalias\" -Dremote.search.iiop.url default=\"iiop://remotesearch:2809\" -Dremote.search.index.directory default=\"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections\" The following takes place when the DX Portal Server ConfigEngine command is executed: Retrieve the remote SSL key from the remote search server. Export the LPTA key to a file for the Portal server. Suppress the automatic creation of the Default Search Server on Portal restart, if it doesn't already exist. Set all the Resource Environment Providers for the JCR for WCM Authoring search. ConfigEngine tasks on the DX Remote Search Server Complete configuration of WebSphere on the DX Remote Search Server may now be accomplished by executing the following command: ./ConfigEngine.sh configure-remote-search-server-for-remote-search -DWasPassword={Was Password} Note: Complete Remote search server configuration require deploying WebScannerEjbEar.ear, and copying and unzipping file PseLibs.zip on remote search server. Refer to the Preparing for remote service topic for steps. Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dportal.host.name default=\"portaldocker\" -Dportal.port.number default=\"10042\" -Dportal.cert.alias default=\"portaldockeralias\" The following takes place when the DX Remote Search Server ConfigEngine command is executed: Retrieve remote SSL key from the Portal Server. Import the LTPA key exported from the Portal Server in the previous step. Edit the serverindex.xml file to have the correct Remote Search server host name. Important: Both the remote search server and the portal server must both be restarted after the ConfigEngine tasks are complete. Since the changes are IBM WebSphere Application Server Network Deployment-based cluster DX deployment changes in the profile, the changes are not picked up until the restart. New REST APIs Like all REST services, the type of HTTP command ( GET , PUT , POST , DELETE ) dictates the type of operation. The format of the URL is very similar for each type. However, some of the types (e.g. POST ) require JSON input to define the add. Here are the HTTP mapping types: GET -> list POST -> add DELETE -> delete The following example illustrates the elements of a URL, which generally consists of the following: /wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content The initial portal (/wps/mycontenthandler/!ut/p/searchadmin/service) is invariant and is present in all REST commands for remote search configuration. Remote+PSE+service+EJB presents the name of the service on which to perform an operation. Note that in a URL, the space character is NOT allowed. You can either replace the space character with the \" + \" character, or replace the space character with \" %20 \". Both forms are equivalent. The collection character sequence is only required when operating on a collection or providers within a collection. In the example, the collection name is JCRCollection1. This happens to be the required collection name for searches of artifacts by the WCM Authoring GUI. If the URL is malformed for whatever reason, an error will be returned in response to the request. Lastly, and only required when doing operations on a content provider for a particular service and collection, is you need to add the required character sequence provider, followed by the name of the provider in question. In our example, the provider is called JCR Content. Note that a \" + \" replaces a space character in the URL. Thus, the actual provider name is JCR Content. For all commands, the HTTP response code is useful. For example, if the HTTP response code is 401 , then it is likely that the one has NOT used the REST login before the REST configure command. All these commands require an \"Authenticated\" status. The POST and DEL commands require administrator access rights on the search configuration objects. In all cases, a combination of the HTTP response code along with a potential error message in the response payload indicates a variety of potential issues. Some of these issues may include a lack of access rights for the intended operation, the fact that the resource already exists (for example, trying to create/POST a service name that already exists), and more. Otherwise, a successful returns an HTTP response code of 20x List The following command list details of various remote search resources. No JSON body is required on the request. The HTTP response is the JSON which matches the type of the request. If the requested resource to \" LIST \" doesn't exist, the returned JSON will be empty (e.g. \" {} \"). http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} Sample command and output: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services { \"services\": [ { \"name\": \"Remote PSE service EJB\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB\" } ] } Note that each service name is followed by a relative link, which can be used to get more details of the service. The next command shows an example of this: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/ { \"RESOURCE_ENVIRONMENT_PROVIDER_NAME\": \"SearchPropertiesService\", \"facetedFields\": \"null\", \"WORK_MANAGER_DEPLOY\": \"wps/searchIndexWM\", \"EJB_Example\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"DefaultCollectionsDirectory\": \"null\", \"CONTENT_SOURCE_TYPE_FEATURE_NAME\": \"ContentSourceType\", \"EJB\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"MAX_BUILD_BATCH_SIZE\": \"10000\", \"fieldTypes\": \"null\", \"WORK_MANAGER_NATIVE\": \"force.hrl.work.manager.use.native.threads\", \"WORK_MANAGER\": \"wps/searchIndexWM\", \"PSE_TYPE_option_3\": \"soap\", \"PSE_TYPE_option_2\": \"ejb\", \"PSE_TYPE_option_1\": \"localhost\", \"IIOP_URL\": \"iiop://remotesearch:2809\", \"VALIDATE_COOKIE\": \"123\", \"PortalCollectionSourceName\": \"Remote PSE service EJB\", \"WORK_MANAGER_NAME\": \"wps/searchIndexWM\", \"PSE_TYPE\": \"ejb\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_PORTAL\": \"Portal\", \"HTTP_MAX_BODY_SIZE_MB\": \"20\", \"MAX_BUILD_INTERVAL_TIME_SECONDS\": \"300\", \"SetProperties\": \"on\", \"PortalCollectionName\": \"TestGood\", \"IIOP_URL_Example\": \"iiop://localhost:2811\", \"CLEAN_UP_TIME_OF_DAY_HOURS\": \"0\", \"SOAP_URL_Example\": \"http://localhost:10000/WebScannerSOAP/servlet/rpcrouter\", \"mappedFields\": \"null\", \"OPEN_WCM_WINDOW\": \"/wps/myportal/wcmContent?WCM_GLOBAL_CONTEXT=\", \"SOAP_URL\": \"null\", \"DEFAULT_acls_FIELDINFO\": \"contentSearchable=false, fieldSearchable=true, returnable=true, sortable=false, supportsExactMatch=true, parametric=false, typeAhead=false\", \"SecurityResolverId\": \"com.ibm.lotus.search.plugins.provider.core.PortalSecurityResolverFactory\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_UPLOAD\": \"Upload\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_WEB\": \"Web\", \"OpenResultMode\": \"new\", \"SEARCH_SECURITY_MODE\": \"SECURITY_MODE_PRE_POST_FILTER\", \"collections\": [ { \"name\": \"JCRCollection1\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1\" }, { \"name\": \"Portal Search Collection\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection\" } ] } Again, do note that the end of the list shows two collections, and the URLs that can be used to gather more information regarding those collections. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1 { \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"IndexTitleKey\": \"JCRCollection1\", \"IndexNameKey\": \"JCRCollection1\", \"IndexLanguageKey\": \"en_US\", \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"CollectionStatus\": \"true\", \"IndexDescriptionKey\": \"JCRCollection1\", \"DictionaryAnalysis\": \"true\", \"providers\": [ { \"name\": \"JCR Content\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content\" } ] } Delete If a resource to be deleted does not exist, then the returned JSON will return null (e.g. \" {} \"), which is the same as the returned JSON if the request is successful. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} In general, after a successful delete operation (HTTP 200), expect that the response JSON payload is null (e.g. \" {} \"). Add http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} If the resource to be added already exists, then an error message is returned like the following: Error 400: {resource} Already Exists where {resource} is one of \"service\", \"collection\" or \"content provider\" as is appropriate for the invalid request URL. The JSON returned as a result of an add REST call is exactly that, which is returned for the same GET call. Effectively, the returned JSON echoes the input add JSON request. Starting a Crawler Once the Service/Collection/Content Provider is configured, the crawlers will still not populate the indexes. To populate the indexes, the crawlers must be started. Crawlers can be started in one of two different ways: The first is via a scheduler, which automatically runs the crawler on a set schedule. Currently this schedule can only be configured in the search GUI. The second method is to immediately start the crawler either from the GUI or via a REST service. The REST service to start a crawler looks as follows: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection/provider/WCMContentSource/crawl This URL looks like very much a POST command to add a content provider. The only difference is that the crawl command is located at the end of the URL. This command will start an immediate crawl on the content provider in the previous portion of the URL. The output of the command is an HTTP 201 return code, along with a JSON body that is exactly like this: { \"crawl\": \"started\" } Use of API on Main Virtual Portal versus all other Virtual Portals On a Virtual Portal, the \u201c!ut/p/digest\u201d portal of the URL must be included as the contenthandler cannot issue the redirect when using the URL format without the portion mentioned. As such, referring to the example URLs above, the \u201c!ut/p/digest\u201d portal of the URL is NOT included. This implies that this URL is issued in the \"main\" VP of the DX Portal. A 302 redirect will take place, and the \u201c!ut/p/digest\u201d will be inserted in the final URL. This portion of the URL can also be used for the VP URL request. Parent topic: Customizing your container deployment","title":"Configure Remote Search using REST APIs"},{"location":"containerization/REST_APIs_remote_search/#configure-remote-search-using-rest-apis","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs.","title":"Configure Remote Search using REST APIs"},{"location":"containerization/REST_APIs_remote_search/#introduction","text":"REST APIs are available to allow listing, deleting, modifying, and defining Portal Search Services (and their parameters). REST APIs may also be used to list, delete, and define Portal Search collections and Portal Search Content Providers (and their parameters). Reference the HCL DX 9.5 Help Center topic REST APIs for Search for additional information. Configuring Remote Search for DX Portal requires changes to the WebSphere configuration of both the remote search server, as well as the DX Portal server. In addition, Portal changes need to be made on the Portal server itself. Traditionally, these changes occurred via the Search Admin GUI on DX Portal. Starting HCL Digital Experience 9.5 CF199 and higher deployments, a new set of REST services also enables users to configure Remote Search. A REST service is implemented, and may be used to perform many of the same Remote Search configuration tasks in a selected environment. The environment can be a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment, a set of Docker images, or a set of DX Kubernetes PODs.","title":"Introduction"},{"location":"containerization/REST_APIs_remote_search/#prerequisites","text":"In general, at least one (1) DX Portal Server and exactly one (1) DX Remote Search Server instances must be running. This can be in Docker, in Kubernetes, or a cluster. The Portal Servers must have addressability to the Remote Search Server and vice-versa. Optimally, this is handled through a DNS server so each of the servers has an IP address statically assigned and resolvable via DNS. Note: Some services typically assign IP addresses dynamically and are NOT available in DNS. This is true (by default) for Docker. To resolve this issue in Docker, add parameters to the docker run command. Docker images can be started like it follows both - to statistically assign a DNS address, and for that DNS address to be in the /etc/hosts file on the servers: ```","title":"Prerequisites"},{"location":"containerization/REST_APIs_remote_search/#binbash","text":"PORTALIP=\"172.18.0.10\" REMOTESEARCHIP=\"172.18.0.11\" DOCKERHOST=\"172.19.0.1\" PORTAL_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dx-build-output/core/dxen:v95_CF192_20210206-022427_rohan_DXQ-14209_on_develop_601eaed4\" REMOTE_SEARCH_DOCKER_IMAGE=\"quintana-docker.artifactory.cwp.pnp-hcl.com/dxrs:v95_CF192_20210208-055522_rohan_develop_60215986\" echo \"Starting portal docker image with tag\" $PORTAL_DOCKER_IMAGE echo \"Starting remote search docker image with tag\" $REMOTE_SEARCH_DOCKER_IMAGE","title":"!/bin/bash"},{"location":"containerization/REST_APIs_remote_search/#start-the-two-docker-images","text":"","title":"Start the two docker images"},{"location":"containerization/REST_APIs_remote_search/#portal","text":"docker run -d --name portaldocker --net aDockerNetwork --ip=\"$PORTALIP\" -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200-10205:10200-10205 -p 7777:7777 --add-host=\"remotesearch:$REMOTESEARCHIP\" --add-host=\"remotesearch:$REMOTESEARCHIP2\" --add-host=\"dockerHost:$DOCKERHOST\" $PORTAL_DOCKER_IMAGE","title":"Portal"},{"location":"containerization/REST_APIs_remote_search/#remote-search","text":"docker run -d --name remotesearch --net aDockerNetwork --ip=\"$REMOTESEARCHIP\" -p 8880:8880 -p 2809:2809 -p 9043:9043 -p 9060:9060 -p 9080:9080 -p 9403:9403 --add-host=\"portaldocker:$PORTALIP\" --add-host=\"dockerHost:$DOCKERHOST\" $REMOTE_SEARCH_DOCKER_IMAGE ``` In order to use statically assigned IP addresses like in the example above, a private Docker subnet is created using the following command: docker network create --internal --subnet 172.18.0.0/16 aDockerNetwork The default network used in the example is 172.19.0.0/16. This is the address range used by the Docker host. The example will result to a new set of ConfigEngine tasks to exist for the WebSphere configuration portion. A new set of REST APIs are also now in place to support the command-line configuration of Portal Search. Note: The configuration commands used in the example configures remote search in a DX environment. However, the collections are empty even though they are defined. To populate the collections, the crawlers must be started. This can either be achieved by manually starting them, putting them on a schedule, or a combination of both.","title":"Remote Search"},{"location":"containerization/REST_APIs_remote_search/#access-rights","text":"For any attempted operation, the user that makes the request must first log into the Portal. The logged-in user is then checked for sufficient privileges before the requested action to any subsequent Remote Search REST API request is executed. If the logged-in user has no sufficient privileges, the Remote Search REST API request is rejected, and an appropriate response is returned.","title":"Access Rights"},{"location":"containerization/REST_APIs_remote_search/#new-configengine-tasks","text":"ConfigEngine tasks on the DX Portal Server Complete configuration of WebSphere on the DX Portal Server is accomplished by executing the following command: ./ConfigEngine.sh configure-portal-for-remote-search -DWasPassword={Was Password} Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dremote.search.host.port default=\"9043\" -Dremote.search.cert.alias default=\"remotesearchalias\" -Dremote.search.iiop.url default=\"iiop://remotesearch:2809\" -Dremote.search.index.directory default=\"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections\" The following takes place when the DX Portal Server ConfigEngine command is executed: Retrieve the remote SSL key from the remote search server. Export the LPTA key to a file for the Portal server. Suppress the automatic creation of the Default Search Server on Portal restart, if it doesn't already exist. Set all the Resource Environment Providers for the JCR for WCM Authoring search. ConfigEngine tasks on the DX Remote Search Server Complete configuration of WebSphere on the DX Remote Search Server may now be accomplished by executing the following command: ./ConfigEngine.sh configure-remote-search-server-for-remote-search -DWasPassword={Was Password} Note: Complete Remote search server configuration require deploying WebScannerEjbEar.ear, and copying and unzipping file PseLibs.zip on remote search server. Refer to the Preparing for remote service topic for steps. Parameters may be added to this command to customize it. The following are all given -D parameters, along with the default values for each: -Dremote.search.host.name default=\"remotesearch\" -Dportal.host.name default=\"portaldocker\" -Dportal.port.number default=\"10042\" -Dportal.cert.alias default=\"portaldockeralias\" The following takes place when the DX Remote Search Server ConfigEngine command is executed: Retrieve remote SSL key from the Portal Server. Import the LTPA key exported from the Portal Server in the previous step. Edit the serverindex.xml file to have the correct Remote Search server host name. Important: Both the remote search server and the portal server must both be restarted after the ConfigEngine tasks are complete. Since the changes are IBM WebSphere Application Server Network Deployment-based cluster DX deployment changes in the profile, the changes are not picked up until the restart.","title":"New ConfigEngine tasks"},{"location":"containerization/REST_APIs_remote_search/#new-rest-apis","text":"Like all REST services, the type of HTTP command ( GET , PUT , POST , DELETE ) dictates the type of operation. The format of the URL is very similar for each type. However, some of the types (e.g. POST ) require JSON input to define the add. Here are the HTTP mapping types: GET -> list POST -> add DELETE -> delete The following example illustrates the elements of a URL, which generally consists of the following: /wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content The initial portal (/wps/mycontenthandler/!ut/p/searchadmin/service) is invariant and is present in all REST commands for remote search configuration. Remote+PSE+service+EJB presents the name of the service on which to perform an operation. Note that in a URL, the space character is NOT allowed. You can either replace the space character with the \" + \" character, or replace the space character with \" %20 \". Both forms are equivalent. The collection character sequence is only required when operating on a collection or providers within a collection. In the example, the collection name is JCRCollection1. This happens to be the required collection name for searches of artifacts by the WCM Authoring GUI. If the URL is malformed for whatever reason, an error will be returned in response to the request. Lastly, and only required when doing operations on a content provider for a particular service and collection, is you need to add the required character sequence provider, followed by the name of the provider in question. In our example, the provider is called JCR Content. Note that a \" + \" replaces a space character in the URL. Thus, the actual provider name is JCR Content. For all commands, the HTTP response code is useful. For example, if the HTTP response code is 401 , then it is likely that the one has NOT used the REST login before the REST configure command. All these commands require an \"Authenticated\" status. The POST and DEL commands require administrator access rights on the search configuration objects. In all cases, a combination of the HTTP response code along with a potential error message in the response payload indicates a variety of potential issues. Some of these issues may include a lack of access rights for the intended operation, the fact that the resource already exists (for example, trying to create/POST a service name that already exists), and more. Otherwise, a successful returns an HTTP response code of 20x","title":"New REST APIs"},{"location":"containerization/REST_APIs_remote_search/#list","text":"The following command list details of various remote search resources. No JSON body is required on the request. The HTTP response is the JSON which matches the type of the request. If the requested resource to \" LIST \" doesn't exist, the returned JSON will be empty (e.g. \" {} \"). http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} Sample command and output: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/services { \"services\": [ { \"name\": \"Remote PSE service EJB\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB\" } ] } Note that each service name is followed by a relative link, which can be used to get more details of the service. The next command shows an example of this: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/ { \"RESOURCE_ENVIRONMENT_PROVIDER_NAME\": \"SearchPropertiesService\", \"facetedFields\": \"null\", \"WORK_MANAGER_DEPLOY\": \"wps/searchIndexWM\", \"EJB_Example\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"DefaultCollectionsDirectory\": \"null\", \"CONTENT_SOURCE_TYPE_FEATURE_NAME\": \"ContentSourceType\", \"EJB\": \"ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome\", \"MAX_BUILD_BATCH_SIZE\": \"10000\", \"fieldTypes\": \"null\", \"WORK_MANAGER_NATIVE\": \"force.hrl.work.manager.use.native.threads\", \"WORK_MANAGER\": \"wps/searchIndexWM\", \"PSE_TYPE_option_3\": \"soap\", \"PSE_TYPE_option_2\": \"ejb\", \"PSE_TYPE_option_1\": \"localhost\", \"IIOP_URL\": \"iiop://remotesearch:2809\", \"VALIDATE_COOKIE\": \"123\", \"PortalCollectionSourceName\": \"Remote PSE service EJB\", \"WORK_MANAGER_NAME\": \"wps/searchIndexWM\", \"PSE_TYPE\": \"ejb\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_PORTAL\": \"Portal\", \"HTTP_MAX_BODY_SIZE_MB\": \"20\", \"MAX_BUILD_INTERVAL_TIME_SECONDS\": \"300\", \"SetProperties\": \"on\", \"PortalCollectionName\": \"TestGood\", \"IIOP_URL_Example\": \"iiop://localhost:2811\", \"CLEAN_UP_TIME_OF_DAY_HOURS\": \"0\", \"SOAP_URL_Example\": \"http://localhost:10000/WebScannerSOAP/servlet/rpcrouter\", \"mappedFields\": \"null\", \"OPEN_WCM_WINDOW\": \"/wps/myportal/wcmContent?WCM_GLOBAL_CONTEXT=\", \"SOAP_URL\": \"null\", \"DEFAULT_acls_FIELDINFO\": \"contentSearchable=false, fieldSearchable=true, returnable=true, sortable=false, supportsExactMatch=true, parametric=false, typeAhead=false\", \"SecurityResolverId\": \"com.ibm.lotus.search.plugins.provider.core.PortalSecurityResolverFactory\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_UPLOAD\": \"Upload\", \"CONTENT_SOURCE_TYPE_FEATURE_VAL_WEB\": \"Web\", \"OpenResultMode\": \"new\", \"SEARCH_SECURITY_MODE\": \"SECURITY_MODE_PRE_POST_FILTER\", \"collections\": [ { \"name\": \"JCRCollection1\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1\" }, { \"name\": \"Portal Search Collection\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection\" } ] } Again, do note that the end of the list shows two collections, and the URLs that can be used to gather more information regarding those collections. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1 { \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"IndexTitleKey\": \"JCRCollection1\", \"IndexNameKey\": \"JCRCollection1\", \"IndexLanguageKey\": \"en_US\", \"location\": \"/opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1\", \"CollectionStatus\": \"true\", \"IndexDescriptionKey\": \"JCRCollection1\", \"DictionaryAnalysis\": \"true\", \"providers\": [ { \"name\": \"JCR Content\", \"link\": \"/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/JCRCollection1/provider/JCR+Content\" } ] }","title":"List"},{"location":"containerization/REST_APIs_remote_search/#delete","text":"If a resource to be deleted does not exist, then the returned JSON will return null (e.g. \" {} \"), which is the same as the returned JSON if the request is successful. http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} In general, after a successful delete operation (HTTP 200), expect that the response JSON payload is null (e.g. \" {} \").","title":"Delete"},{"location":"containerization/REST_APIs_remote_search/#add","text":"http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/ http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name} http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/{service name}/collection/{collection name}/provider/{content provider name} If the resource to be added already exists, then an error message is returned like the following: Error 400: {resource} Already Exists where {resource} is one of \"service\", \"collection\" or \"content provider\" as is appropriate for the invalid request URL. The JSON returned as a result of an add REST call is exactly that, which is returned for the same GET call. Effectively, the returned JSON echoes the input add JSON request.","title":"Add"},{"location":"containerization/REST_APIs_remote_search/#starting-a-crawler","text":"Once the Service/Collection/Content Provider is configured, the crawlers will still not populate the indexes. To populate the indexes, the crawlers must be started. Crawlers can be started in one of two different ways: The first is via a scheduler, which automatically runs the crawler on a set schedule. Currently this schedule can only be configured in the search GUI. The second method is to immediately start the crawler either from the GUI or via a REST service. The REST service to start a crawler looks as follows: http://localhost:10039/wps/mycontenthandler/!ut/p/searchadmin/service/Remote+PSE+service+EJB/collection/Portal+Search+Collection/provider/WCMContentSource/crawl This URL looks like very much a POST command to add a content provider. The only difference is that the crawl command is located at the end of the URL. This command will start an immediate crawl on the content provider in the previous portion of the URL. The output of the command is an HTTP 201 return code, along with a JSON body that is exactly like this: { \"crawl\": \"started\" }","title":"Starting a Crawler"},{"location":"containerization/REST_APIs_remote_search/#use-of-api-on-main-virtual-portal-versus-all-other-virtual-portals","text":"On a Virtual Portal, the \u201c!ut/p/digest\u201d portal of the URL must be included as the contenthandler cannot issue the redirect when using the URL format without the portion mentioned. As such, referring to the example URLs above, the \u201c!ut/p/digest\u201d portal of the URL is NOT included. This implies that this URL is issued in the \"main\" VP of the DX Portal. A 302 redirect will take place, and the \u201c!ut/p/digest\u201d will be inserted in the final URL. This portion of the URL can also be used for the VP URL request. Parent topic: Customizing your container deployment","title":"Use of API on Main Virtual Portal versus all other Virtual Portals"},{"location":"containerization/azure_aks/","text":"Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). About this task Follow these steps to deploy HCL Digital Experience 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying HCL DX 9.5 Container Update CF191 and earlier, view the instructions to deploy using script commands instead of the dxctl tool as described below in this Help Center section. Note: Reference the latest HCL DX 9.5 Container Release and Update file list in the Docker deployment topic. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Example: Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. The following tools must be installed on a machine other than the Portal server: Docker Microsoft Azure CLI If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: It requires an AccessMode of ReadWriteMany . It requires a minimum of 40 GB , with the default request set to 100 GB . RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Azure container registry (For tagging and pushing). Deploying HCL Digital Experience (DX) 9.5 CF192 and later version Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release to the Microsoft Azure AKS platform: Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system. The file system can be on a local workstation or cloud platform. If deploying HCL DX 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Log in to your Microsoft Azure AKS platform. For more information, refer to the Microsoft Azure documentation if the Microsoft Azure CLI needs to be installed. Example log in command: az login Example: Create a resource group in Microsoft Azure using the following command: az group create --name <resourceGroupName> --location <region> Example: Azure Console Example: For more information, refer to the Microsoft Azure documentation on Resource . Create a Microsoft Azure Container Registry (ACR) to push the HCL DX 9.5 CF192 and later container images to. Azure Console Example: Once the ACR gets created, log in using the following command: az acr login --name <containerRegistry> Example: For more information, refer to the Microsoft Azure documentation on Container Registry . Set up the NFS server. Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this, and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw Default. sync Default after NFS 1.0, means that the server does not reply until after the commit. insecure Requires requests originate on ports less than 1024. ** root_squash Map requests to the nobody user.** Hard Required because this means the system keeps trying to write until it works.** nfsvers=4.1 rsize=8388608 Avoids dropped packages, default 8192. wsize=8388608 Avoids dropped packages, default 8192 timeo=600 60 seconds. retrans=2 Number of retries after a time out. noresvport ** Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with (**) are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. For more information, refer to the Microsoft Azure documentation on Storage . Configure the Microsoft Azure Kubernetes cluster. To configure kubectl to connect to your Kubernetes cluster, use the az aks get-credentials command. Example: az aks get-credentials --resource-group <resourcegroup> --name <clusterName> For more information, refer to the Microsoft Azure documentation on Cluster . DX-Container Image Management Change directory. Open a terminal window and change to the root directory of the extracted package. Docker load, tag and push by using the following commands: List Docker images docker images Docker load Load the containers into your Docker repository: docker load -i hcl-dx-core-image-v95_CF192_20210225-035822.tar.gz docker load -i hcl-dx-ambassador-image-154.tar.gz docker load -i hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz docker load -i hcl-dx-redis-image-5.0.1.tar.gz ACR details To tag and push the images to ACR, obtain login server details: az acr list --resource-group <resourceGroup> --query \"[].{acrLoginServer:loginServer}\" --output table Docker tag Tag your images using the tag command as shown in the examples below: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Example: docker tag hcl/dx/core:v95_CF192_20210225-035822 YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker tag hcl/dx/cloud-operator:v95_CF192_20210225-0546-YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker tag hcl/dx/ambassador:YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker tag hcl/dx/redis:5.0.1 YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Docker push Push the images to ACR using the following push command: docker push [OPTIONS] NAME[:TAG] Example commands: docker push YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Once the images are pushed, they can be listed using the commands below, or through use of the Microsoft Azure Kubernetes platform console. Command Example: az acr repository list --name <acrName> --output table Microsoft Azure AKS Console - DX 9.5 example: DX-Deployment using dxctl Create a StorageClass . Sample StorageClass YAML: ``` kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: example.com/nfs ``` Create a Persistence Volume (pv) with AccessMode as ReadWriteMany and reclaim policy as Retain. Sample PV YAML: apiVersion: v1 kind: PersistentVolume metadata: name: blrcaps-core-3 spec: capacity: storage: 100Gi accessModes: - ReadWriteMany nfs: path: NFS_PATH server: NFS_SERVER persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=10485760 - wsize=10485760 - timeo=600 - retrans=2 - noresvport Note: Make sure the PV is available. If it is not, remove claimRef: from the YAML file. Log in to the cluster. Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: az login Download dxctl . Instructions for downloading the latest packages are available here . Once downloaded and extracted, the hcl-dx-cloud-scripts directory structure is as follows: For more information about dxctl , visit the following documentation here . Configure and deploy using the HCL DX dcxtl tool. To start, change to the extracted files directory using the following command: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure the dxctl properties for the DX 9.5 Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Then, update the dxctl properties file values. Sample values: dx.namespace: endgametest-jeet1 dx.image: dxen dx.tag: v95_CF192_20210225-035822_rohan_release_95_CF192_60374773 dx.storageclass:dx-deploy-stg dx.volume: jeet3 dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl . Run the following command to deploy HCL DX 9.5 Container Update CF192 and later to Microsoft Azure AKS: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a deployment being created. Validate the deployment. Make sure all the pods are \"Running\" and in \"Ready\" state on your Microsoft Azure AKS platform, as shown in the example below: Generate TLS Certificate Create a TLS certification to be used by the deployment. Prior to this step, create a self-signed certificate to enable HTTPS using the following command: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '/CN=ambassador-cert' -nodes Then, store the Certificate and Key in a Kubernetes Secret using the following command: kubectl create secret tls dx-tls-cert --cert=cert.pem --key=key.pem -n <YourNamespace> Afterward, access the HCL DX 9.5 CF192 and later container deployment. To do so, obtain the external IP from the container platform Load balancer to access the HCL DX 9.5 deployment, as shown in the example below: $ kubectl get all -n NAMESPACE Then run the next command: https://EXTERNAL_IP/wps/portal Note: It is required to ensure the Microsoft Azure AKS load balancer configured permits external access. Consult the Microsoft Azure documentation for Load Balancer setup and default configuration details . Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment to later HCL DX 9.5 Container Update releases To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Update the deployment properties file with new image values, and run the Update command. Note: If the properties file is not available, then execute the following command to generate one. ./win/dxctl --getproperties --dx.namespace <Your Namespace> Example: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties Example: On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment . Delete the HCL Digital Experience (DX) 9.5 CF192 and later release Azure AKS deployment To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true Example: If some resources like services are still not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Deploying HCL Digital Experience (DX) 9.5 CF191 and earlier version Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF191 and earlier container version to the Microsoft Azure AKS platform: Download and extract the contents of the HCL DX 9.5 CF182 package to the local file system. In Microsoft Azure Kubernetes Service (AKS), load, tag, and push the HCL Digital Experience images into your MS Azure Container Registries. Note: In Microsoft Azure, when using AKS a single Container Registry, or multiple Container Registries may be used. See the Microsoft Azure Container Registry documentation for additional information about this topic. In this example, 10 Container Registries are created: As an alternative, DX Administrators can use a single or fewer registries and create 'Repositories' within. In this example, a Container Registry named azambassador with a repository 'ambassador' is shown: Administrators can tag and push another image into this Container Registry to get a second repository. In the following example, the Ambassador Redis image is added: The HCL DX 9.5 Container deployment does not assume 1, or many registries are defined, and either definition setup works. In the following example, the HCL DX 9.5 Redis 5.0.1 image is added to the azambassador Container Registry. This example shows loading the HCL DX 9.5 CF181 and earlier container into a local repository, tagging it and pushing it to the azuredxen Container Registry in the dxen \u2018Repository\u2019. Install the HCL Digital Experience (DX) 9.5 CF182 and later core images Load the HCL DX 9.5 CF182 and later images to your deployment. The following example uses the CF183 version in the load command: Docker load -I hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz Docker tag and docker push to the Azure environment: Once complete, the image is viewable in the Microsoft Azure repository: Reminder : Consult the HCL Digital Experience 9.5 Deployment \u2013 Docker topic for the latest list of HCL DX 9.5 container files that are available. HCL DX 9.5 Container Update CF183 files are used in these examples: CF183-core.zip files HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files HCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz To install HCL Digital Experience 9.5 core software to Microsoft Azure AKS, the following images are required: hcl-dx-cloud-operator-image-v95 hcl-dx-core-image-v95 hcl-dx-ambassador-image hcl-dx-redis-image Images included in the \u2018other\u2019 package are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. See examples that show how to load HCL DX 9.5 images to MS Azure below. In the following example, the items are loaded into the azuredxen Content Registry and multiple repositories are created. Images are tagged with dx-183 reflecting the HCL DX 9.5 Container Update CF183 version images used in this deployment. At this stage, the ./deploy/operator.yaml needs to be properly updated and the operator, and Redis image details need to be provided: First, replace the line: From: \u2018image: REPOSITORY_NAME /hcldx-cloud-operator:9.5.next\u2019 To: Add the proper value for the deployment, as in the following example: \u2018image: azuredxen.azurecr.io/hcldx-cloud-operator:v95_CF183_20200819-1711\u2019 Next, replace the values: \"REDIS_REPO\",\"REDIS_IMG_ENV\",\u201cREDIS_TAG_ENV\u201d with proper values. See the following example: Reviewing the Azure dashboard, administrators can see the following for redis: Deploy the Custom Resource Definition using the scripts/deployCrd.sh file. See the following example: Important : Ensure there is an available persistent volume for the deployment or a self-provisioning storage class. The HCL DX Help Center topic ( Sample Storage Class and Volume for HCL Digital Experience 9.5 Container Deployments ) can be referenced for related guidance. In this example, a storage class named dx-deploy-stg and a volume dxdeployhave been created: Run the deployment scripts as follows: ./scripts/deployDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador. INGRESSTAG - The image tag to use for Ambassador. The command output shows the values as they align with the deployment, and the result of each step. DX Administrators can use \u2018kubectl get pods -n az-emo\u2019 to check the pods as they are starting. See the following example: While waiting for the pods to start up DX Administrators must create a tls secret for ambassador as follows: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace In this example, an existing key and certification created using OpenSSL was used. Using SSL, administrators can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, administrators can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' At this stage, the deployment writes out the wp_profile into the persistent volume, and configure HCL DX 9.5 a minimum default configuration. See the HCL DX 9.5 Container Requirements and Customization topics for additional information. Once the HCL DX 9.5 dx-deployment-0 pod is running, administrators can access the HCL DX 9.5 deployment by obtaining the ambassador service details. Command examples to obtain this information: \u2018kubectl get svc -n az-demo\u2019 or \u2018kubectl get svc ambassador -n az-demo\u2019 Using the external IP address obtained via the kubectl get command ( https://external-ip/wps/portal ), select the resulting URL obtained to access your HCL DX 9.5 deployment. Note: It is required to ensure the MS Azure AKS load balancer configured permits external access. For more information, refer to the MS Azure documentation for Load Balancer setup for the default configuration details. (Optional) Deploy the OpenLDAP, Experience API, Content Composer, and Digital Asset Management components to Microsoft AKS Create a config map with the same name as the DX statefulset used to deploy the HCL DX 9.5 CF182 and later Core image software. By default, the DX statefulset is dx-deployment, as shown in this example: kubectl create configmap dx-deployment -n az-demo Once created, populate it with the following data: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Administrators can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n az-demo . After creating the config map, the HCL DX 9.5 CF182 and later deployment goes into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. Administrators can check the status via the command line using the command (example) kubectl get pods -n az-demo : As an alternative approach, administrators can check the status of the deployment progress through the MS Azure AKS dashboard: In this deployment of HCL DX 9.5 core and optional images, the DX core image is the last container to start successfully. Note that it restarts twice. Once restarts are complete, administrators can confirm the deployment and configuration of the DX core and OpenLDAP, Experience API, Content Composer, and Digital Asset Management images as follows: OpenLDAP image deployment validation: Navigate to Practitioner Studio > Administration > Security > Users and Groups , and search for all available groups: The group ldap_test_users should appear in this listing. To validate the Content Composer and Experience API image deployments, navigate to Practitioner Studio > Web Content > Content Composer : 3. To validate the Digital Asset Management and Experience API image deployments, navigate to **Practitioner Studio** \\> **Digital Assets**: ![](../images/containerization_aks_dam_validation.png \"DAM validation\") 4. To validate access to the Experience API, administrators and developers should be able to access the Experience API at the following URL: ``` https://external-ip/dx/api/core/v1/explorer/ ``` See the following section for additional information: - [Install Experience API, Content Composer, and Digital Asset Management](install_config_cc_dam.md) Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: ./scripts/UpdateDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE needs to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment. Delete the HCL Digital Experience (DX) 9.5 Azure AKS deployment Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh **NAMESPACE** NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv <pv_name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: az-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/<pv_name> edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploy DX Container to Microsoft Azure Kubernetes Service \\(AKS\\)"},{"location":"containerization/azure_aks/#deploy-dx-container-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS).","title":"Deploy DX Container to Microsoft Azure Kubernetes Service (AKS)"},{"location":"containerization/azure_aks/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying HCL DX 9.5 Container Update CF191 and earlier, view the instructions to deploy using script commands instead of the dxctl tool as described below in this Help Center section. Note: Reference the latest HCL DX 9.5 Container Release and Update file list in the Docker deployment topic.","title":"About this task"},{"location":"containerization/azure_aks/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Example: Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. The following tools must be installed on a machine other than the Portal server: Docker Microsoft Azure CLI If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: It requires an AccessMode of ReadWriteMany . It requires a minimum of 40 GB , with the default request set to 100 GB . RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Azure container registry (For tagging and pushing).","title":"Prerequisites"},{"location":"containerization/azure_aks/#deploying-hcl-digital-experience-dx-95-cf192-and-later-version","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release to the Microsoft Azure AKS platform: Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system. The file system can be on a local workstation or cloud platform. If deploying HCL DX 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Log in to your Microsoft Azure AKS platform. For more information, refer to the Microsoft Azure documentation if the Microsoft Azure CLI needs to be installed. Example log in command: az login Example: Create a resource group in Microsoft Azure using the following command: az group create --name <resourceGroupName> --location <region> Example: Azure Console Example: For more information, refer to the Microsoft Azure documentation on Resource . Create a Microsoft Azure Container Registry (ACR) to push the HCL DX 9.5 CF192 and later container images to. Azure Console Example: Once the ACR gets created, log in using the following command: az acr login --name <containerRegistry> Example: For more information, refer to the Microsoft Azure documentation on Container Registry . Set up the NFS server. Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this, and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw Default. sync Default after NFS 1.0, means that the server does not reply until after the commit. insecure Requires requests originate on ports less than 1024. ** root_squash Map requests to the nobody user.** Hard Required because this means the system keeps trying to write until it works.** nfsvers=4.1 rsize=8388608 Avoids dropped packages, default 8192. wsize=8388608 Avoids dropped packages, default 8192 timeo=600 60 seconds. retrans=2 Number of retries after a time out. noresvport ** Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with (**) are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. For more information, refer to the Microsoft Azure documentation on Storage . Configure the Microsoft Azure Kubernetes cluster. To configure kubectl to connect to your Kubernetes cluster, use the az aks get-credentials command. Example: az aks get-credentials --resource-group <resourcegroup> --name <clusterName> For more information, refer to the Microsoft Azure documentation on Cluster .","title":"Deploying HCL Digital Experience (DX) 9.5 CF192 and later version"},{"location":"containerization/azure_aks/#dx-container-image-management","text":"Change directory. Open a terminal window and change to the root directory of the extracted package. Docker load, tag and push by using the following commands: List Docker images docker images Docker load Load the containers into your Docker repository: docker load -i hcl-dx-core-image-v95_CF192_20210225-035822.tar.gz docker load -i hcl-dx-ambassador-image-154.tar.gz docker load -i hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz docker load -i hcl-dx-redis-image-5.0.1.tar.gz ACR details To tag and push the images to ACR, obtain login server details: az acr list --resource-group <resourceGroup> --query \"[].{acrLoginServer:loginServer}\" --output table Docker tag Tag your images using the tag command as shown in the examples below: docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] Example: docker tag hcl/dx/core:v95_CF192_20210225-035822 YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker tag hcl/dx/cloud-operator:v95_CF192_20210225-0546-YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker tag hcl/dx/ambassador:YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker tag hcl/dx/redis:5.0.1 YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Docker push Push the images to ACR using the following push command: docker push [OPTIONS] NAME[:TAG] Example commands: docker push YOUR_CONTAINER.azurecr.io/hcl/dx/core:v95_CF192_20210225-035822 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/cloud-operator:v95_CF192_20210225-0546 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/ambassador:154 docker push YOUR_CONTAINER.azurecr.io/hcl/dx/redis:5.0.1 Once the images are pushed, they can be listed using the commands below, or through use of the Microsoft Azure Kubernetes platform console. Command Example: az acr repository list --name <acrName> --output table Microsoft Azure AKS Console - DX 9.5 example:","title":"DX-Container Image Management"},{"location":"containerization/azure_aks/#dx-deployment-using-dxctl","text":"Create a StorageClass . Sample StorageClass YAML: ``` kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: example.com/nfs ``` Create a Persistence Volume (pv) with AccessMode as ReadWriteMany and reclaim policy as Retain. Sample PV YAML: apiVersion: v1 kind: PersistentVolume metadata: name: blrcaps-core-3 spec: capacity: storage: 100Gi accessModes: - ReadWriteMany nfs: path: NFS_PATH server: NFS_SERVER persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=10485760 - wsize=10485760 - timeo=600 - retrans=2 - noresvport Note: Make sure the PV is available. If it is not, remove claimRef: from the YAML file. Log in to the cluster. Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: az login Download dxctl . Instructions for downloading the latest packages are available here . Once downloaded and extracted, the hcl-dx-cloud-scripts directory structure is as follows: For more information about dxctl , visit the following documentation here . Configure and deploy using the HCL DX dcxtl tool. To start, change to the extracted files directory using the following command: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure the dxctl properties for the DX 9.5 Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Then, update the dxctl properties file values. Sample values: dx.namespace: endgametest-jeet1 dx.image: dxen dx.tag: v95_CF192_20210225-035822_rohan_release_95_CF192_60374773 dx.storageclass:dx-deploy-stg dx.volume: jeet3 dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl . Run the following command to deploy HCL DX 9.5 Container Update CF192 and later to Microsoft Azure AKS: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a deployment being created. Validate the deployment. Make sure all the pods are \"Running\" and in \"Ready\" state on your Microsoft Azure AKS platform, as shown in the example below:","title":"DX-Deployment using dxctl"},{"location":"containerization/azure_aks/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment. Prior to this step, create a self-signed certificate to enable HTTPS using the following command: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '/CN=ambassador-cert' -nodes Then, store the Certificate and Key in a Kubernetes Secret using the following command: kubectl create secret tls dx-tls-cert --cert=cert.pem --key=key.pem -n <YourNamespace> Afterward, access the HCL DX 9.5 CF192 and later container deployment. To do so, obtain the external IP from the container platform Load balancer to access the HCL DX 9.5 deployment, as shown in the example below: $ kubectl get all -n NAMESPACE Then run the next command: https://EXTERNAL_IP/wps/portal Note: It is required to ensure the Microsoft Azure AKS load balancer configured permits external access. Consult the Microsoft Azure documentation for Load Balancer setup and default configuration details .","title":"Generate TLS Certificate"},{"location":"containerization/azure_aks/#update-the-hcl-digital-experience-dx-95-azure-aks-deployment-to-later-hcl-dx-95-container-update-releases","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Update the deployment properties file with new image values, and run the Update command. Note: If the properties file is not available, then execute the following command to generate one. ./win/dxctl --getproperties --dx.namespace <Your Namespace> Example: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties Example: On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment .","title":"Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment to later HCL DX 9.5 Container Update releases"},{"location":"containerization/azure_aks/#delete-the-hcl-digital-experience-dx-95-cf192-and-later-release-azure-aks-deployment","text":"To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project using the following command: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true Example: If some resources like services are still not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Delete the HCL Digital Experience (DX) 9.5 CF192 and later release Azure AKS deployment"},{"location":"containerization/azure_aks/#deploying-hcl-digital-experience-dx-95-cf191-and-earlier-version","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF191 and earlier container version to the Microsoft Azure AKS platform: Download and extract the contents of the HCL DX 9.5 CF182 package to the local file system. In Microsoft Azure Kubernetes Service (AKS), load, tag, and push the HCL Digital Experience images into your MS Azure Container Registries. Note: In Microsoft Azure, when using AKS a single Container Registry, or multiple Container Registries may be used. See the Microsoft Azure Container Registry documentation for additional information about this topic. In this example, 10 Container Registries are created: As an alternative, DX Administrators can use a single or fewer registries and create 'Repositories' within. In this example, a Container Registry named azambassador with a repository 'ambassador' is shown: Administrators can tag and push another image into this Container Registry to get a second repository. In the following example, the Ambassador Redis image is added: The HCL DX 9.5 Container deployment does not assume 1, or many registries are defined, and either definition setup works. In the following example, the HCL DX 9.5 Redis 5.0.1 image is added to the azambassador Container Registry. This example shows loading the HCL DX 9.5 CF181 and earlier container into a local repository, tagging it and pushing it to the azuredxen Container Registry in the dxen \u2018Repository\u2019.","title":"Deploying HCL Digital Experience (DX) 9.5 CF191 and earlier version"},{"location":"containerization/azure_aks/#install-the-hcl-digital-experience-dx-95-cf182-and-later-core-images","text":"Load the HCL DX 9.5 CF182 and later images to your deployment. The following example uses the CF183 version in the load command: Docker load -I hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz Docker tag and docker push to the Azure environment: Once complete, the image is viewable in the Microsoft Azure repository: Reminder : Consult the HCL Digital Experience 9.5 Deployment \u2013 Docker topic for the latest list of HCL DX 9.5 container files that are available. HCL DX 9.5 Container Update CF183 files are used in these examples: CF183-core.zip files HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz hcl-dx-redis-image-5.0.1.tar.gz CF183-other.zip files HCL DX notices V9.5 CF183.txt hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz To install HCL Digital Experience 9.5 core software to Microsoft Azure AKS, the following images are required: hcl-dx-cloud-operator-image-v95 hcl-dx-core-image-v95 hcl-dx-ambassador-image hcl-dx-redis-image Images included in the \u2018other\u2019 package are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. See examples that show how to load HCL DX 9.5 images to MS Azure below. In the following example, the items are loaded into the azuredxen Content Registry and multiple repositories are created. Images are tagged with dx-183 reflecting the HCL DX 9.5 Container Update CF183 version images used in this deployment. At this stage, the ./deploy/operator.yaml needs to be properly updated and the operator, and Redis image details need to be provided: First, replace the line: From: \u2018image: REPOSITORY_NAME /hcldx-cloud-operator:9.5.next\u2019 To: Add the proper value for the deployment, as in the following example: \u2018image: azuredxen.azurecr.io/hcldx-cloud-operator:v95_CF183_20200819-1711\u2019 Next, replace the values: \"REDIS_REPO\",\"REDIS_IMG_ENV\",\u201cREDIS_TAG_ENV\u201d with proper values. See the following example: Reviewing the Azure dashboard, administrators can see the following for redis: Deploy the Custom Resource Definition using the scripts/deployCrd.sh file. See the following example: Important : Ensure there is an available persistent volume for the deployment or a self-provisioning storage class. The HCL DX Help Center topic ( Sample Storage Class and Volume for HCL Digital Experience 9.5 Container Deployments ) can be referenced for related guidance. In this example, a storage class named dx-deploy-stg and a volume dxdeployhave been created: Run the deployment scripts as follows: ./scripts/deployDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador. INGRESSTAG - The image tag to use for Ambassador. The command output shows the values as they align with the deployment, and the result of each step. DX Administrators can use \u2018kubectl get pods -n az-emo\u2019 to check the pods as they are starting. See the following example: While waiting for the pods to start up DX Administrators must create a tls secret for ambassador as follows: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace In this example, an existing key and certification created using OpenSSL was used. Using SSL, administrators can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, administrators can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' At this stage, the deployment writes out the wp_profile into the persistent volume, and configure HCL DX 9.5 a minimum default configuration. See the HCL DX 9.5 Container Requirements and Customization topics for additional information. Once the HCL DX 9.5 dx-deployment-0 pod is running, administrators can access the HCL DX 9.5 deployment by obtaining the ambassador service details. Command examples to obtain this information: \u2018kubectl get svc -n az-demo\u2019 or \u2018kubectl get svc ambassador -n az-demo\u2019 Using the external IP address obtained via the kubectl get command ( https://external-ip/wps/portal ), select the resulting URL obtained to access your HCL DX 9.5 deployment. Note: It is required to ensure the MS Azure AKS load balancer configured permits external access. For more information, refer to the MS Azure documentation for Load Balancer setup for the default configuration details.","title":"Install the HCL Digital Experience (DX) 9.5 CF182 and later core images"},{"location":"containerization/azure_aks/#optional-deploy-the-openldap-experience-api-content-composer-and-digital-asset-management-components-to-microsoft-aks","text":"Create a config map with the same name as the DX statefulset used to deploy the HCL DX 9.5 CF182 and later Core image software. By default, the DX statefulset is dx-deployment, as shown in this example: kubectl create configmap dx-deployment -n az-demo Once created, populate it with the following data: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Administrators can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n az-demo . After creating the config map, the HCL DX 9.5 CF182 and later deployment goes into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. Administrators can check the status via the command line using the command (example) kubectl get pods -n az-demo : As an alternative approach, administrators can check the status of the deployment progress through the MS Azure AKS dashboard: In this deployment of HCL DX 9.5 core and optional images, the DX core image is the last container to start successfully. Note that it restarts twice. Once restarts are complete, administrators can confirm the deployment and configuration of the DX core and OpenLDAP, Experience API, Content Composer, and Digital Asset Management images as follows: OpenLDAP image deployment validation: Navigate to Practitioner Studio > Administration > Security > Users and Groups , and search for all available groups: The group ldap_test_users should appear in this listing. To validate the Content Composer and Experience API image deployments, navigate to Practitioner Studio > Web Content > Content Composer : 3. To validate the Digital Asset Management and Experience API image deployments, navigate to **Practitioner Studio** \\> **Digital Assets**: ![](../images/containerization_aks_dam_validation.png \"DAM validation\") 4. To validate access to the Experience API, administrators and developers should be able to access the Experience API at the following URL: ``` https://external-ip/dx/api/core/v1/explorer/ ``` See the following section for additional information: - [Install Experience API, Content Composer, and Digital Asset Management](install_config_cc_dam.md)","title":"(Optional) Deploy the OpenLDAP, Experience API, Content Composer, and Digital Asset Management components to Microsoft AKS"},{"location":"containerization/azure_aks/#update-the-hcl-digital-experience-dx-95-azure-aks-deployment","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: ./scripts/UpdateDx.sh az-demo 1 azuredxen.azurecr.io dxen v95_CF183_20200819-1159 dxeploy dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE needs to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment.","title":"Update the HCL Digital Experience (DX) 9.5 Azure AKS deployment"},{"location":"containerization/azure_aks/#delete-the-hcl-digital-experience-dx-95-azure-aks-deployment","text":"Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh **NAMESPACE** NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv <pv_name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: az-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/<pv_name> edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Delete the HCL Digital Experience (DX) 9.5 Azure AKS deployment"},{"location":"containerization/basic_monitor_helm_deployment/","text":"Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitoring Requirements Video: Understanding the Liveness and Readiness Probes for HCL DX 9.5 Container Helm Deployments To use the monitoring commands described below, the Kubernetes Metrics Server must be installed, configured and running. For information on how to do this, please see the Kubernetes documentation . Monitoring commands With the Metrics Server installed, standard kubectl top commands can be used to monitor Digital Experience 9.5 components or the nodes on which they are installed. Examples To get memory and CPU usage details for the pods in your DX deployment: kubectl top pod -n your-namespace -l release=your-release-name In the above example your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. To get memory and CPU usage details for the current Kubernetes node: kubectl top node Parent topic: Troubleshooting your Helm deployment","title":"Basic monitoring"},{"location":"containerization/basic_monitor_helm_deployment/#basic-monitoring","text":"This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm.","title":"Basic monitoring"},{"location":"containerization/basic_monitor_helm_deployment/#monitoring","text":"Requirements Video: Understanding the Liveness and Readiness Probes for HCL DX 9.5 Container Helm Deployments To use the monitoring commands described below, the Kubernetes Metrics Server must be installed, configured and running. For information on how to do this, please see the Kubernetes documentation . Monitoring commands With the Metrics Server installed, standard kubectl top commands can be used to monitor Digital Experience 9.5 components or the nodes on which they are installed. Examples To get memory and CPU usage details for the pods in your DX deployment: kubectl top pod -n your-namespace -l release=your-release-name In the above example your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. To get memory and CPU usage details for the current Kubernetes node: kubectl top node Parent topic: Troubleshooting your Helm deployment","title":"Monitoring"},{"location":"containerization/c_kubesupportstatement/","text":"Container platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination. General Kubernetes Support Policy: HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated, with the intent of staying up-to-date as possible. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base and the following table for additional details. From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes or Cloud Platform version, you may be asked to install a supported level. DX CF Releases Red Hat OpenShift Amazon EKS Azure AKS Google GKE CF201 OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF200|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF199|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF198|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.19AKS 1.20 |GKE 1.20| |CF197|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.20| |CF196|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF195|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF194|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19|GKE 1.18| |CF193|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19AKS 1.17.7 |GKE 1.18| |CF192|OS 4.4Kubernetes 1.17 |EKS 1.17EKS 1.18 |AKS 1.17.7 AKS 1.18.4 |GKE 1.16 GKE 1.17 GKE 1.18 | |CF191|OS 4.4 Kubernetes 1.17 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF19|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF184|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| DX CF Releases Red Hat OpenShift Amazon EKS CF201 OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux) |EKS 1.19 (Clustered Linux)| |CF200|OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux)|EKS 1.19 (Clustered Linux)| |CF199|OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux)|EKS 1.19 (Clustered Linux)| |CF198|OS 4.7- Kubernetes 1.20 OS 4.5 Kubernetes 1.18 |NA| |CF197|OS 4.5Kubernetes 1.18 |NA| |CF196|OS 4.5Kubernetes 1.18 |NA| |CF195|OS 4.5Kubernetes 1.18 |NA| |CF194|OS 4.5Kubernetes 1.18 |NA| |CF193|OS 4.5Kubernetes 1.18 |NA| |CF192|OS 4.4Kubernetes 1.17 |NA| |CF191|OS 4.4Kubernetes 1.17 |NA| |CF19|OS 4.3Kubernetes 1.16 |NA| |CF184|OS 4.3Kubernetes 1.16 |NA| Parent topic: Digital Experience on containerized platforms","title":"Container platform support matrix"},{"location":"containerization/c_kubesupportstatement/#container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . HCL encourages customers to remain up-to-date on the latest DX and Kubernetes releases. As a result, DX will provide all fixes on the latest release. Customers may be asked to upgrade to the latest DX release to assist with problem determination.","title":"Container platform support matrix"},{"location":"containerization/c_kubesupportstatement/#general-kubernetes-support-policy","text":"HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated, with the intent of staying up-to-date as possible. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base and the following table for additional details. From time-to-time, platform providers may release previews of upcoming Kubernetes versions. We will not provide support for those versions. If you encounter any issue on an unsupported or untested Kubernetes or Cloud Platform version, you may be asked to install a supported level. DX CF Releases Red Hat OpenShift Amazon EKS Azure AKS Google GKE CF201 OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF200|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF199|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.20|GKE 1.21| |CF198|OS 4.7- Kubernetes 1.20 |EKS 1.19|AKS 1.19AKS 1.20 |GKE 1.20| |CF197|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.20| |CF196|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF195|OS 4.6- Kubernetes 1.19 OS 4.5 Kubernetes 1.18 |EKS 1.19|AKS 1.19|GKE 1.18| |CF194|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19|GKE 1.18| |CF193|OS 4.5- Kubernetes 1.18 OS 4.6 Kubernetes 1.19 |EKS 1.19|AKS 1.19AKS 1.17.7 |GKE 1.18| |CF192|OS 4.4Kubernetes 1.17 |EKS 1.17EKS 1.18 |AKS 1.17.7 AKS 1.18.4 |GKE 1.16 GKE 1.17 GKE 1.18 | |CF191|OS 4.4 Kubernetes 1.17 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF19|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| |CF184|OS 4.3Kubernetes 1.16 |EKS 1.17|AKS 1.17.7|GKE 1.16| DX CF Releases Red Hat OpenShift Amazon EKS CF201 OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux) |EKS 1.19 (Clustered Linux)| |CF200|OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux)|EKS 1.19 (Clustered Linux)| |CF199|OS 4.7Kubernetes 1.20 (Standalone Linux + Clustered Linux)|EKS 1.19 (Clustered Linux)| |CF198|OS 4.7- Kubernetes 1.20 OS 4.5 Kubernetes 1.18 |NA| |CF197|OS 4.5Kubernetes 1.18 |NA| |CF196|OS 4.5Kubernetes 1.18 |NA| |CF195|OS 4.5Kubernetes 1.18 |NA| |CF194|OS 4.5Kubernetes 1.18 |NA| |CF193|OS 4.5Kubernetes 1.18 |NA| |CF192|OS 4.4Kubernetes 1.17 |NA| |CF191|OS 4.4Kubernetes 1.17 |NA| |CF19|OS 4.3Kubernetes 1.16 |NA| |CF184|OS 4.3Kubernetes 1.16 |NA| Parent topic: Digital Experience on containerized platforms","title":"General Kubernetes Support Policy:"},{"location":"containerization/ci_cd/","text":"Digital Experience Application deployment This section outlines features and functionality delivered as part of continuous integration and continuous delivery for HCL Digital Experience. Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience container deployments while minimizing operations downtime. Deploying Custom Code in OpenShift This section outlines deploying custom code to HCL Digital Experience in OpenShift. Container Staging This section describes how to move from an existing HCL Portal environment to a containerized Digital Experience environment. Deploy DX components using HCL DXClient and DXConnect HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process.","title":"Digital Experience Application deployment"},{"location":"containerization/ci_cd/#digital-experience-application-deployment","text":"This section outlines features and functionality delivered as part of continuous integration and continuous delivery for HCL Digital Experience. Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience container deployments while minimizing operations downtime. Deploying Custom Code in OpenShift This section outlines deploying custom code to HCL Digital Experience in OpenShift. Container Staging This section describes how to move from an existing HCL Portal environment to a containerized Digital Experience environment. Deploy DX components using HCL DXClient and DXConnect HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process.","title":"Digital Experience Application deployment"},{"location":"containerization/configure_access_helm_logs/","text":"Configure and access logs in Helm This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. HCL Digital Experience logs are important for maintaining and troubleshooting both environments and custom applications. These logs frequently form part of the essential information requested by HCL Support to diagnose issues. In a Helm-based deployment of DX, logs are exposed as Kubernetes container logs which give a consistent mechanism for retrieving the logs of different components, as well as making them consumable by cluster-level logging solutions in Kubernetes. Configure logging In CF200, a new mechanism is introduced for configuring log settings at runtime (without pod restarts) in Helm-based DX deployments. Log levels and trace strings are set in your custom-values.yaml file and applied using a helm upgrade command. Under the covers, this sets values in a new <release-name>-global config map which are monitored by the various running DX containers. When the containers detect a change to the values pertinent to themselves, they update their log configurations accordingly (without restarting). At that point, the new log behavior is immediately reflected in their Kubernetes logs. Note: OpenLDAP, Ambassador, and Redis are not yet configurable using this feature. Setting the log configuration for a DX application You can set a desired log configuration for a DX application by specifying an appropriate log string in your Helm custom-values.yaml file. Place the log string in the level property for the specified application. These properties are found in the logging subsection of the incubator section. For example, to set the configuration for Content Composer, use the following property: incubator: logging: # Content Composer specific logging configuration contentComposer: level: \"api:server-v1:*=info\" You can see the string format in the following section. Once the property is set, run the helm upgrade command. Log configuration string format Log configuration strings (the values set in the level properties of the custom-values.yaml) use the following common format, where multiple trace settings for the same application are separated by commas: <component>:<pattern>=<log-level>,<component>:<pattern>=<log-level> component - represents a subsystem of the application and must be from a limited list per application (see the following examples). pattern - describes the specific component area to log (for example, a Java package). log-level - defines the granularity at which logging is enabled (see later for permitted levels). The exact format of pattern depends on the configured application. The appropriate values are provided by HCL Support, if you are asked to enable tracing as part of a case. Some examples of log configuration strings for different DX applications are given as follows: DX Core example : wp_profile:com.hcl.App=info,wp_profile:com.hcl.util.Data=finest Digital Asset Management example : api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug Supported application and component names Following are the supported application and component names, where the application names are the subsections under logging in the custom-values.yaml: Application Component names core wp_profile , cw_profile contentComposer api designStudio api digitalAssetManagement api , worker imageProcessor api persistenceConnectionPool pgpool persistenceNode psql , repmgr remoteSearch prs_profile ringApi api runtimeController controller Supported log levels For most applications, three log levels are supported: debug , info , and error . Core and Remote Search, where all existing WebSphere Application Server trace levels are supported, such as all or finest . Accessing Kubernetes container logs Container logs for DX applications can be accessed individually or collectively, as described in the following subsections. Logs for DX Core and Remote Search are accessed differently from other applications, as those pods have multiple containers to provide access to additional logs. Accessing DX Core logs To access a Core application log, use the command: kubectl logs -n <namespace> <core-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-core-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Core log file. Note: The additional logging enabled for Core goes to trace.log. To configure trace.log for sidecar logging, see Configure Core sidecar logging . By default, two sidecar containers are launched with Core: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Core sidecar log containers, please see Configure Core sidecar logging . Accessing Remote Search logs To access a Remote Search application log, use the command: kubectl logs -n <namespace> <remote-search-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Remote Search log file. Note: The additional logging enabled for Remote Search goes to trace.log. To configure trace.log for sidecar logging, see Configure Remote Search sidecar logging . By default, two sidecar containers are launched with Remote Search: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Remote Search sidecar log containers, please see Configure Remote Search sidecar logging . Accessing logs for other applications Applications other than Core and Remote Search do not have logging sidecar containers and only provide a single log per pod, which can typically be obtained using the command: kubectl logs -n <namespace> <pod-name> (omitting a container name), for example: kubectl logs -n dxns dx-deployment-digital-asset-management-0 This is not the case for Persistence Node pods, which have non-logging sidecar containers (for metrics gathering). For these pods, you must append the main container name ( persistence-node ) when accessing the log, for example: kubectl logs -n dxns dx-deployment-persistence-node-0 persistence-node Accessing all application logs simultaneously All application logs from DX pods in a deployment can be combined into a single output using the command: kubectl logs -n <namespace> -l release=<release-name> --tail=-1 --all-containers where: namespace - is the namespace in which your HCL Digital Experience deployment is installed. release-name - is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command. Default log output The log output for a DX deployment is set to a non-verbose configuration by default. Application name Default log settings Core *=info Content Composer api:server-v1:*=info Design Studio api:server-v1:*=info Digital Asset Management api:server-v1:*=info,worker:server-v1:*=info Image Processor api:server-v1:*=info Persistence Connection Pool pgpool:=info Persistence Node psql:=info,repmgr:=info Remote Search *=info Ring API api:server-v1:*=info Runtime Controller controller:.*=INFO,controller:com.hcl.dx.*=INFO All applications send their log output directly to stdout and stderr of the corresponding container that they are running in. Besides that, the following applications also write their log output into a file that is available in the file system of the containers: Application name Log location Core /opt/HCL/wp_profile/logs/WebSphere_Portal and /opt/HCL/AppServer/profiles/cw_profile/logs/server1 Remote Search /opt/HCL/AppServer/profiles/prs_profile/logs/server1 Persistence Node /var/lib/pgsql/11/data/log and /var/lib/pgsql/11/data/dx/repmgr/log Note: The Core and Remote Search have the following default settings for their log output files and that needs to be considered when sizing their persistent volumes: Output file Size per file Files kept SystemOut.log 5MB 3 SystemErr.log 5MB 3 trace.log 20MB 3 The amount of logs that are stored per container in running the Pods depends on the configuration of your Kubernetes Cluster. Refer to the documentation of your cloud provider for further information. Note that for all applications that do not write their logs separately to a file, the only source of historical log data is the Kubernetes logging. We encourage the customers to process the logging of their Kubernetes Cluster in a separate logging solution of their choice. Parent topic: Troubleshooting your Helm deployment","title":"Configure and access logs in Helm"},{"location":"containerization/configure_access_helm_logs/#configure-and-access-logs-in-helm","text":"This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. HCL Digital Experience logs are important for maintaining and troubleshooting both environments and custom applications. These logs frequently form part of the essential information requested by HCL Support to diagnose issues. In a Helm-based deployment of DX, logs are exposed as Kubernetes container logs which give a consistent mechanism for retrieving the logs of different components, as well as making them consumable by cluster-level logging solutions in Kubernetes.","title":"Configure and access logs in Helm"},{"location":"containerization/configure_access_helm_logs/#configure-logging","text":"In CF200, a new mechanism is introduced for configuring log settings at runtime (without pod restarts) in Helm-based DX deployments. Log levels and trace strings are set in your custom-values.yaml file and applied using a helm upgrade command. Under the covers, this sets values in a new <release-name>-global config map which are monitored by the various running DX containers. When the containers detect a change to the values pertinent to themselves, they update their log configurations accordingly (without restarting). At that point, the new log behavior is immediately reflected in their Kubernetes logs. Note: OpenLDAP, Ambassador, and Redis are not yet configurable using this feature.","title":"Configure logging"},{"location":"containerization/configure_access_helm_logs/#setting-the-log-configuration-for-a-dx-application","text":"You can set a desired log configuration for a DX application by specifying an appropriate log string in your Helm custom-values.yaml file. Place the log string in the level property for the specified application. These properties are found in the logging subsection of the incubator section. For example, to set the configuration for Content Composer, use the following property: incubator: logging: # Content Composer specific logging configuration contentComposer: level: \"api:server-v1:*=info\" You can see the string format in the following section. Once the property is set, run the helm upgrade command.","title":"Setting the log configuration for a DX application"},{"location":"containerization/configure_access_helm_logs/#log-configuration-string-format","text":"Log configuration strings (the values set in the level properties of the custom-values.yaml) use the following common format, where multiple trace settings for the same application are separated by commas: <component>:<pattern>=<log-level>,<component>:<pattern>=<log-level> component - represents a subsystem of the application and must be from a limited list per application (see the following examples). pattern - describes the specific component area to log (for example, a Java package). log-level - defines the granularity at which logging is enabled (see later for permitted levels). The exact format of pattern depends on the configured application. The appropriate values are provided by HCL Support, if you are asked to enable tracing as part of a case. Some examples of log configuration strings for different DX applications are given as follows: DX Core example : wp_profile:com.hcl.App=info,wp_profile:com.hcl.util.Data=finest Digital Asset Management example : api:server-v1:dist=info,worker:server-v1:dist=info,api:server-v1:dist:server=debug","title":"Log configuration string format"},{"location":"containerization/configure_access_helm_logs/#supported-application-and-component-names","text":"Following are the supported application and component names, where the application names are the subsections under logging in the custom-values.yaml: Application Component names core wp_profile , cw_profile contentComposer api designStudio api digitalAssetManagement api , worker imageProcessor api persistenceConnectionPool pgpool persistenceNode psql , repmgr remoteSearch prs_profile ringApi api runtimeController controller","title":"Supported application and component names"},{"location":"containerization/configure_access_helm_logs/#supported-log-levels","text":"For most applications, three log levels are supported: debug , info , and error . Core and Remote Search, where all existing WebSphere Application Server trace levels are supported, such as all or finest .","title":"Supported log levels"},{"location":"containerization/configure_access_helm_logs/#accessing-kubernetes-container-logs","text":"Container logs for DX applications can be accessed individually or collectively, as described in the following subsections. Logs for DX Core and Remote Search are accessed differently from other applications, as those pods have multiple containers to provide access to additional logs.","title":"Accessing Kubernetes container logs"},{"location":"containerization/configure_access_helm_logs/#accessing-dx-core-logs","text":"To access a Core application log, use the command: kubectl logs -n <namespace> <core-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-core-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Core log file. Note: The additional logging enabled for Core goes to trace.log. To configure trace.log for sidecar logging, see Configure Core sidecar logging . By default, two sidecar containers are launched with Core: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Core sidecar log containers, please see Configure Core sidecar logging .","title":"Accessing DX Core logs"},{"location":"containerization/configure_access_helm_logs/#accessing-remote-search-logs","text":"To access a Remote Search application log, use the command: kubectl logs -n <namespace> <remote-search-pod-name> <sidecar-container-name> For example: kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log This retrieves the log for a single sidecar container, which corresponds to a single Remote Search log file. Note: The additional logging enabled for Remote Search goes to trace.log. To configure trace.log for sidecar logging, see Configure Remote Search sidecar logging . By default, two sidecar containers are launched with Remote Search: system-out-log - Exposes the WebSphere_Portal/SystemOut.log file. system-err-log - Exposes the WebSphere_Portal/SystemErr.log file. For information on configuring additional Remote Search sidecar log containers, please see Configure Remote Search sidecar logging .","title":"Accessing Remote Search logs"},{"location":"containerization/configure_access_helm_logs/#accessing-logs-for-other-applications","text":"Applications other than Core and Remote Search do not have logging sidecar containers and only provide a single log per pod, which can typically be obtained using the command: kubectl logs -n <namespace> <pod-name> (omitting a container name), for example: kubectl logs -n dxns dx-deployment-digital-asset-management-0 This is not the case for Persistence Node pods, which have non-logging sidecar containers (for metrics gathering). For these pods, you must append the main container name ( persistence-node ) when accessing the log, for example: kubectl logs -n dxns dx-deployment-persistence-node-0 persistence-node","title":"Accessing logs for other applications"},{"location":"containerization/configure_access_helm_logs/#accessing-all-application-logs-simultaneously","text":"All application logs from DX pods in a deployment can be combined into a single output using the command: kubectl logs -n <namespace> -l release=<release-name> --tail=-1 --all-containers where: namespace - is the namespace in which your HCL Digital Experience deployment is installed. release-name - is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command.","title":"Accessing all application logs simultaneously"},{"location":"containerization/configure_access_helm_logs/#default-log-output","text":"The log output for a DX deployment is set to a non-verbose configuration by default. Application name Default log settings Core *=info Content Composer api:server-v1:*=info Design Studio api:server-v1:*=info Digital Asset Management api:server-v1:*=info,worker:server-v1:*=info Image Processor api:server-v1:*=info Persistence Connection Pool pgpool:=info Persistence Node psql:=info,repmgr:=info Remote Search *=info Ring API api:server-v1:*=info Runtime Controller controller:.*=INFO,controller:com.hcl.dx.*=INFO All applications send their log output directly to stdout and stderr of the corresponding container that they are running in. Besides that, the following applications also write their log output into a file that is available in the file system of the containers: Application name Log location Core /opt/HCL/wp_profile/logs/WebSphere_Portal and /opt/HCL/AppServer/profiles/cw_profile/logs/server1 Remote Search /opt/HCL/AppServer/profiles/prs_profile/logs/server1 Persistence Node /var/lib/pgsql/11/data/log and /var/lib/pgsql/11/data/dx/repmgr/log Note: The Core and Remote Search have the following default settings for their log output files and that needs to be considered when sizing their persistent volumes: Output file Size per file Files kept SystemOut.log 5MB 3 SystemErr.log 5MB 3 trace.log 20MB 3 The amount of logs that are stored per container in running the Pods depends on the configuration of your Kubernetes Cluster. Refer to the documentation of your cloud provider for further information. Note that for all applications that do not write their logs separately to a file, the only source of historical log data is the Kubernetes logging. We encourage the customers to process the logging of their Kubernetes Cluster in a separate logging solution of their choice. Parent topic: Troubleshooting your Helm deployment","title":"Default log output"},{"location":"containerization/configure_digital_asset_management_in_virtual_portals/","text":"Configure Digital Asset Management in virtual portals This section provides steps to configure Digital Asset Management support in virtual portals. Using HCL Digital Experience 9.5 CF19 and higher, DX administrators can configure Digital Asset Management services support in virtual portals. For additional information about virtual portal capabilities for HCL Digital Experience, reference the virtual portals topic in the HCL DX Help Center documentation. Complete the following tasks to enable Digital Asset Management support in virtual portals. Prerequisites : Install HCL Digital Experience 9.5 CF19 or higher release, including Digital Asset Management and the Experience API. Configure virtual portal support to the HCL DX 9.5 CF19 or higher release deployment. Verify access to the Practitioner Studio interface on the virtual portal. See Enabling Practitioner Studio and Woodburn Studio in an existing virtual portal for additional information. Installing Digital Asset Management on a virtual portal Execute the following configuration tasks to enable Digital Asset Management on the virtual portal: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh enable-media-library-vp -Dstatic.ui.url=... -DVirtualPortalContext=... -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh enable-media-library-vp -Dstatic.ui.url=https://myhost.com/dx/ui/dam/static -DVirtualPortalContext=dam -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin Verify access to the Practitioner Studio and Digital Asset Management features from the HCL DX virtual portal. See the Practitioner Studio and Digital Asset Management topics for additional information. Remove Digital Asset Management from a virtual portal To optionally remove support for Digital Asset Management in an HCL DX virtual portal, execute the following configuration task: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh disable-media-library-vp -DVirtualPortalContext=... -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh disable-media-library-vp -DVirtualPortalContext=dam -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin Parent topic: Customizing your container deployment","title":"Configure Digital Asset Management in virtual portals"},{"location":"containerization/configure_digital_asset_management_in_virtual_portals/#configure-digital-asset-management-in-virtual-portals","text":"This section provides steps to configure Digital Asset Management support in virtual portals. Using HCL Digital Experience 9.5 CF19 and higher, DX administrators can configure Digital Asset Management services support in virtual portals. For additional information about virtual portal capabilities for HCL Digital Experience, reference the virtual portals topic in the HCL DX Help Center documentation.","title":"Configure Digital Asset Management in virtual portals"},{"location":"containerization/configure_digital_asset_management_in_virtual_portals/#complete-the-following-tasks-to-enable-digital-asset-management-support-in-virtual-portals","text":"Prerequisites : Install HCL Digital Experience 9.5 CF19 or higher release, including Digital Asset Management and the Experience API. Configure virtual portal support to the HCL DX 9.5 CF19 or higher release deployment. Verify access to the Practitioner Studio interface on the virtual portal. See Enabling Practitioner Studio and Woodburn Studio in an existing virtual portal for additional information.","title":"Complete the following tasks to enable Digital Asset Management support in virtual portals."},{"location":"containerization/configure_digital_asset_management_in_virtual_portals/#installing-digital-asset-management-on-a-virtual-portal","text":"Execute the following configuration tasks to enable Digital Asset Management on the virtual portal: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh enable-media-library-vp -Dstatic.ui.url=... -DVirtualPortalContext=... -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh enable-media-library-vp -Dstatic.ui.url=https://myhost.com/dx/ui/dam/static -DVirtualPortalContext=dam -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin Verify access to the Practitioner Studio and Digital Asset Management features from the HCL DX virtual portal. See the Practitioner Studio and Digital Asset Management topics for additional information.","title":"Installing Digital Asset Management on a virtual portal"},{"location":"containerization/configure_digital_asset_management_in_virtual_portals/#remove-digital-asset-management-from-a-virtual-portal","text":"To optionally remove support for Digital Asset Management in an HCL DX virtual portal, execute the following configuration task: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh disable-media-library-vp -DVirtualPortalContext=... -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh disable-media-library-vp -DVirtualPortalContext=dam -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin Parent topic: Customizing your container deployment","title":"Remove Digital Asset Management from a virtual portal"},{"location":"containerization/configure_openldap_image/","text":"Configure the OpenLDAP container image Read more about configuring the OpenLDAP container image to the 9.5 container deployment, which is available with HCL Digital Experience 9.5 Container Update release CF181 and later. OpenLDAP Software is an open-source implementation of the Lightweight Directory Access Protocol. For more information on OpenLDAP, visit https://www.openldap.org/ . The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container, and a customization of the operator to deploy the LDAP container and configure the HCL Digital Experience 9.5 Container deployment to use it. Consult the HCL Digital Experience 9.5 Container deployment topic for the latest list of HCL Digital Experience 9.5 container deployments available with your HCL Digital Experience entitlements from the HCL Software License Portal . Note: The OpenLDAP port is not accessible externally on non-OpenShift Kubernetes platforms. This feature is considered for future releases. Usage Deployment of the OpenLDAP container in a production environment is not supported. This optional process of deploying OpenLDAP is solely intended for non-production environments to help one get started with HCL Digital Experience 9.5 container environment deployment. For production environments, use a production-quality LDAP (e.g. one with security hardening, firewall protection, etc). For production use, Administrators can choose to deploy their organization's LDAP (and possibly mirror the contents of that enterprise LDAP back to this newly deployed LDAP) or connect to an already defined LDAP server or database or custom user registry. Note: To use the OpenLDAP container in a Docker (non-Kubernetes) environment, include the following statement in the Docker run command for this image. Example: docker run -e LOCAL=true -p 1389:1389 -p 1636:1636 -p 1666:1666 -v dx-openldap-certs:/var/dx-openldap/certs -v dx-openldap-slapd.d:/var/dx-openldap/etc/openldap/slapd.d -v dx-openldap-ldap:/var/dx-openldap/ldap --name dx_openldap hcl/dx/openldap:v1.0.0-release_20200622_1592846796 Deployment of the HCL OpenLDAP container with custom parameters using dxctl does not configure the OpenLDAP container with custom values. Instead, dxctl deploys and configures OpenLDAP to DX with the default out-of-the-box values. To work around this limitation, and configure OpenLDAP to DX with custom values, do the following steps: Deploy DX without OpenLDAP (set openldap.enabled:false in the deployment.properties file). Ensure the image name, tag, and (if needed) the repository values for the your deployment are correct. These can be updated in the next step. Edit the dx-deployment configmap to add the customization parameters: For example: dx.deploy.openldap.country: US dx.deploy.openldap.org: HCL dx.deploy.openldap.pwd: mycoolnewpwd dx.deploy.openldap.suffix: 'dc=dx,dc=hcl,dc=com' Set openldap.enabled: true in the deployment.properties file. Save the configmap changes. Once these steps are completed, the DX container is recreated to run the OpenLDAP configuration task. For more information on dxctl , see the dxctl topic. Video: Using OpenLDAP with HCL DX 9.5 on Red Hat OpenShift Deployment To deploy the OpenLDAP image container for use with an HCL Digital Experience 9.5 container deployment, first create or update the dx-deployment config map. Note: The config map MUST be named the same as the deployment. By default, the deployment name is dx-deployment, however if you have modified the name in the git_v1_dxdeployment_cr.yaml, your config map name value must match your DxDeployment name value. For example, use my-custom-deployment for the config map name if your git_v1_dxdeployment_cr.yaml was modified as in the sample below: kind: DxDeployment metadata: name: my-custom-deployment The following sample file yaml file deploys the OpenLDAP container with default data and ten (10) sample users with the password passw0rd. The LDAP administrator user name is dx_user with password p0rtal4u. DX is the organization, dc=dx,dc=com is the basedn and US is the country. **kind**: ConfigMap **apiVersion**: v1 **metadata**: **name**: dx-deployment **namespace**: your-namespace **apiVersion**: v1 **data**: dx.deploy.openldap.**enabled**: 'true' dx.deploy.openldap.**image**: dx-openldap dx.deploy.openldap.**tag**: v1.0.0_20200428_1588034443 The following sample yaml file deploys an OpenLDAP container using your custom organization, basedn, country and LDAP administrator password. LDAP administrator user name is dx_user, which cannot be customized at this time. kind: ConfigMap apiVersion: v1 metadata: name: dx-deployment namespace: your-namespace apiVersion: v1 data: dx.deploy.openldap.country: US dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.image: dx-openldap dx.deploy.openldap.org: DX dx.deploy.openldap.pwd: mycoolnewpwd dx.deploy.openldap.suffix: 'dc=dx,dc=hcl,dc=com' dx.deploy.openldap.tag: v1.0.0_20200428_1588034443 Once the OpenLDAP container is fully deployed, the password entry can be removed from the config map. Sample users have a password of passw0rd. Additional users can be added or changes made to the data using ldapadd, ldapdelete and ldapmodify commands according to instructions in the OpenLDAP Administrator Guide . Sample ldif file format for adding users: **dn**: cn=my_new_user_group,ou=groups,dc=dx,dc=hcl,dc=com **objectClass**: groupOfUniqueNames **cn**: my_new_user_group **uniqueMember**: uid=newuser1 **uniqueMember**: uid=newuser2 **dn**: uid=newuser1,ou=users,dc=dx,dc=hcl,dc=com **objectClass**: inetOrgPerson **objectclass**: top **objectclass**: person **objectclass**: organizationalPerson **cn**: my newuser1 **sn**: newuser1 **uid**: newuser1 **userPassword**: passw0rd **givenName**: my **mail**: mynewuser1@dx.hcl.com **preferredLanguage**: en_us **title**: engineer **telephoneNumber**: 9999999999 **dn**: uid=newuser2,ou=users,dc=dx,dc=hcl,dc=com **objectClass**: inetOrgPerson **objectclass**: top **objectclass**: person **objectclass**: organizationalPerson **cn**: my newuser2 **sn**: newuser2 **uid**: newuser2 **userPassword**: passw0rd **givenName**: my **mail**: mynewuser2@dx.hcl.com **preferredLanguage**: en_us **title**: engineer **telephoneNumber**: 9999999999 Copy the ldif file into the OpenLDAP container: kubectl cp ./myldiffile.ldif dx-deployment-openldap-0:/tmp/ Execute into the OpenLDAP container and add the users: kubectl exec -it dx-deployment-openldap-0 /bin/bash cd /var/dx-openldap/bin ./ldapadd -h $HOSTNAME -p 1389 -f /tmp/myldiffile.ldif -x -D cn=dx_user,dc=dx,dc=hcl,dc=com -w p0rtal4u -v The dx.deploy.openldap.enabled config map entry tells the HCL Digital Experience 9.5 container operator to deploy the OpenLDAP container and configure the HCL Digital Experience 9.5 deployed container to it. Note: There are three (3) volumes self-provisioned for the OpenLDAP environment: dx-slapd which maps to the /var/dx-openldap/etc/openldap/slapd.d directory within the container and holds the configuration data; dx-lapd which maps to the /var/dx-openldap/ldap directory within the container and holds the db; and dx-certs which maps to the /var/dx-openldap/certs directory within the container and holds the OpenLDAP TLS certificates. If these volumes are not present, the OpenLDAP data is lost once the container is restarted. Additional configuration options are not currently supported. If you are adding the OpenLDAP container to an existing HCL Digital Experience 9.5 container deployment, you must stop the deployment and restart it with one (1) replica using the removeDx and deployDX scripts. Once fully started, you can safely scale it to N instances. Note: TLS is 'allowed' out of the box with the HCL Digital Experience 9.5 container deployment, but the certificates created with the OpenLDAP container are not imported for use by HCL Digital Experience 9.5 containers. Maintenance and Support HCL Digital Experience will periodically provide updated OpenLDAP container images in the HCL Digital Experience entitlements available for customers from the HCL Software License Portal . HCL Product Support may be contacted with questions regarding this installation procedure. For additional information and support for OpenLDAP topics, please reference https://www.openldap.org/ . Consult the HCL Digital Experience 9.5 Container deployment page for the latest list of Digital Experience 9.5 container components available with your Digital Experience entitlements from the HCL Software License Portal . Parent topic: Customizing your container deployment","title":"Configure the OpenLDAP container image"},{"location":"containerization/configure_openldap_image/#configure-the-openldap-container-image","text":"Read more about configuring the OpenLDAP container image to the 9.5 container deployment, which is available with HCL Digital Experience 9.5 Container Update release CF181 and later. OpenLDAP Software is an open-source implementation of the Lightweight Directory Access Protocol. For more information on OpenLDAP, visit https://www.openldap.org/ . The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container, and a customization of the operator to deploy the LDAP container and configure the HCL Digital Experience 9.5 Container deployment to use it. Consult the HCL Digital Experience 9.5 Container deployment topic for the latest list of HCL Digital Experience 9.5 container deployments available with your HCL Digital Experience entitlements from the HCL Software License Portal . Note: The OpenLDAP port is not accessible externally on non-OpenShift Kubernetes platforms. This feature is considered for future releases.","title":"Configure the OpenLDAP container image"},{"location":"containerization/configure_openldap_image/#usage","text":"Deployment of the OpenLDAP container in a production environment is not supported. This optional process of deploying OpenLDAP is solely intended for non-production environments to help one get started with HCL Digital Experience 9.5 container environment deployment. For production environments, use a production-quality LDAP (e.g. one with security hardening, firewall protection, etc). For production use, Administrators can choose to deploy their organization's LDAP (and possibly mirror the contents of that enterprise LDAP back to this newly deployed LDAP) or connect to an already defined LDAP server or database or custom user registry. Note: To use the OpenLDAP container in a Docker (non-Kubernetes) environment, include the following statement in the Docker run command for this image. Example: docker run -e LOCAL=true -p 1389:1389 -p 1636:1636 -p 1666:1666 -v dx-openldap-certs:/var/dx-openldap/certs -v dx-openldap-slapd.d:/var/dx-openldap/etc/openldap/slapd.d -v dx-openldap-ldap:/var/dx-openldap/ldap --name dx_openldap hcl/dx/openldap:v1.0.0-release_20200622_1592846796 Deployment of the HCL OpenLDAP container with custom parameters using dxctl does not configure the OpenLDAP container with custom values. Instead, dxctl deploys and configures OpenLDAP to DX with the default out-of-the-box values. To work around this limitation, and configure OpenLDAP to DX with custom values, do the following steps: Deploy DX without OpenLDAP (set openldap.enabled:false in the deployment.properties file). Ensure the image name, tag, and (if needed) the repository values for the your deployment are correct. These can be updated in the next step. Edit the dx-deployment configmap to add the customization parameters: For example: dx.deploy.openldap.country: US dx.deploy.openldap.org: HCL dx.deploy.openldap.pwd: mycoolnewpwd dx.deploy.openldap.suffix: 'dc=dx,dc=hcl,dc=com' Set openldap.enabled: true in the deployment.properties file. Save the configmap changes. Once these steps are completed, the DX container is recreated to run the OpenLDAP configuration task. For more information on dxctl , see the dxctl topic. Video: Using OpenLDAP with HCL DX 9.5 on Red Hat OpenShift","title":"Usage"},{"location":"containerization/configure_openldap_image/#deployment","text":"To deploy the OpenLDAP image container for use with an HCL Digital Experience 9.5 container deployment, first create or update the dx-deployment config map. Note: The config map MUST be named the same as the deployment. By default, the deployment name is dx-deployment, however if you have modified the name in the git_v1_dxdeployment_cr.yaml, your config map name value must match your DxDeployment name value. For example, use my-custom-deployment for the config map name if your git_v1_dxdeployment_cr.yaml was modified as in the sample below: kind: DxDeployment metadata: name: my-custom-deployment The following sample file yaml file deploys the OpenLDAP container with default data and ten (10) sample users with the password passw0rd. The LDAP administrator user name is dx_user with password p0rtal4u. DX is the organization, dc=dx,dc=com is the basedn and US is the country. **kind**: ConfigMap **apiVersion**: v1 **metadata**: **name**: dx-deployment **namespace**: your-namespace **apiVersion**: v1 **data**: dx.deploy.openldap.**enabled**: 'true' dx.deploy.openldap.**image**: dx-openldap dx.deploy.openldap.**tag**: v1.0.0_20200428_1588034443 The following sample yaml file deploys an OpenLDAP container using your custom organization, basedn, country and LDAP administrator password. LDAP administrator user name is dx_user, which cannot be customized at this time. kind: ConfigMap apiVersion: v1 metadata: name: dx-deployment namespace: your-namespace apiVersion: v1 data: dx.deploy.openldap.country: US dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.image: dx-openldap dx.deploy.openldap.org: DX dx.deploy.openldap.pwd: mycoolnewpwd dx.deploy.openldap.suffix: 'dc=dx,dc=hcl,dc=com' dx.deploy.openldap.tag: v1.0.0_20200428_1588034443 Once the OpenLDAP container is fully deployed, the password entry can be removed from the config map. Sample users have a password of passw0rd. Additional users can be added or changes made to the data using ldapadd, ldapdelete and ldapmodify commands according to instructions in the OpenLDAP Administrator Guide . Sample ldif file format for adding users: **dn**: cn=my_new_user_group,ou=groups,dc=dx,dc=hcl,dc=com **objectClass**: groupOfUniqueNames **cn**: my_new_user_group **uniqueMember**: uid=newuser1 **uniqueMember**: uid=newuser2 **dn**: uid=newuser1,ou=users,dc=dx,dc=hcl,dc=com **objectClass**: inetOrgPerson **objectclass**: top **objectclass**: person **objectclass**: organizationalPerson **cn**: my newuser1 **sn**: newuser1 **uid**: newuser1 **userPassword**: passw0rd **givenName**: my **mail**: mynewuser1@dx.hcl.com **preferredLanguage**: en_us **title**: engineer **telephoneNumber**: 9999999999 **dn**: uid=newuser2,ou=users,dc=dx,dc=hcl,dc=com **objectClass**: inetOrgPerson **objectclass**: top **objectclass**: person **objectclass**: organizationalPerson **cn**: my newuser2 **sn**: newuser2 **uid**: newuser2 **userPassword**: passw0rd **givenName**: my **mail**: mynewuser2@dx.hcl.com **preferredLanguage**: en_us **title**: engineer **telephoneNumber**: 9999999999 Copy the ldif file into the OpenLDAP container: kubectl cp ./myldiffile.ldif dx-deployment-openldap-0:/tmp/ Execute into the OpenLDAP container and add the users: kubectl exec -it dx-deployment-openldap-0 /bin/bash cd /var/dx-openldap/bin ./ldapadd -h $HOSTNAME -p 1389 -f /tmp/myldiffile.ldif -x -D cn=dx_user,dc=dx,dc=hcl,dc=com -w p0rtal4u -v The dx.deploy.openldap.enabled config map entry tells the HCL Digital Experience 9.5 container operator to deploy the OpenLDAP container and configure the HCL Digital Experience 9.5 deployed container to it. Note: There are three (3) volumes self-provisioned for the OpenLDAP environment: dx-slapd which maps to the /var/dx-openldap/etc/openldap/slapd.d directory within the container and holds the configuration data; dx-lapd which maps to the /var/dx-openldap/ldap directory within the container and holds the db; and dx-certs which maps to the /var/dx-openldap/certs directory within the container and holds the OpenLDAP TLS certificates. If these volumes are not present, the OpenLDAP data is lost once the container is restarted. Additional configuration options are not currently supported. If you are adding the OpenLDAP container to an existing HCL Digital Experience 9.5 container deployment, you must stop the deployment and restart it with one (1) replica using the removeDx and deployDX scripts. Once fully started, you can safely scale it to N instances. Note: TLS is 'allowed' out of the box with the HCL Digital Experience 9.5 container deployment, but the certificates created with the OpenLDAP container are not imported for use by HCL Digital Experience 9.5 containers.","title":"Deployment"},{"location":"containerization/configure_openldap_image/#maintenance-and-support","text":"HCL Digital Experience will periodically provide updated OpenLDAP container images in the HCL Digital Experience entitlements available for customers from the HCL Software License Portal . HCL Product Support may be contacted with questions regarding this installation procedure. For additional information and support for OpenLDAP topics, please reference https://www.openldap.org/ . Consult the HCL Digital Experience 9.5 Container deployment page for the latest list of Digital Experience 9.5 container components available with your Digital Experience entitlements from the HCL Software License Portal . Parent topic: Customizing your container deployment","title":"Maintenance and Support"},{"location":"containerization/container_init_performance/","text":"HCL Digital Experience 9.5 Docker and Container initialization performance Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms. Introduction When deployed on the supported Red Hat OpenShift and Kubernetes environments, the HCL Digital Experience core platform \"Pod\" must be started before it can start serving requests. Furthermore, once the Pod is started, HCL Digital Experience Portal and Web Content Manager core must be initialized before the OpenShift or Kubernetes \u201creadiness\u201d probes can determine that the Pod is able to serve requests. The OpenShift or Kubernetes readiness probe functions to execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds.The HCL DX core must be initialized by execution of the \"startServer.sh WebSphere_Portal\" process before the readiness probes completes successfully. This starts the IBM WebSphere Application Server profile containing HCL DX Portal and Web Content Manager. This initialization process includes several IBM WebSphere and HCL DX applications including select portlets that must initialize before the HCL DX Core can respond to the readiness probe. In DX 9.5 Container Update CF192 and higher, to support a faster initialization of DX core Portal and Web Content Manager in Docker, Red Hat OpenShift, and supported Kubernetes platforms, most DX portlets not required for initial operations will default to a \"lazy load\" initialization. Using this means of initialization, an HCL DX portlet application is not started by a user request, but by the first standard HTTP request that occurs and renders a DX portal page that contains the portlet application on the server. Direct access to the portlet, for example an Ajax request, does not start the portlet. In addition, some IBM WebSphere applications not required for initial operations will not be autostarted. ConfigEngine Tasks Three ConfigEngine tasks are deployed to support improvements to HCL DX Core initialization times. They are: stop-autostart-docker-applications default -autostart-docker-applications start-advanced-editor-applications stop-autostart-docker-applications The stop-autostart-docker-applications task is executed during the Docker image build for DX Core when initialized on Docker, Red Hat OpenShift, or Kubernetes platforms. This task manages the following functions: It will stop the Advanced Rich Text Editor.ear (Textbox.io) files from autostarting. It will \u201clazy load\u201d all DX portlets that do not require the Portal and WCM functions to operate. Portlets required for DX operations that will be loaded and initialized include for example, theme modules that are loaded from Portlets. These portlets must be started in order for the theme modules to load. The \"Login\" and \"WCM Local Rendering\" portlets are in also this list as they are required to present the Woodburn Studio demonstration site entry page, and therefore the Kubernetes readiness probe. Note that the readiness probe defaults to the WebSphere Application Console via probe functions that execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds. See Figure: Configure Editor Options the list of portlets that are needed for DX operations and will automatically load. default-autostart-docker-applications The default-autostart-docker-applications task will restore the autostart status of all applications to their \u201cout of the box\u201d status (and not apply \u201cLazy load\u201d initialization functions). start-advanced-editor-applications The start-advanced-editor-applications task will start the Advanced Rich Text Editor.ear (Textbox.io) application, if required, and if the customer configures WCM to use the WCM Advanced Editor. Prerequisites: Portal Administrators should run the start-advanced-editor-applications task to start the Advanced Rich Text Editor.ear (Textbox.io) application, then proceed to select the Advanced Editor in the Web Content Manager Authoring > Configure > Editor Options interface. Important Considerations and Limitations of the Container Initialization Improvements As a result of not autostarting, these applications and portlets, initialization of DX Portal may be faster, but the initial access of most pages will initially be slower due to the fact that the application/portlet must now be initialized. Note this only affects the first access of that application/portlet (as initialization is a once per system activity). As new DX PODS are started, initialization of DX pages with non-required applications and portlets will be slower on first HTTP request. Using Advanced Editors for WCM As noted above, beginning with Container Update CF192, and default settings for \u2018lazy load\u2019 of non-required portlets and applications, the Advanced Rich Text Editor .ear Textbox.io for WCM is now NOT started. Since this is not a lazy load but rather a stop of the Advanced Rich Text Editor Textbox IO EAR containing the advanced editor, they must also start the Advanced Rich Text Editor EAR by running the \"start-advanced-editor-applications\" task, before configuring the Advanced Rich Text editor in the Web Content Manager configuration settings , to make the editor available for content authors. It is not necessary in addition to \"commit\u201d the new Docker images once this task completes, because these changes are in the profile which is persisted in an external volume and not in the Docker image. Source File listing of HCL DX required portlets and applications that will autostart: These configuration tasks use four files to obtain the list of HCL DX portlet/applications to autostart. All the files are located in the same directory: configuration root}/PortalServer/installer/wp.config/config/includes For example, a list on this reference system is are located at: /opt/HCL/PortalServer/installer/wp.config/config/includes The four import files are: advancedEditorEAR - Enable the Advanced Editors defaultListOfEnabledApps - The out of the box autostart parameters listOfAppsDockerDisable - List of all portlets and applications who autostart is initially disabled listOfAppsDockerEnable - List of portlets and applications to autostart after having disabled the one in listOfAppsDockerDisable *List of portlets and applications that are automatically initialized by default * (Container Update CF192 release and later): PA_Login_Portlet_App PA_Site_Builder PA_WCMLRingPortJSR286 PA_WCM_Authoring_UI PA_Pingpageproperties PA_Styles PA_Wiring PA_wp.pzn.ui.actions PA_Orphaned PA_VanityUrl PA_New_Page PA_Create_Content PA_Applications PA_Toolbar_Content PA_Toolbar_SiteMap PA_Impersonation PA_WebScanner PA_Theme_Creator *List of portlets and applications initialized via \u201clazy load\u201d * (Container Update CF192 release and later): AJAX Proxy Configuration Default_Theme_85 Dojo_Resources EphoxEditLive ibmasyncrsp isclite JavaContentRepository jQuery_Resources Live_Object_Framework lwp.addtosametimelist_war lwp_groupsViewer_war lwp_peoplefinder_war lwp_peoplePicker_war MashupCommonComponent Mobile_Preview PA_Applications PA_AtiveSiteAnalytics PA_Banner_Ad PA_Blurb_1 PA_CM_Picker PA_ContactList PA_Create_Content PA_CredVaultDialog PA_Dialog_Stack PA_Dialog_State PA_DynamicUIApp PA_EitThemeProperties PA_FedDocumentsPicker PA_Feed_Service_Admin PA_FS_Disambiguation PA_Impersonation PA_IWidget_Wrapper PA_Login_Portlet_App PA_MosoftExchange2010 PA_New_Page PA_Orphaned PA_Page_Picker PA_Pingpageproperties PA_Pmizationframework PA_PortalWSRPProxy PA_PTransformationApp PA_Search_Center PA_SearchSitemapPort PA_Selfcare_Port_App PA_Site_Builder PA_spa PA_Styles PA_Tag_Cloud PA_Theme_Creator PA_Theme_Manager PA_Toolbar_Content PA_Toolbar_SiteMap PA_VanityUrl PA_WCM_Authoring_UI PA_WCMLRingPortJSR286 PA_WCMSupportTools PA_WebDockPortServlet PA_WebScanner PA_Wiring PA_WPF PA_wp.feedspace PA_wp.pzn.ui.actions PA_wp.vwat.manager Personalization_Lists Personalization_Workspace Practitioner_Studio_Theme_95 PSESearchAdapter pznpublish pznscheduler PZN_Utilities Quickr_Document_Picker RESTAPIDocs Seedlist_Servlet Simple_Theme SpellChecker StartupCheck SwaggerUI Theme_Dev_Assets ThemeDevSite Theme_Modules TinyEditorsServices TinyEditorsTextboxio Toolbar_Modules Toolbar_Theme_85 UserProfileRESTServlet wci wcm WCM_EXTENSION wcm-remote-admin-ejb Woodburn_Studio_Theme_95 worklight_extension wp.scriptportlet.editor wp.scriptportlet.importexport wps wps_scheduler wp.theme.ckeditor.ear wp.theme.toolbar.xslt wp.vwat.servlet.ear WSPolicyManager Parent topic: Container administration 9.5","title":"HCL Digital Experience 9.5 Docker and Container initialization performance"},{"location":"containerization/container_init_performance/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms.","title":"HCL Digital Experience 9.5 Docker and Container initialization performance"},{"location":"containerization/container_init_performance/#introduction","text":"When deployed on the supported Red Hat OpenShift and Kubernetes environments, the HCL Digital Experience core platform \"Pod\" must be started before it can start serving requests. Furthermore, once the Pod is started, HCL Digital Experience Portal and Web Content Manager core must be initialized before the OpenShift or Kubernetes \u201creadiness\u201d probes can determine that the Pod is able to serve requests. The OpenShift or Kubernetes readiness probe functions to execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds.The HCL DX core must be initialized by execution of the \"startServer.sh WebSphere_Portal\" process before the readiness probes completes successfully. This starts the IBM WebSphere Application Server profile containing HCL DX Portal and Web Content Manager. This initialization process includes several IBM WebSphere and HCL DX applications including select portlets that must initialize before the HCL DX Core can respond to the readiness probe. In DX 9.5 Container Update CF192 and higher, to support a faster initialization of DX core Portal and Web Content Manager in Docker, Red Hat OpenShift, and supported Kubernetes platforms, most DX portlets not required for initial operations will default to a \"lazy load\" initialization. Using this means of initialization, an HCL DX portlet application is not started by a user request, but by the first standard HTTP request that occurs and renders a DX portal page that contains the portlet application on the server. Direct access to the portlet, for example an Ajax request, does not start the portlet. In addition, some IBM WebSphere applications not required for initial operations will not be autostarted.","title":"Introduction"},{"location":"containerization/container_init_performance/#configengine-tasks","text":"Three ConfigEngine tasks are deployed to support improvements to HCL DX Core initialization times. They are: stop-autostart-docker-applications default -autostart-docker-applications start-advanced-editor-applications","title":"ConfigEngine Tasks"},{"location":"containerization/container_init_performance/#stop-autostart-docker-applications","text":"The stop-autostart-docker-applications task is executed during the Docker image build for DX Core when initialized on Docker, Red Hat OpenShift, or Kubernetes platforms. This task manages the following functions: It will stop the Advanced Rich Text Editor.ear (Textbox.io) files from autostarting. It will \u201clazy load\u201d all DX portlets that do not require the Portal and WCM functions to operate. Portlets required for DX operations that will be loaded and initialized include for example, theme modules that are loaded from Portlets. These portlets must be started in order for the theme modules to load. The \"Login\" and \"WCM Local Rendering\" portlets are in also this list as they are required to present the Woodburn Studio demonstration site entry page, and therefore the Kubernetes readiness probe. Note that the readiness probe defaults to the WebSphere Application Console via probe functions that execute an HTTP request to the \"/wps/portal\" or \"/ibm/console\" page and ensures that it responds. See Figure: Configure Editor Options the list of portlets that are needed for DX operations and will automatically load.","title":"stop-autostart-docker-applications"},{"location":"containerization/container_init_performance/#default-autostart-docker-applications","text":"The default-autostart-docker-applications task will restore the autostart status of all applications to their \u201cout of the box\u201d status (and not apply \u201cLazy load\u201d initialization functions).","title":"default-autostart-docker-applications"},{"location":"containerization/container_init_performance/#start-advanced-editor-applications","text":"The start-advanced-editor-applications task will start the Advanced Rich Text Editor.ear (Textbox.io) application, if required, and if the customer configures WCM to use the WCM Advanced Editor. Prerequisites: Portal Administrators should run the start-advanced-editor-applications task to start the Advanced Rich Text Editor.ear (Textbox.io) application, then proceed to select the Advanced Editor in the Web Content Manager Authoring > Configure > Editor Options interface.","title":"start-advanced-editor-applications"},{"location":"containerization/container_init_performance/#important-considerations-and-limitations-of-the-container-initialization-improvements","text":"As a result of not autostarting, these applications and portlets, initialization of DX Portal may be faster, but the initial access of most pages will initially be slower due to the fact that the application/portlet must now be initialized. Note this only affects the first access of that application/portlet (as initialization is a once per system activity). As new DX PODS are started, initialization of DX pages with non-required applications and portlets will be slower on first HTTP request.","title":"Important Considerations and Limitations of the Container Initialization Improvements"},{"location":"containerization/container_init_performance/#using-advanced-editors-for-wcm","text":"As noted above, beginning with Container Update CF192, and default settings for \u2018lazy load\u2019 of non-required portlets and applications, the Advanced Rich Text Editor .ear Textbox.io for WCM is now NOT started. Since this is not a lazy load but rather a stop of the Advanced Rich Text Editor Textbox IO EAR containing the advanced editor, they must also start the Advanced Rich Text Editor EAR by running the \"start-advanced-editor-applications\" task, before configuring the Advanced Rich Text editor in the Web Content Manager configuration settings , to make the editor available for content authors. It is not necessary in addition to \"commit\u201d the new Docker images once this task completes, because these changes are in the profile which is persisted in an external volume and not in the Docker image.","title":"Using Advanced Editors for WCM"},{"location":"containerization/container_init_performance/#source-file-listing-of-hcl-dx-required-portlets-and-applications-that-will-autostart","text":"These configuration tasks use four files to obtain the list of HCL DX portlet/applications to autostart. All the files are located in the same directory: configuration root}/PortalServer/installer/wp.config/config/includes For example, a list on this reference system is are located at: /opt/HCL/PortalServer/installer/wp.config/config/includes The four import files are: advancedEditorEAR - Enable the Advanced Editors defaultListOfEnabledApps - The out of the box autostart parameters listOfAppsDockerDisable - List of all portlets and applications who autostart is initially disabled listOfAppsDockerEnable - List of portlets and applications to autostart after having disabled the one in listOfAppsDockerDisable *List of portlets and applications that are automatically initialized by default * (Container Update CF192 release and later): PA_Login_Portlet_App PA_Site_Builder PA_WCMLRingPortJSR286 PA_WCM_Authoring_UI PA_Pingpageproperties PA_Styles PA_Wiring PA_wp.pzn.ui.actions PA_Orphaned PA_VanityUrl PA_New_Page PA_Create_Content PA_Applications PA_Toolbar_Content PA_Toolbar_SiteMap PA_Impersonation PA_WebScanner PA_Theme_Creator *List of portlets and applications initialized via \u201clazy load\u201d * (Container Update CF192 release and later): AJAX Proxy Configuration Default_Theme_85 Dojo_Resources EphoxEditLive ibmasyncrsp isclite JavaContentRepository jQuery_Resources Live_Object_Framework lwp.addtosametimelist_war lwp_groupsViewer_war lwp_peoplefinder_war lwp_peoplePicker_war MashupCommonComponent Mobile_Preview PA_Applications PA_AtiveSiteAnalytics PA_Banner_Ad PA_Blurb_1 PA_CM_Picker PA_ContactList PA_Create_Content PA_CredVaultDialog PA_Dialog_Stack PA_Dialog_State PA_DynamicUIApp PA_EitThemeProperties PA_FedDocumentsPicker PA_Feed_Service_Admin PA_FS_Disambiguation PA_Impersonation PA_IWidget_Wrapper PA_Login_Portlet_App PA_MosoftExchange2010 PA_New_Page PA_Orphaned PA_Page_Picker PA_Pingpageproperties PA_Pmizationframework PA_PortalWSRPProxy PA_PTransformationApp PA_Search_Center PA_SearchSitemapPort PA_Selfcare_Port_App PA_Site_Builder PA_spa PA_Styles PA_Tag_Cloud PA_Theme_Creator PA_Theme_Manager PA_Toolbar_Content PA_Toolbar_SiteMap PA_VanityUrl PA_WCM_Authoring_UI PA_WCMLRingPortJSR286 PA_WCMSupportTools PA_WebDockPortServlet PA_WebScanner PA_Wiring PA_WPF PA_wp.feedspace PA_wp.pzn.ui.actions PA_wp.vwat.manager Personalization_Lists Personalization_Workspace Practitioner_Studio_Theme_95 PSESearchAdapter pznpublish pznscheduler PZN_Utilities Quickr_Document_Picker RESTAPIDocs Seedlist_Servlet Simple_Theme SpellChecker StartupCheck SwaggerUI Theme_Dev_Assets ThemeDevSite Theme_Modules TinyEditorsServices TinyEditorsTextboxio Toolbar_Modules Toolbar_Theme_85 UserProfileRESTServlet wci wcm WCM_EXTENSION wcm-remote-admin-ejb Woodburn_Studio_Theme_95 worklight_extension wp.scriptportlet.editor wp.scriptportlet.importexport wps wps_scheduler wp.theme.ckeditor.ear wp.theme.toolbar.xslt wp.vwat.servlet.ear WSPolicyManager Parent topic: Container administration 9.5","title":"Source File listing of HCL DX required portlets and applications that will autostart:"},{"location":"containerization/container_staging/","text":"Container Staging This section describes how to move from an existing HCL Portal environment to a containerized Digital Experience environment. Prerequisite The target environment that is existing in a customer-owned Kubernetes environment requires HCL Digital Experience 9.5 and IBM WebSphere Application Server 9.0.5. The HCL Digital Experience and IBM WebSphere Application Server product versions for the source and target environment must be at the same level, though it is sufficient to be on IBM WebSphere Application Server 8.5.5.x with JDK 8. Export the source HCL Portal server Follow these steps to export the source HCL Portal server. Upgrade the source environment. Using the IBM Installation Manager, upgrade the HCL Portal product to CF17 or later and HCL Portal 9.5. Log in to the machine where the source environment is located and set the ulimit -n to 24000 . For example, ulimit - n 24000 . Verify that the HCL Portal server is started. Navigate to the PortalServer/bin directory to export the base server. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://mysource.machine.fqdn:10039/wps/config -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/ExportRelease.xml -out /tmp/ExportReleaseResults.xml Save the output XML file (ExportReleaseResults.xml) to an external or shared drive, for use later when importing to the target environment. Export the content for each Virtual Portal that exists in the source environment, renaming each file uniquely for easy identification. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://mysource.machine.fqdn:10039/wps/config/vpcontextroot -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/ExportUniqueRelease.xml -out /tmp/ExportVP1Results.xml Save the Virtual Portal output files to an external or shared drive for later use when importing to the target environment. Save the /opt/HCL/wp_profile/PortalServer/deployed/archive directory files to an external or shared drive, for later use when importing to the target environment. If you are using PZN rules, export the PZN rules using the Personalization Administration Portlet functions and save the generated Workspace.nodes file to an external or shared drive, for later use when importing to the target environment. Log in to the HCL Portal Home Page. Navigate to Personalization > Business Rules > Extra Actions > Export . Save the output file. When applicable, save all custom files (application and theme EAR files, WAR files) to an external or shared drive, for use later when importing to the target environment. Import the source HCL Portal server Log in to the machine from where you will access your HCL Portal Container. Log in to the machine from where you will access your HCL Portal Container. Download, install, and log in to the command line client for your Kubernetes environment according to the client instructions. For OpenShift, that is Red Hat OpenShift Command Line Client. For Non OpenShift, that is the Kubectl command line tool. With only a single instance of an HCL Portal container running, exec in, and ensure the ulimit -n value is at least 24000 . Empty the base HCL Portal server. OpenShift: oc exec \u2013it dx-deployment-nnnnn /bin/bash Non OpenShift: kubectl exec \u2013it dx-deployment-nnnnn /bin/bash 2. ``` /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh empty-portal \u2013DWasPassword= \u2013DPortalAdminPwd= The output displays a **BUILD SUCCESSFUL** message. If not, check the /opt/HCL/wp\\_profile/ConfigEngine/log/ConfigTrace.log for errors. 5. Clean up the remaining content in the target server by using XML Access: ``` /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/Task.xml -out /tmp/task_result.xml ``` The output displays a **BUILD SUCCESSFUL** message. If not, check the /opt/HCL/wp\\_profile/logs/WebSphere\\_Portal/SystemOut.log for errors. 6. Copy the output XML files, custom EAR and WAR files, Workspace.nodes file, and the ../deployed/archive directory files to a location on this local machine, making sure to preserve the file names and structure from the external or shared drive and then into the DX container. 1. ``` cp /drive/* /tmp/ 2. OpenShift: ``` oc cp /tmp/* dx-deployment-nnnnn:/tmp/ ``` Non OpenShift: ``` kubectl cp /tmp/* dx-deployment-nnnnn:/tmp/ ``` Create a directory under /opt/HCL/wp_profile to house any custom code or shared libraries. ``` mkdir \u2013p /opt/HCL/wp_profile/customApps **Note:** In any containerized environment, all custom code and shared libraries need to exist under the persisted profile volume. 8. Move the copied files to the appropriate locations in the container. 1. ``` mv /tmp/custom.ear /opt/HCL/wp_profile/customApps/ 2. ``` mv /tmp/deployed/archive/* /opt/HCL/wp_profile/PortalServer/deployed/archive/ 9. Deploy custom applications, predeployed portlets, or themes. 10. Configure any required syndication properties in the WCM ConfigService. For example, enabling memberfixer to run during syndication. 11. Create any required configuration items. For example, URLs, namespace bindings, etc. 12. Import the source server base content into the HCL Portal server in the container. 1. OpenShift: ``` oc exec \u2013it dx-deployment-nnnnn /bin/bash ``` Non OpenShift: ``` kubectl exec \u2013it dx-deployment-nnnnn /bin/bash ``` 2. ``` /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config -user <your DX admin user> -password <your DX admin user password> -in /tmp/ExportReleaseResults.xml -out /tmp/ExportReleaseResults_ImportResult.xml The output displays a successful execution. If not, check /tmp/ExportReleaseResults\\_ImportResult.xml for errors. Update the WCM content in the HCL Portal server instance: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh update-wcm -DWasPassword=<your WAS admin user password> -DPortalAdminPwd=<your DX admin user password> The output displays a BUILD SUCCESSFUL message. If not, check the /opt/HCL/wp_profile/ConfigEngine/log/ConfigTrace.log for errors. If you are using PZN rules, import the PZN rules by using the Personalization Administration Portlet functions. Log in to the HCL Portal home page. Navigate to Personalization > Business Rules > Extra Actions > Import . Browse to the /tmp/Workspace.nodes file and click Import . Log in to the HCL Portal home page and verify that the base server is functioning correctly: http://my.target.fqdn/wps/portal Check the /opt/HCL/wp_profile/logs/WebSphere_Portal/SystemOut.log to ensure that there are no startup errors. Create all of your Virtual Portals. /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh create-virtual-portal -DWasPassword=<your WAS admin user password> -DPortalAdminPwd=<your DX admin user password> -DVirtualPortalTitle=VirtualPortal1 -DVirtualPortalRealm=VirtualPortal1Realm -DVirtualPortalContext=VirtualPortal1 For each Virtual Portal, import the content using XML Access. Make sure that the context root and the Virtual Portal name both match in the XML Access command. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config/VirtualPortal1 -user <your DX admin user> -password <your DX admin user password> -in /tmp/ExportVP1Results.xml -out /tmp/ExportVP1Results_ImportResults.xml Restart the HCL Portal server and check /opt/HCL/wp_profile/logs/WebSphere_Portal/SystemOut.log to ensure no startup errors. Syndicate the source and target environments Follow these steps to syndicate the source and target environments. Note: If you have larger libraries, the default database must be transferred to any of the supported databases. For information about supported databases, see Database Management Systems . If you want to know more about transferring the default database of the DX 9.5 Container to IBM DB2, see Transfer HCL Digital Experience 9.5 container default database to IBM DB2 . Since the Kubernetes deployment typically allows only SSL traffic, you need to update the SSL Signer certificates for the Syndicator and Subscriber setups so that they can communicate with each other. To do this, log into the WAS console ( https://machine_name/ibm/console or https://machine_name:port/ibm/console ) and go to the Signer certificates page that is available in the Security > SSL certificate and key management menu: Select Retrieve from port and create the signer certificate, and then save the certificate. Log in to HCL Portal instance to configure syndication: http://my.target.fqdn/wps/portal. Navigate to Administration > Security > Credential Vault > Add a Vault Slot . On the Credential Vault page, select New and provide the following: Name - enter the name for the vault slot. Vault resource associated with vault slot - select new and enter the vault resource name. Vault slot is shared check box - tick this check box and provide the credentials for a user that has appropriate access on the source/syndication system: Shared userid , Shared password , and Confirm password . Click OK to save the changes. Navigate to Portal Content > Subscribers . Click Subscribe Now . In the Subscribe to a syndicator pop-up, provide the following: Syndicator URL Syndicator Name Subscriber Name Credential Vault Slot created in step 2. Click Next . Select the libraries to syndicate and the Scope of the syndication. Click Finish . If you have Virtual Portals, you must repeat the syndication steps for each Virtual Portal. If needed, configure library permissions when syndication is completed. Note: As with syndication between on-premise setups, it is possible to do a one-way syndication from an earlier to a later release. You do not need to disable Practitioner Studio to do this syndication. Parent topic: Digital Experience Application deployment","title":"Container Staging"},{"location":"containerization/container_staging/#container-staging","text":"This section describes how to move from an existing HCL Portal environment to a containerized Digital Experience environment.","title":"Container Staging"},{"location":"containerization/container_staging/#prerequisite","text":"The target environment that is existing in a customer-owned Kubernetes environment requires HCL Digital Experience 9.5 and IBM WebSphere Application Server 9.0.5. The HCL Digital Experience and IBM WebSphere Application Server product versions for the source and target environment must be at the same level, though it is sufficient to be on IBM WebSphere Application Server 8.5.5.x with JDK 8.","title":"Prerequisite"},{"location":"containerization/container_staging/#export-the-source-hcl-portal-server","text":"Follow these steps to export the source HCL Portal server. Upgrade the source environment. Using the IBM Installation Manager, upgrade the HCL Portal product to CF17 or later and HCL Portal 9.5. Log in to the machine where the source environment is located and set the ulimit -n to 24000 . For example, ulimit - n 24000 . Verify that the HCL Portal server is started. Navigate to the PortalServer/bin directory to export the base server. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://mysource.machine.fqdn:10039/wps/config -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/ExportRelease.xml -out /tmp/ExportReleaseResults.xml Save the output XML file (ExportReleaseResults.xml) to an external or shared drive, for use later when importing to the target environment. Export the content for each Virtual Portal that exists in the source environment, renaming each file uniquely for easy identification. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://mysource.machine.fqdn:10039/wps/config/vpcontextroot -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/ExportUniqueRelease.xml -out /tmp/ExportVP1Results.xml Save the Virtual Portal output files to an external or shared drive for later use when importing to the target environment. Save the /opt/HCL/wp_profile/PortalServer/deployed/archive directory files to an external or shared drive, for later use when importing to the target environment. If you are using PZN rules, export the PZN rules using the Personalization Administration Portlet functions and save the generated Workspace.nodes file to an external or shared drive, for later use when importing to the target environment. Log in to the HCL Portal Home Page. Navigate to Personalization > Business Rules > Extra Actions > Export . Save the output file. When applicable, save all custom files (application and theme EAR files, WAR files) to an external or shared drive, for use later when importing to the target environment.","title":"Export the source HCL Portal server"},{"location":"containerization/container_staging/#import-the-source-hcl-portal-server","text":"Log in to the machine from where you will access your HCL Portal Container. Log in to the machine from where you will access your HCL Portal Container. Download, install, and log in to the command line client for your Kubernetes environment according to the client instructions. For OpenShift, that is Red Hat OpenShift Command Line Client. For Non OpenShift, that is the Kubectl command line tool. With only a single instance of an HCL Portal container running, exec in, and ensure the ulimit -n value is at least 24000 . Empty the base HCL Portal server. OpenShift: oc exec \u2013it dx-deployment-nnnnn /bin/bash Non OpenShift: kubectl exec \u2013it dx-deployment-nnnnn /bin/bash 2. ``` /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh empty-portal \u2013DWasPassword= \u2013DPortalAdminPwd= The output displays a **BUILD SUCCESSFUL** message. If not, check the /opt/HCL/wp\\_profile/ConfigEngine/log/ConfigTrace.log for errors. 5. Clean up the remaining content in the target server by using XML Access: ``` /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config -user <your DX admin user> \u2013password <your DX admin user password> \u2013in /opt/HCL/PortalServer/doc/xml-samples/Task.xml -out /tmp/task_result.xml ``` The output displays a **BUILD SUCCESSFUL** message. If not, check the /opt/HCL/wp\\_profile/logs/WebSphere\\_Portal/SystemOut.log for errors. 6. Copy the output XML files, custom EAR and WAR files, Workspace.nodes file, and the ../deployed/archive directory files to a location on this local machine, making sure to preserve the file names and structure from the external or shared drive and then into the DX container. 1. ``` cp /drive/* /tmp/ 2. OpenShift: ``` oc cp /tmp/* dx-deployment-nnnnn:/tmp/ ``` Non OpenShift: ``` kubectl cp /tmp/* dx-deployment-nnnnn:/tmp/ ``` Create a directory under /opt/HCL/wp_profile to house any custom code or shared libraries. ``` mkdir \u2013p /opt/HCL/wp_profile/customApps **Note:** In any containerized environment, all custom code and shared libraries need to exist under the persisted profile volume. 8. Move the copied files to the appropriate locations in the container. 1. ``` mv /tmp/custom.ear /opt/HCL/wp_profile/customApps/ 2. ``` mv /tmp/deployed/archive/* /opt/HCL/wp_profile/PortalServer/deployed/archive/ 9. Deploy custom applications, predeployed portlets, or themes. 10. Configure any required syndication properties in the WCM ConfigService. For example, enabling memberfixer to run during syndication. 11. Create any required configuration items. For example, URLs, namespace bindings, etc. 12. Import the source server base content into the HCL Portal server in the container. 1. OpenShift: ``` oc exec \u2013it dx-deployment-nnnnn /bin/bash ``` Non OpenShift: ``` kubectl exec \u2013it dx-deployment-nnnnn /bin/bash ``` 2. ``` /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config -user <your DX admin user> -password <your DX admin user password> -in /tmp/ExportReleaseResults.xml -out /tmp/ExportReleaseResults_ImportResult.xml The output displays a successful execution. If not, check /tmp/ExportReleaseResults\\_ImportResult.xml for errors. Update the WCM content in the HCL Portal server instance: /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh update-wcm -DWasPassword=<your WAS admin user password> -DPortalAdminPwd=<your DX admin user password> The output displays a BUILD SUCCESSFUL message. If not, check the /opt/HCL/wp_profile/ConfigEngine/log/ConfigTrace.log for errors. If you are using PZN rules, import the PZN rules by using the Personalization Administration Portlet functions. Log in to the HCL Portal home page. Navigate to Personalization > Business Rules > Extra Actions > Import . Browse to the /tmp/Workspace.nodes file and click Import . Log in to the HCL Portal home page and verify that the base server is functioning correctly: http://my.target.fqdn/wps/portal Check the /opt/HCL/wp_profile/logs/WebSphere_Portal/SystemOut.log to ensure that there are no startup errors. Create all of your Virtual Portals. /opt/HCL/wp_profile/ConfigEngine/ConfigEngine.sh create-virtual-portal -DWasPassword=<your WAS admin user password> -DPortalAdminPwd=<your DX admin user password> -DVirtualPortalTitle=VirtualPortal1 -DVirtualPortalRealm=VirtualPortal1Realm -DVirtualPortalContext=VirtualPortal1 For each Virtual Portal, import the content using XML Access. Make sure that the context root and the Virtual Portal name both match in the XML Access command. /opt/HCL/wp_profile/PortalServer/bin/xmlaccess.sh -url http://my.target.fqdn/wps/config/VirtualPortal1 -user <your DX admin user> -password <your DX admin user password> -in /tmp/ExportVP1Results.xml -out /tmp/ExportVP1Results_ImportResults.xml Restart the HCL Portal server and check /opt/HCL/wp_profile/logs/WebSphere_Portal/SystemOut.log to ensure no startup errors.","title":"Import the source HCL Portal server"},{"location":"containerization/container_staging/#syndicate-the-source-and-target-environments","text":"Follow these steps to syndicate the source and target environments. Note: If you have larger libraries, the default database must be transferred to any of the supported databases. For information about supported databases, see Database Management Systems . If you want to know more about transferring the default database of the DX 9.5 Container to IBM DB2, see Transfer HCL Digital Experience 9.5 container default database to IBM DB2 . Since the Kubernetes deployment typically allows only SSL traffic, you need to update the SSL Signer certificates for the Syndicator and Subscriber setups so that they can communicate with each other. To do this, log into the WAS console ( https://machine_name/ibm/console or https://machine_name:port/ibm/console ) and go to the Signer certificates page that is available in the Security > SSL certificate and key management menu: Select Retrieve from port and create the signer certificate, and then save the certificate. Log in to HCL Portal instance to configure syndication: http://my.target.fqdn/wps/portal. Navigate to Administration > Security > Credential Vault > Add a Vault Slot . On the Credential Vault page, select New and provide the following: Name - enter the name for the vault slot. Vault resource associated with vault slot - select new and enter the vault resource name. Vault slot is shared check box - tick this check box and provide the credentials for a user that has appropriate access on the source/syndication system: Shared userid , Shared password , and Confirm password . Click OK to save the changes. Navigate to Portal Content > Subscribers . Click Subscribe Now . In the Subscribe to a syndicator pop-up, provide the following: Syndicator URL Syndicator Name Subscriber Name Credential Vault Slot created in step 2. Click Next . Select the libraries to syndicate and the Scope of the syndication. Click Finish . If you have Virtual Portals, you must repeat the syndication steps for each Virtual Portal. If needed, configure library permissions when syndication is completed. Note: As with syndication between on-premise setups, it is possible to do a one-way syndication from an earlier to a later release. You do not need to disable Practitioner Studio to do this syndication. Parent topic: Digital Experience Application deployment","title":"Syndicate the source and target environments"},{"location":"containerization/container_upgrade_overview/","text":"Upgrade options for containerized deployments HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path. Fresh install For fresh installations on any containerized platform, see Deploying container platforms using Helm . Upgrade from any Operator-based deployment Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing DX Operator-based deployments and provides support only for Helm-based deployments . HCL no longer provides further updates or code fixes for Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator to Helm deployments . Upgrade your Helm-based deployment For initial Helm-based deployment from CF196 or CF197, you must Migrate to new DAM DB in Helm-based deployments . If you are not sure if your deployment is based on the first version of the Digital Asset Management (DAM) persistence solution, run the following commands to check: kubectl get -n <namespace> pod/<deployment-name>-persistence-rw-0 kubectl get -n <namespace> pod/<deployment-name>-persistence-node-0 Example: kubectl get -n dxns pod/dx-deployment-persistence-rw-0 kubectl get -n dxns pod/dx-deployment-persistence-node-0 One of commands will return an error message similar to the following: Error from server (NotFound): pods \"dx-deployment-persistence-rw-0\" not found If the first command returned an error, while the second command was successful, you are already on the new database model and can follow the normal upgrade path. For more information, see Update deployment to a later version . Otherwise, you must upgrade to the new HCL DX persistence solution. For more information, see Migrate to new DAM DB in Helm-based deployments . Parent topic: Digital Experience on containerized platforms","title":"Upgrade options for containerized deployments"},{"location":"containerization/container_upgrade_overview/#upgrade-options-for-containerized-deployments","text":"HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path.","title":"Upgrade options for containerized deployments"},{"location":"containerization/container_upgrade_overview/#fresh-install","text":"For fresh installations on any containerized platform, see Deploying container platforms using Helm .","title":"Fresh install"},{"location":"containerization/container_upgrade_overview/#upgrade-from-any-operator-based-deployment","text":"Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing DX Operator-based deployments and provides support only for Helm-based deployments . HCL no longer provides further updates or code fixes for Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator to Helm deployments .","title":"Upgrade from any Operator-based deployment"},{"location":"containerization/container_upgrade_overview/#upgrade-your-helm-based-deployment","text":"For initial Helm-based deployment from CF196 or CF197, you must Migrate to new DAM DB in Helm-based deployments . If you are not sure if your deployment is based on the first version of the Digital Asset Management (DAM) persistence solution, run the following commands to check: kubectl get -n <namespace> pod/<deployment-name>-persistence-rw-0 kubectl get -n <namespace> pod/<deployment-name>-persistence-node-0 Example: kubectl get -n dxns pod/dx-deployment-persistence-rw-0 kubectl get -n dxns pod/dx-deployment-persistence-node-0 One of commands will return an error message similar to the following: Error from server (NotFound): pods \"dx-deployment-persistence-rw-0\" not found If the first command returned an error, while the second command was successful, you are already on the new database model and can follow the normal upgrade path. For more information, see Update deployment to a later version . Otherwise, you must upgrade to the new HCL DX persistence solution. For more information, see Migrate to new DAM DB in Helm-based deployments . Parent topic: Digital Experience on containerized platforms","title":"Upgrade your Helm-based deployment"},{"location":"containerization/core_interactions_kubernetes/","text":"DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Volume mount points The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog. Additional Information about profile directories The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile. Core container Version-to-Version upgrade When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note: If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories). Parent topic: Overview of the Helm architecture","title":"DX 9.5 Core Interactions with Kubernetes"},{"location":"containerization/core_interactions_kubernetes/#dx-95-core-interactions-with-kubernetes","text":"This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm.","title":"DX 9.5 Core Interactions with Kubernetes"},{"location":"containerization/core_interactions_kubernetes/#volume-mount-points","text":"The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog.","title":"Volume mount points"},{"location":"containerization/core_interactions_kubernetes/#additional-information-about-profile-directories","text":"The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile.","title":"Additional Information about profile directories"},{"location":"containerization/core_interactions_kubernetes/#core-container-version-to-version-upgrade","text":"When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note: If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories). Parent topic: Overview of the Helm architecture","title":"Core container Version-to-Version upgrade"},{"location":"containerization/credentialvaultslot/","text":"Create or update credential vault slot This topic describes the commands that are used to create or update credential vault slot in the DX server. Credential vault slot Command description Use the create-credential-vault command to create or update a credential vault slot. dxclient create-credential-vault Help command This command shows the help information for create-credential-vault command usage: dxclient create-credential-vault -h Command options Use this attribute to specify the protocol to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the path to DX configuration endpoint: -xmlConfigPath <value> Use this attribute to specify the credential vault segment slot name: -credentialSlotName <value> Use this attribute to specify the credential vault Username: -vaultUsername <value> Use this attribute to specify the credential vault UserGroup: -vaultUserGroup <value> Use this attribute to specify the credential vault shared userid password: -vaultPassword <value> Use this attribute to specify the credential vault segment name and the default is set to DefaultAdminSegment : -vaultSegmentName <value> Use this attribute to specify the credential vault segment description: -vaultDescription <value> Example: dxclient create-credential-vault -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlConfigPath <xmlConfigPath> -credentialSlotName <credentialSlotName> -vaultUsername <vaultUsername> -vaultPassword <vaultPassword> Parent topic: DXClient Artifact Types","title":"Create or update credential vault slot"},{"location":"containerization/credentialvaultslot/#create-or-update-credential-vault-slot","text":"This topic describes the commands that are used to create or update credential vault slot in the DX server.","title":"Create or update credential vault slot"},{"location":"containerization/credentialvaultslot/#credential-vault-slot","text":"Command description Use the create-credential-vault command to create or update a credential vault slot. dxclient create-credential-vault Help command This command shows the help information for create-credential-vault command usage: dxclient create-credential-vault -h Command options Use this attribute to specify the protocol to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the path to DX configuration endpoint: -xmlConfigPath <value> Use this attribute to specify the credential vault segment slot name: -credentialSlotName <value> Use this attribute to specify the credential vault Username: -vaultUsername <value> Use this attribute to specify the credential vault UserGroup: -vaultUserGroup <value> Use this attribute to specify the credential vault shared userid password: -vaultPassword <value> Use this attribute to specify the credential vault segment name and the default is set to DefaultAdminSegment : -vaultSegmentName <value> Use this attribute to specify the credential vault segment description: -vaultDescription <value> Example: dxclient create-credential-vault -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlConfigPath <xmlConfigPath> -credentialSlotName <credentialSlotName> -vaultUsername <vaultUsername> -vaultPassword <vaultPassword> Parent topic: DXClient Artifact Types","title":"Credential vault slot"},{"location":"containerization/customization/","text":"Customizing your container deployment This section outlines the customization options when deploying HCL Digital Experience Container. Customizing the container deployment This section describes how to customize your HCL Digital Experience 9.5 container deployment. Replicating the DynaCache service This section describes how to replicate Dynacache service in HCL Digital Experience by customizing timeout properties in the WAS Resource Environment Provider (REP). Transfer HCL Digital Experience 9.5 container default database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. Configure Remote Search in Docker This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers. Configure Remote Search in OpenShift and Kubernetes This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Configure Remote Search using REST APIs This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs. Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment Read more about configuring the OpenLDAP container image to the 9.5 container deployment, which is available with HCL Digital Experience 9.5 Container Update release CF181 and later. Configure Digital Asset Management in virtual portals This section provides steps to configure Digital Asset Management support in virtual portals. Customizing the HCL DX URL when deployed to container platforms This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Sample storage class and volume for HCL Digital Experience 9.5 containers Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. Parent topic: Container administration 9.5","title":"Customizing your container deployment"},{"location":"containerization/customization/#customizing-your-container-deployment","text":"This section outlines the customization options when deploying HCL Digital Experience Container. Customizing the container deployment This section describes how to customize your HCL Digital Experience 9.5 container deployment. Replicating the DynaCache service This section describes how to replicate Dynacache service in HCL Digital Experience by customizing timeout properties in the WAS Resource Environment Provider (REP). Transfer HCL Digital Experience 9.5 container default database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. Configure Remote Search in Docker This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers. Configure Remote Search in OpenShift and Kubernetes This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Configure Remote Search using REST APIs This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on a traditional IBM WebSphere Application Server Network Deployment-based cluster DX deployment cluster, a Docker container, or on supported Red Hat OpenShift and Kubernetes container platforms using REST APIs. Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment Read more about configuring the OpenLDAP container image to the 9.5 container deployment, which is available with HCL Digital Experience 9.5 Container Update release CF181 and later. Configure Digital Asset Management in virtual portals This section provides steps to configure Digital Asset Management support in virtual portals. Customizing the HCL DX URL when deployed to container platforms This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Sample storage class and volume for HCL Digital Experience 9.5 containers Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. Parent topic: Container administration 9.5","title":"Customizing your container deployment"},{"location":"containerization/customizing_container_deployment/","text":"Customizing the container deployment This section describes how to customize your HCL Digital Experience 9.5 container deployment. About this task Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation. Before you begin Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment. Procedure Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment. Parent topic: Customizing your container deployment","title":"Customizing the container deployment"},{"location":"containerization/customizing_container_deployment/#customizing-the-container-deployment","text":"This section describes how to customize your HCL Digital Experience 9.5 container deployment.","title":"Customizing the container deployment"},{"location":"containerization/customizing_container_deployment/#about-this-task","text":"Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation.","title":"About this task"},{"location":"containerization/customizing_container_deployment/#before-you-begin","text":"Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git.cwp.pnp-hcl.com_v1_dxdeployment_cr.yamlfile located in the hcl-dx-cloud-scripts/deploy/crds directory in the HCL Digital Experience 9.5 platform packages deployed. Reference the HCL Digital Experience Deployment topic for the list of supported platforms and latest HCL DX 9.5 container package list files that can be downloaded from your HCL Digital Experience entitlements on the HCL Software License Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment.","title":"Before you begin"},{"location":"containerization/customizing_container_deployment/#procedure","text":"Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. HCL DX 9.5 Container Update CF171 and higher Limits are enforced. Auto-scaling based on average CPU and memory utilization can be configured. Auto-scaling When using a Horizontal Pod Autoscale Service, by design, scaling up the amount of HCL DX 9.5 pods is done one at a time. HCL DX 9.5 processes will initiate the requested single instance at a given time until the instance starting is started, to manage scaling in a controlled manager from 1 to N minimum pods set. Each deployment takes approximately ~3 to 4 minutes to start, operating on typical hardware environments. Pod instance terminations are also managed with these control practices. Scaling is controlled in the configuration map with these settings, which can be configured. In this example, 5 is the maximum number of DX 9.5 Container pods requested: dx.deploy.dxcore.resources.scale.maxreplicas: '5' dx.deploy.dxcore.resources.scale.minreplicas: '1' Routes/Ingress By default in 9.5, base routes are created for the deployment. HCL DX 9.5 Container Update CF171 and higher allows a customer to configure the available routes. You can enable or disable any route and change the name of the secret to be used in the TLS context. The Configuration Wizard is still impacted by the number of running instances. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. See the Logging and tracing for containers and new services Help Center topic for additional information. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment. Parent topic: Customizing your container deployment","title":"Procedure"},{"location":"containerization/customizing_kubernetes_eks_deployment/","text":"Customizing the Kubernetes EKS deployment This section describes how to customize your HCL Portal deployment. About this task Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation. Before you begin Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git_v1_dxdeployment_cr.yaml file located in the hcl-dx-cloud-scripts/deploy/crds directory in the hcl-dx-kubernetes-v95-CF184 package downloaded from the HCL Software Licensing Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment. Procedure Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled, # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment. Parent topic: Deploy DX Container to Amazon EKS","title":"Customizing the Kubernetes EKS deployment"},{"location":"containerization/customizing_kubernetes_eks_deployment/#customizing-the-kubernetes-eks-deployment","text":"This section describes how to customize your HCL Portal deployment.","title":"Customizing the Kubernetes EKS deployment"},{"location":"containerization/customizing_kubernetes_eks_deployment/#about-this-task","text":"Follow this procedure to deploy or update your HCL Portal deployment. DX 9.5 containerization is focused on deployment and it uses an operator-based deployment. Goals To introduce a supported containerized deployment that HCL Digital Experience can continually extend to provide customers with the best possible experience. To provide a high level of customization in the deployment and continue to expand on that, along with increased automation.","title":"About this task"},{"location":"containerization/customizing_kubernetes_eks_deployment/#before-you-begin","text":"Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Customizing the deployment requires updating the deploy/crds/git_v1_dxdeployment_cr.yaml file located in the hcl-dx-cloud-scripts/deploy/crds directory in the hcl-dx-kubernetes-v95-CF184 package downloaded from the HCL Software Licensing Portal. Once modified, the deployDx.sh or the updateDx.sh scripts should be run to perform (or update) the target deployment. Note: All modifications should be made to the custom resource instance and not the individual parts of the deployment.","title":"Before you begin"},{"location":"containerization/customizing_kubernetes_eks_deployment/#procedure","text":"Create a backup of the git_v1_dxdeployment_cr.yaml file. Open the original file in edit mode. Find the line with the text labeled, # Add fields here . Customizations should be done below this line. Add the following customizations as applicable: Volume Size By default, the volume size is 100 GB . This can be modified by changing the following: Note: The volume name and storageClassName should not be modified here. Resources By default, the resource requests are set at **2** CPU and **7G** RAM. These values can be changed. It is recommended to adjust the server heap size before changing these values. Note: Limits are not enforced in the initial 9.5 release. Probes The default readiness and liveness probes run against the ../ibm/console. This can and should be overridden. Notes: There are two types of checks: **command** runs a command against the server **http** hits either an http or an https URL. The syntax and required fields are shown in the above image. Logging By default, logging is done on the shared profile so all instances are writing to a single set of logs, with the volume set for each instance at **1G** . For diagnosing production issues this is not ideal. This option allows each instance to write the log to its own log directory. Notes: The environment must have a self-provisioning storage class provisioner. **Enabled** must be set to **true** . Adjusting the log settings must be done to prevent running out of disk storage. Ports By default, the deployment uses the default DX ports. The routes in these ports expose Portal through http and https . Note: If there is a need to configure the containerized Portal to use different ports, the defaults can be overwritten. Once modified, the deployDx.sh and the updateDx.sh scripts should be run to create (or update) the target deployment. Parent topic: Deploy DX Container to Amazon EKS","title":"Procedure"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/","text":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. The directions closely follow the Digital Experience database transfer steps provided for deployments to supported \"on-premises\" platforms, such as Windows, Linux, and AIX. The unique steps that account for differences in these instructions for use with an HCL Digital Experience 9.5 Docker container deployment as opposed to an on-premises Digital Experience installation are highlighted. The on-premises based HCL Digital Experience database transfer instructions can be viewed in this topic: DB2 worksheet: Transfer to multiple databases . These container deployment instructions cover the transfer of the default Derby database running on an HCL Digital Experience Container Update CF18 container image to an IBM DB2 Enterprise Version 11.1.0.0 database running on a CentOS Linux release 7.71908 server. The IBM DB2 11.5 Standard and Enterprise database is also supported for these procedures. In these instructions, the IBM DB2 database is installed to a supported on-premises platform (see IBM DB2 System Support requirements for a list of supported platforms ). The HCL Digital Experience 9.5 Docker container is deployed to a supported Docker or Kubernetes platform. The container version deployed in this example is Docker CE version 19.0.3.8. Video : HCL Digital Experience - Perform a Database Transfer on HCL Portal 9.5 Prerequisites The HCL Digital Experience 9.5 container image is deployed, and loaded to your Docker repository using the following command: docker load < hcl-dx-core-image-v95_CF18_20200427-2055.tar.gz See the HCL Digital Experience 9.5 topic Deployment for instructions and latest list of HCL Digital Experience 9.5 Container image and file names. IBM DB2 is installed on an on-premises server with a DNS hostname that is available to the HCL Digital Experience 9.5 deployed container. Start the HCL Digital Experience 9.5 Container in Docker Execute the following command to start the DX 9.5 container: docker run -d --add-host {database DNS name}:{database IP address} -p 10025-10045:10025-10045 -p 10200-10210:10200-10210 -v '{directory on the Docker host for DX container profile}':'/opt/HCL/wp_profile' {your repo name}:{your tag name} Note: An --add-host entry needs to be made to insert the DNS name of the IBM DB2 database server into the /etc/hosts file in the container running the Digital Experience 9.5 container. If, and only if, the DNS name of the DB2 server is already in a named server available to the container, then the --add-host parameter would not be needed. The ports for the Digital Experience 9.5 container (100025-10045) need to be mapped as well as the ports using by the Configuration Wizard. The Configuration Wizard is used to manage the transfer of the database. The Configuration Wizard uses ports in the range of 10200-10202. The HCL Digital Experience 9.5 container persists the profile information across restarts. This is persisted on the Docker host as {directory on docker host for DX container profile} in the run command above. The container will map /opt/HCL/wp_profile to this directory on the docker via the -v command. You need to specify the Docker repository and tag name as the reader has loaded the HCL Digital Experience container image into their docker repository. When this docker run command is executed, ensure time is allocated to check the profile as well as initialize the HCL Digital Experience 9.5 container instance. One can ensure that the HCL Digital Experience 9.5 container deployment is ready by \"tail\"-ing the file in the container located at /opt/HCL/wp_profile/logs/HCL Portal and HCL Web Content Manager/SystemOut.log. When the file displays the following message, the HCL Digital Experience 9.5 container instance is initialized: \u2026open for e- business One can access this file (as well as all others in the running container) via the following command: docker exec -it {container name} bash Transferring the Database First, ensure that your HCL Digital Experience 9.5 docker container can access the IBM DB2 on-premises platform server. Using the following command, one can simply \"ping\" the DNS name of the IBM DB2 database server and verify that it answers: docker exec -it {container name} bash If this is not successful, consult with your platform administrator for other methods to debug the network issues between a Docker container and Servers running external to Docker in your environment. Next, once connectivity is established, follow the directions for using the Configuration Wizard from the traditional (on-premises platform-based) Digital Experience database transfer steps to transfer the data from the default Derby database and configure the target IBM DB2 database server for use with the HCL Digital Experience 9.5 Docker container deployment. One can access the Configuration Wizard from a browser on the HCL Digital Experience 9.5 container deployment via the URL http://{docker host server}:10200/hcl/wizard. Proceed to the Digital Experience database transfer steps topic section Set Up a Stand-alone Server - Database Transfer . Specify the fully qualified DNS name of the database server as set above in the --add-host directive in the docker run command. Parent topic: Customizing your container deployment","title":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#transfer-hcl-digital-experience-95-container-default-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. Administrator users can follow these steps to transfer the default database configuration detail to IBM DB2, if preferred for use as the relational database for HCL Digital Experience 9.5 container deployment data. The directions closely follow the Digital Experience database transfer steps provided for deployments to supported \"on-premises\" platforms, such as Windows, Linux, and AIX. The unique steps that account for differences in these instructions for use with an HCL Digital Experience 9.5 Docker container deployment as opposed to an on-premises Digital Experience installation are highlighted. The on-premises based HCL Digital Experience database transfer instructions can be viewed in this topic: DB2 worksheet: Transfer to multiple databases . These container deployment instructions cover the transfer of the default Derby database running on an HCL Digital Experience Container Update CF18 container image to an IBM DB2 Enterprise Version 11.1.0.0 database running on a CentOS Linux release 7.71908 server. The IBM DB2 11.5 Standard and Enterprise database is also supported for these procedures. In these instructions, the IBM DB2 database is installed to a supported on-premises platform (see IBM DB2 System Support requirements for a list of supported platforms ). The HCL Digital Experience 9.5 Docker container is deployed to a supported Docker or Kubernetes platform. The container version deployed in this example is Docker CE version 19.0.3.8. Video : HCL Digital Experience - Perform a Database Transfer on HCL Portal 9.5 Prerequisites The HCL Digital Experience 9.5 container image is deployed, and loaded to your Docker repository using the following command: docker load < hcl-dx-core-image-v95_CF18_20200427-2055.tar.gz See the HCL Digital Experience 9.5 topic Deployment for instructions and latest list of HCL Digital Experience 9.5 Container image and file names. IBM DB2 is installed on an on-premises server with a DNS hostname that is available to the HCL Digital Experience 9.5 deployed container.","title":"Transfer HCL Digital Experience 9.5 container default database to IBM DB2"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#start-the-hcl-digital-experience-95-container-in-docker","text":"Execute the following command to start the DX 9.5 container: docker run -d --add-host {database DNS name}:{database IP address} -p 10025-10045:10025-10045 -p 10200-10210:10200-10210 -v '{directory on the Docker host for DX container profile}':'/opt/HCL/wp_profile' {your repo name}:{your tag name} Note: An --add-host entry needs to be made to insert the DNS name of the IBM DB2 database server into the /etc/hosts file in the container running the Digital Experience 9.5 container. If, and only if, the DNS name of the DB2 server is already in a named server available to the container, then the --add-host parameter would not be needed. The ports for the Digital Experience 9.5 container (100025-10045) need to be mapped as well as the ports using by the Configuration Wizard. The Configuration Wizard is used to manage the transfer of the database. The Configuration Wizard uses ports in the range of 10200-10202. The HCL Digital Experience 9.5 container persists the profile information across restarts. This is persisted on the Docker host as {directory on docker host for DX container profile} in the run command above. The container will map /opt/HCL/wp_profile to this directory on the docker via the -v command. You need to specify the Docker repository and tag name as the reader has loaded the HCL Digital Experience container image into their docker repository. When this docker run command is executed, ensure time is allocated to check the profile as well as initialize the HCL Digital Experience 9.5 container instance. One can ensure that the HCL Digital Experience 9.5 container deployment is ready by \"tail\"-ing the file in the container located at /opt/HCL/wp_profile/logs/HCL Portal and HCL Web Content Manager/SystemOut.log. When the file displays the following message, the HCL Digital Experience 9.5 container instance is initialized: \u2026open for e- business One can access this file (as well as all others in the running container) via the following command: docker exec -it {container name} bash","title":"Start the HCL Digital Experience 9.5 Container in Docker"},{"location":"containerization/cw_containerdbtransfer_ibm_db2/#transferring-the-database","text":"First, ensure that your HCL Digital Experience 9.5 docker container can access the IBM DB2 on-premises platform server. Using the following command, one can simply \"ping\" the DNS name of the IBM DB2 database server and verify that it answers: docker exec -it {container name} bash If this is not successful, consult with your platform administrator for other methods to debug the network issues between a Docker container and Servers running external to Docker in your environment. Next, once connectivity is established, follow the directions for using the Configuration Wizard from the traditional (on-premises platform-based) Digital Experience database transfer steps to transfer the data from the default Derby database and configure the target IBM DB2 database server for use with the HCL Digital Experience 9.5 Docker container deployment. One can access the Configuration Wizard from a browser on the HCL Digital Experience 9.5 container deployment via the URL http://{docker host server}:10200/hcl/wizard. Proceed to the Digital Experience database transfer steps topic section Set Up a Stand-alone Server - Database Transfer . Specify the fully qualified DNS name of the database server as set above in the --add-host directive in the docker run command. Parent topic: Customizing your container deployment","title":"Transferring the Database"},{"location":"containerization/dam_artifacts/","text":"DAM artifacts This section contains the commands for working with Digital Asset Management (DAM) schemas, managing DAM staging for subscription, or configuring periodic sync. DAM schemas This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence. Parent topic: DXClient Artifact Types","title":"DAM artifacts"},{"location":"containerization/dam_artifacts/#dam-artifacts","text":"This section contains the commands for working with Digital Asset Management (DAM) schemas, managing DAM staging for subscription, or configuring periodic sync. DAM schemas This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence. Parent topic: DXClient Artifact Types","title":"DAM artifacts"},{"location":"containerization/dam_extensibility/","text":"Using DAM extensibility This section describes the DAM extensibility feature and how to configure your setup for extensibility. DAM extensibility allows DAM to support user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, and many more, while supporting default and custom renditions.","title":"Using DAM extensibility"},{"location":"containerization/dam_extensibility/#using-dam-extensibility","text":"This section describes the DAM extensibility feature and how to configure your setup for extensibility. DAM extensibility allows DAM to support user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, and many more, while supporting default and custom renditions.","title":"Using DAM extensibility"},{"location":"containerization/dam_subscription_staging/","text":"Using DAM staging This topic contains the commands that administrators can use to configure the staging of Digital Asset Management (DAM) content. This allows you to manage subscriber registration or configure periodic sync. DAM staging framework The DAM staging framework allows you to stage your DAM content from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber). Using DXClient , you can configure DAM staging to: Trigger a manual staging or use periodic staging processes. Set the cycle length (default: 2 minutes, maximum: 24 hours) for periodic sync. Register a subscriber with a publisher. Note: A subscriber must be registered with a publisher. Access rights from DAM staging assets are not transferred for subscribers that do not share the same Lightweight Directory Access Protocol (LDAP). Manage DAM staging Use the manage-dam-staging trigger-staging command to trigger DAM staging. Command description You can trigger the DAM staging with the following command: dxclient manage-dam-staging trigger-staging Help command This command shows the help information for manage-dam-staging trigger-staging command usage: dxclient manage-dam-staging trigger-staging -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> Command: ``` dxclient manage-dam-staging trigger-staging -dxProtocol -hostname -dxPort -dxUsername -dxPassword -damAPIPort -ringAPIPort -damAPIVersion -ringAPIVersion -targetHostname ``` Example: dxclient manage-dam-staging trigger-staging -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -targetHostname native-kube-dam-production.team-q-dev.com Registering or deregistering for DAM staging Use the manage-dam-staging **register**-dam-subscriber command to register or the manage-dam-staging **deregister**-dam-subscriber command to deregister the subscriber for DAM staging. Command description You can register a subscriber for DAM staging with the following command: dxclient manage-dam-staging register-dam-subscriber You can deregister a subscriber for DAM staging with the following command: dxclient manage-dam-staging deregister-dam-subscriber Help command The following command shows the help information for manage-dam-staging **register**-dam-subscriber command usage: dxclient manage-dam-staging register-dam-subscriber -h The following command shows the help information for manage-dam-staging **deregister**-dam-subscriber command usage: dxclient manage-dam-staging deregister-dam-subscriber -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the subscriber ID of the target environment: -subscriberId <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> Commands: To register: dxclient manage-dam-staging register-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> Example: dxclient manage-dam-staging register-dam-subscriber -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -targetHostname native-kube-dam-production.team-q-dev.com -interval 2 To deregister: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> Example: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -subscriberId d7e5e014-12a0-4dc5-a5d7-971fd4fa86f3 Using WCM with DAM staging The typical setup involves a WCM staging/authoring server connected to DAM staging/authoring, and a separate WCM rendering connected to DAM rendering (there could be multiple WCM rendering/DAM rendering environments, for example, a Blue/Green setup). Syndication is set up for WCM between staging/authoring and WCM rendering. DAM staging is set up between DAM staging/authoring and DAM rendering. (Optional) You can configure WCM WCMConfigService in the WAS Admin Console to allow switching the host name (and port) used for DAM references in WCM using the following: dam.host.overwrite.port=... dam.host.overwrite=... For example: dam.host.overwrite=myserver.com dam.host.overwrite.port=3000 You must restart the DX Core JVM for changes to take effect. Effect : If the properties are in place when using the REST API or WCM Admin UI or WCM API, the returned DAM references have the overwritten host name and port. For example, if a content item is moved from the staging environment to production, and production has the host overwrite set to production.hcl.com , then all DAM references are returned with production.hcl.com . For instance, production.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true even if we received the following from syndication: staging.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true.","title":"Using DAM staging"},{"location":"containerization/dam_subscription_staging/#using-dam-staging","text":"This topic contains the commands that administrators can use to configure the staging of Digital Asset Management (DAM) content. This allows you to manage subscriber registration or configure periodic sync.","title":"Using DAM staging"},{"location":"containerization/dam_subscription_staging/#dam-staging-framework","text":"The DAM staging framework allows you to stage your DAM content from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber). Using DXClient , you can configure DAM staging to: Trigger a manual staging or use periodic staging processes. Set the cycle length (default: 2 minutes, maximum: 24 hours) for periodic sync. Register a subscriber with a publisher. Note: A subscriber must be registered with a publisher. Access rights from DAM staging assets are not transferred for subscribers that do not share the same Lightweight Directory Access Protocol (LDAP).","title":"DAM staging framework"},{"location":"containerization/dam_subscription_staging/#manage-dam-staging","text":"Use the manage-dam-staging trigger-staging command to trigger DAM staging. Command description You can trigger the DAM staging with the following command: dxclient manage-dam-staging trigger-staging Help command This command shows the help information for manage-dam-staging trigger-staging command usage: dxclient manage-dam-staging trigger-staging -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> Command: ``` dxclient manage-dam-staging trigger-staging -dxProtocol -hostname -dxPort -dxUsername -dxPassword -damAPIPort -ringAPIPort -damAPIVersion -ringAPIVersion -targetHostname ``` Example: dxclient manage-dam-staging trigger-staging -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -targetHostname native-kube-dam-production.team-q-dev.com","title":"Manage DAM staging"},{"location":"containerization/dam_subscription_staging/#registering-or-deregistering-for-dam-staging","text":"Use the manage-dam-staging **register**-dam-subscriber command to register or the manage-dam-staging **deregister**-dam-subscriber command to deregister the subscriber for DAM staging. Command description You can register a subscriber for DAM staging with the following command: dxclient manage-dam-staging register-dam-subscriber You can deregister a subscriber for DAM staging with the following command: dxclient manage-dam-staging deregister-dam-subscriber Help command The following command shows the help information for manage-dam-staging **register**-dam-subscriber command usage: dxclient manage-dam-staging register-dam-subscriber -h The following command shows the help information for manage-dam-staging **deregister**-dam-subscriber command usage: dxclient manage-dam-staging deregister-dam-subscriber -h Command options Use this attribute to specify the protocol with which to connect to the DX server (default: \"\") -dxProtocol <value> Use this attribute to specify the host name of the DX server (default: \"\") -hostname <value> Use this attribute to specify the port on which to connect to the DX server (default: \"\"; default port for any Kubernetes environment is 443): -dxPort <value> Use this attribute to specify the user name that is required for authenticating with the DX server (default: \"\") -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core (default: \"\") -dxPassword <value> Use this attribute to specify the port number of the DAM server (default: \"\"; default port for any Kubernetes environment is 443): -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIPort <value> Use this attribute to specify the API version number of DAM (default: \"\"; default port for any Kubernetes environment is 443): -damAPIVersion <value> Use this attribute to specify the API version number of DX Core (default: \"\"; default port for any Kubernetes environment is 443): -ringAPIVersion <value> Use this attribute to specify the host name of the target environment: -targetHostname <value> Use this attribute to specify the subscriber ID of the target environment: -subscriberId <value> Use this attribute to specify the interval between two sync cycles. The unit of interval is in minutes. (default: \"2 minutes\") -interval <value> Commands: To register: dxclient manage-dam-staging register-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> Example: dxclient manage-dam-staging register-dam-subscriber -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -targetHostname native-kube-dam-production.team-q-dev.com -interval 2 To deregister: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -damAPIVersion <damAPIVersion> -ringAPIVersion <ringAPIVersion> -subscriberId <subscriberId> Example: dxclient manage-dam-staging deregister-dam-subscriber -dxProtocol https -hostname native-kube-dam-staging.team-q-dev.com -dxPort 443 -dxUsername xxxx -dxPassword xxxx -damAPIPort 443 -ringAPIPort 443 -damAPIVersion v1 -ringAPIVersion v1 -subscriberId d7e5e014-12a0-4dc5-a5d7-971fd4fa86f3","title":"Registering or deregistering for DAM staging"},{"location":"containerization/dam_subscription_staging/#using-wcm-with-dam-staging","text":"The typical setup involves a WCM staging/authoring server connected to DAM staging/authoring, and a separate WCM rendering connected to DAM rendering (there could be multiple WCM rendering/DAM rendering environments, for example, a Blue/Green setup). Syndication is set up for WCM between staging/authoring and WCM rendering. DAM staging is set up between DAM staging/authoring and DAM rendering. (Optional) You can configure WCM WCMConfigService in the WAS Admin Console to allow switching the host name (and port) used for DAM references in WCM using the following: dam.host.overwrite.port=... dam.host.overwrite=... For example: dam.host.overwrite=myserver.com dam.host.overwrite.port=3000 You must restart the DX Core JVM for changes to take effect. Effect : If the properties are in place when using the REST API or WCM Admin UI or WCM API, the returned DAM references have the overwritten host name and port. For example, if a content item is moved from the staging environment to production, and production has the host overwrite set to production.hcl.com , then all DAM references are returned with production.hcl.com . For instance, production.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true even if we received the following from syndication: staging.hcl.com/dx/api/dam/v1/collections/390e9808-a6d2-4ebe-b6fb-f10046ebf642/items/fd18083c-d84b-4816-af6e-583059c73122/renditions/7855bfae-d741-41f7-815f-d15f427a4da0?binary=true.","title":"Using WCM with DAM staging"},{"location":"containerization/damschemas/","text":"DAM schemas This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence. Listing DAM schemas The list-dam-schemas command is used to list all the DAM schemas. Command description This command invokes list-dam-schemas inside DXClient and provides a list DAM schemas. dxclient list-dam-schemas Help command This command shows the help information for list-dam-schemas command usage: dxclient list-dam-schemas -h Command options Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server(for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server(for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM(for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core(for Kubernetes Environment default port is 443) -ringAPIVersion <value> Example: dxclient list-dam-schemas -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> Deleting DAM schemas Deleting DAM schema is a recommended step when the configuration of the DAM database schema has been changed, due to a release update such as from Container Update CF196 to Container Update CF197. When a DAM database is migrated, a new schema gets generated and the old schema is rendered inactive. To avoid the accumulation of inactive schemas, you can use the delete-dam-schema command to delete them. Use the delete-dam-schema command to delete the inactive DAM schema. Command description This command invokes delete-dam-schema inside DXClient and deletes the DAM schema. dxclient delete-dam-schema Help command This command shows the help information for delete-dam-schema command usage: dxclient delete-dam-schema -h Command options Use this attribute to specify the protocol that is used to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server (for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM (for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core (for Kubernetes Environment default port is 443) -ringAPIVersion <value> Use this attribute to specify the DAM Schema Version (for Kubernetes Environment default port is 443) -schemaVersion <value> Note: In case the user does not enter the schemaVersion , user is prompted with a list of inactive schemas to choose from. Example: dxclient delete-dam-schema -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> -schemaVersion <schemaVersion> Parent topic: DAM artifacts","title":"DAM schemas"},{"location":"containerization/damschemas/#dam-schemas","text":"This topic contains the commands that administrators can use to get a list of all DAM schemas or delete inactive Digital Asset Management (DAM) schemas from persistence.","title":"DAM schemas"},{"location":"containerization/damschemas/#listing-dam-schemas","text":"The list-dam-schemas command is used to list all the DAM schemas. Command description This command invokes list-dam-schemas inside DXClient and provides a list DAM schemas. dxclient list-dam-schemas Help command This command shows the help information for list-dam-schemas command usage: dxclient list-dam-schemas -h Command options Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server(for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server(for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM(for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core(for Kubernetes Environment default port is 443) -ringAPIVersion <value> Example: dxclient list-dam-schemas -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion>","title":"Listing DAM schemas"},{"location":"containerization/damschemas/#deleting-dam-schemas","text":"Deleting DAM schema is a recommended step when the configuration of the DAM database schema has been changed, due to a release update such as from Container Update CF196 to Container Update CF197. When a DAM database is migrated, a new schema gets generated and the old schema is rendered inactive. To avoid the accumulation of inactive schemas, you can use the delete-dam-schema command to delete them. Use the delete-dam-schema command to delete the inactive DAM schema. Command description This command invokes delete-dam-schema inside DXClient and deletes the DAM schema. dxclient delete-dam-schema Help command This command shows the help information for delete-dam-schema command usage: dxclient delete-dam-schema -h Command options Use this attribute to specify the protocol that is used to connect to the server -dxProtocol <value> Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the port number of the DAM server (for Kubernetes Environment default port is 443) -damAPIPort <value> Use this attribute to specify the port number of the DX Core API server (for Kubernetes Environment default port is 443) -ringAPIPort <value> Use this attribute to specify the API Version number of DAM (for Kubernetes Environment default port is 443) -damAPIVersion <value> Use this attribute to specify the API Version number of DX Core (for Kubernetes Environment default port is 443) -ringAPIVersion <value> Use this attribute to specify the DAM Schema Version (for Kubernetes Environment default port is 443) -schemaVersion <value> Note: In case the user does not enter the schemaVersion , user is prompted with a list of inactive schemas to choose from. Example: dxclient delete-dam-schema -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -damAPIPort <damAPIPort> -ringAPIPort <ringAPIPort> -ringAPIVersion <ringAPIVersion> -damAPIVersion <damAPIVersion> -schemaVersion <schemaVersion> Parent topic: DAM artifacts","title":"Deleting DAM schemas"},{"location":"containerization/deploy_container_artifact_updates/","text":"Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience container deployments while minimizing operations downtime. Introduction In a Kubernetes environment, how can one achieve zero downtime deployments of new artifacts? In a \u201ctraditional\u201d HCL Digital Experience deployment to on-premises platforms such as Windows, AIX, or Linux OS, adding new components such as a new theme, new portlets, new pages, new PZN rules, etc. is typically achieved by first validating these new artifacts in lower \u201cnon-production\u201d cells prior to introduction to the production cell or cells. When referring to a \u201ctraditional\u201d deployment, that refers to a single cluster in a single cell as depicted in the HCL DX Roadmap: Production and delivery environment . Once validated in non-production cells, the \u201cleast risk\u201d topology for new artifacts (as well as for individual and cumulative fixes) is to employ a two-cell production environment with either active/active or active/passive access. New artifacts would be deployed into a cell that was \u201coff-line\u201d (e.g. not in the active load balance), validating that cell and then moving that cell to the active load balance. Using this process almost always guaranteed a zero-downtime deployment of fixes and artifacts. HCL Digital Experience refers to this topology as the \u201cGold Standard\u201d topology for business critical deployments. One could always take more risk and not employ a two-cell HA environment for production. Rather, one could assume that if the new artifacts were proven in several lower environments, the risk of failure in a single cell production environment was very low. Kubernetes \u2013 Two Views There are two ways to view deployment to a supported Kubernetes environment. The first is purely from a tooling point of view. In other words, what processes and tooling does one use to get new artifacts into a Kubernetes system? Are these processes and tools different than one would use in a non-Kubernetes HCL Digital Experience platform deployment? The second view, as discussed in the introduction, is how to achieve deployment while minimizing operations downtime? Difference and Similarities Between Traditional and Kubernetes DX Deployments To properly understand zero downtime deployment, it is helpful to understand how a traditional Digital Experience Portal cluster is similar and different than a multi-pod Kubernetes deployment. A \u201ctraditional\u201d HCL Digital Experience cluster is based on an IBM WebSphere Application Server (WAS) Network Deployment (ND) cell. One or more HCL DX Servers are in the same WAS cell. They share a common database. Each of the HCL DX Server cluster members produce exactly the same HTTP response as the other members of the cell. When deploying new binary artifacts (e.g. new portlets, themes, etc.), the IBM WAS Deployment Manager (DMgr) in this topology owns the \u201cmaster\u201d copy of the binary. It \u201csyncs\u201d that binary out to each of the cluster members when updated. There are many other features of a WAS ND cell that HCL DX deployments takes advantage of in this topology as well. It is important to note that in this WAS ND topology, regardless of which HCL DX cluster member provides the role of DX administration, all cluster members and the database are properly updated when portlets, themes and configuration changes occur. In a Kubernetes deployment, each HCL DX POD is a standalone WAS entity. These DX PODs have no knowledge of other PODs running DX. All the PODs do share a common database like in the WAS ND cluster case, however. In addition, each POD has a \u201cvolume mount\u201d for the WAS profile governing the configuration of that HCL DX deployment. This profile is also shared between all the PODs to ensure that each POD provides the same response to an inbound HTTP request. In either topology (on-premises OS, or Kubernetes), the union of the WAS profile and the database provide all the configuration required. If the HCL DX Kubernetes PODs all share the same database as well as the same profile, then their responses will be the same for inbound HTTP requests. Also note that no configuration changes are reflected in the HCL DX container (e.g. a Docker \u201ccommit\u201d command is not used). All configuration changes are reflected in the WAS profile. One important difference is that when a configuration change takes place in a Kubernetes environment, typically it is recommended that only one HCL DX POD is active. In that case, the profile and database are updated with any WAS or HCL DX changes. As new PODs are then spawned, they also inherit these changes. Single Kubernetes Cluster Versus Multiple Kubernetes Cluster Just like in the \u201ctraditional\u201d WAS ND cluster, the lowest risk topology for deployment of new artifacts into a Kubernetes cluster is to maintain two independent Kubernetes clusters. Just like in the traditional WAS ND cluster, one would deploy all changes to the \u201coff-line\u201d cluster, validate that cluster and then switch that cluster at the load balancer to be active. One could then apply the same processes to the down-level Kubernetes cluster, which is now out of the active load balancer. Processes and Tooling to Deploy Artifacts to Kubernetes The main difference between deploying HCL DX with WAS ND services to Kubernetes and traditional on-premises platforms is that it is recommended to only allow a single POD of HCL DX to be active in Kubernetes while any configuration changes takes place. The process to deploy new artifacts to a Kubernetes environment mirrors the processes used to deploy those same artifacts to a traditional WAS ND based cluster. All existing deployment tooling is still present and available in the Kubernetes case. The reason one only wants one POD of the DX deployment active is due to the fact that the (shared) database might reflect differences with existing (older) PODs if multiple PODs were allowed to be active. Once all changes are complete, one can allow autoscaling of the number of HCL DX PODs back to the \u201cproduction\u201d number. For DX container autoscaling guidance, see Customizing the Container Deployment . Note that during this process, this single HCL DX POD node can remain in the active load balance if one is running a single Kubernetes cluster. Parent topic: Digital Experience Application deployment","title":"Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime"},{"location":"containerization/deploy_container_artifact_updates/#deploying-hcl-dx-95-container-artifact-updates-with-minimal-operations-downtime","text":"This topic provides guidance to update artifacts in HCL Digital Experience container deployments while minimizing operations downtime.","title":"Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime"},{"location":"containerization/deploy_container_artifact_updates/#introduction","text":"In a Kubernetes environment, how can one achieve zero downtime deployments of new artifacts? In a \u201ctraditional\u201d HCL Digital Experience deployment to on-premises platforms such as Windows, AIX, or Linux OS, adding new components such as a new theme, new portlets, new pages, new PZN rules, etc. is typically achieved by first validating these new artifacts in lower \u201cnon-production\u201d cells prior to introduction to the production cell or cells. When referring to a \u201ctraditional\u201d deployment, that refers to a single cluster in a single cell as depicted in the HCL DX Roadmap: Production and delivery environment . Once validated in non-production cells, the \u201cleast risk\u201d topology for new artifacts (as well as for individual and cumulative fixes) is to employ a two-cell production environment with either active/active or active/passive access. New artifacts would be deployed into a cell that was \u201coff-line\u201d (e.g. not in the active load balance), validating that cell and then moving that cell to the active load balance. Using this process almost always guaranteed a zero-downtime deployment of fixes and artifacts. HCL Digital Experience refers to this topology as the \u201cGold Standard\u201d topology for business critical deployments. One could always take more risk and not employ a two-cell HA environment for production. Rather, one could assume that if the new artifacts were proven in several lower environments, the risk of failure in a single cell production environment was very low.","title":"Introduction"},{"location":"containerization/deploy_container_artifact_updates/#kubernetes-two-views","text":"There are two ways to view deployment to a supported Kubernetes environment. The first is purely from a tooling point of view. In other words, what processes and tooling does one use to get new artifacts into a Kubernetes system? Are these processes and tools different than one would use in a non-Kubernetes HCL Digital Experience platform deployment? The second view, as discussed in the introduction, is how to achieve deployment while minimizing operations downtime?","title":"Kubernetes \u2013 Two Views"},{"location":"containerization/deploy_container_artifact_updates/#difference-and-similarities-between-traditional-and-kubernetes-dx-deployments","text":"To properly understand zero downtime deployment, it is helpful to understand how a traditional Digital Experience Portal cluster is similar and different than a multi-pod Kubernetes deployment. A \u201ctraditional\u201d HCL Digital Experience cluster is based on an IBM WebSphere Application Server (WAS) Network Deployment (ND) cell. One or more HCL DX Servers are in the same WAS cell. They share a common database. Each of the HCL DX Server cluster members produce exactly the same HTTP response as the other members of the cell. When deploying new binary artifacts (e.g. new portlets, themes, etc.), the IBM WAS Deployment Manager (DMgr) in this topology owns the \u201cmaster\u201d copy of the binary. It \u201csyncs\u201d that binary out to each of the cluster members when updated. There are many other features of a WAS ND cell that HCL DX deployments takes advantage of in this topology as well. It is important to note that in this WAS ND topology, regardless of which HCL DX cluster member provides the role of DX administration, all cluster members and the database are properly updated when portlets, themes and configuration changes occur. In a Kubernetes deployment, each HCL DX POD is a standalone WAS entity. These DX PODs have no knowledge of other PODs running DX. All the PODs do share a common database like in the WAS ND cluster case, however. In addition, each POD has a \u201cvolume mount\u201d for the WAS profile governing the configuration of that HCL DX deployment. This profile is also shared between all the PODs to ensure that each POD provides the same response to an inbound HTTP request. In either topology (on-premises OS, or Kubernetes), the union of the WAS profile and the database provide all the configuration required. If the HCL DX Kubernetes PODs all share the same database as well as the same profile, then their responses will be the same for inbound HTTP requests. Also note that no configuration changes are reflected in the HCL DX container (e.g. a Docker \u201ccommit\u201d command is not used). All configuration changes are reflected in the WAS profile. One important difference is that when a configuration change takes place in a Kubernetes environment, typically it is recommended that only one HCL DX POD is active. In that case, the profile and database are updated with any WAS or HCL DX changes. As new PODs are then spawned, they also inherit these changes.","title":"Difference and Similarities Between Traditional and Kubernetes DX Deployments"},{"location":"containerization/deploy_container_artifact_updates/#single-kubernetes-cluster-versus-multiple-kubernetes-cluster","text":"Just like in the \u201ctraditional\u201d WAS ND cluster, the lowest risk topology for deployment of new artifacts into a Kubernetes cluster is to maintain two independent Kubernetes clusters. Just like in the traditional WAS ND cluster, one would deploy all changes to the \u201coff-line\u201d cluster, validate that cluster and then switch that cluster at the load balancer to be active. One could then apply the same processes to the down-level Kubernetes cluster, which is now out of the active load balancer.","title":"Single Kubernetes Cluster Versus Multiple Kubernetes Cluster"},{"location":"containerization/deploy_container_artifact_updates/#processes-and-tooling-to-deploy-artifacts-to-kubernetes","text":"The main difference between deploying HCL DX with WAS ND services to Kubernetes and traditional on-premises platforms is that it is recommended to only allow a single POD of HCL DX to be active in Kubernetes while any configuration changes takes place. The process to deploy new artifacts to a Kubernetes environment mirrors the processes used to deploy those same artifacts to a traditional WAS ND based cluster. All existing deployment tooling is still present and available in the Kubernetes case. The reason one only wants one POD of the DX deployment active is due to the fact that the (shared) database might reflect differences with existing (older) PODs if multiple PODs were allowed to be active. Once all changes are complete, one can allow autoscaling of the number of HCL DX PODs back to the \u201cproduction\u201d number. For DX container autoscaling guidance, see Customizing the Container Deployment . Note that during this process, this single HCL DX POD node can remain in the active load balance if one is running a single Kubernetes cluster. Parent topic: Digital Experience Application deployment","title":"Processes and Tooling to Deploy Artifacts to Kubernetes"},{"location":"containerization/deploy_container_platforms/","text":"Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments HCL Digital Experience 9.5 Container Deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Container administration 9.5 The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. Troubleshooting cloud container Containers This section lists the basics of troubleshooting the containerized image or your deployment. Parent topic: Digital Experience on containerized platforms","title":"Operator-based deployment"},{"location":"containerization/deploy_container_platforms/#operator-based-deployment","text":"This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Notes: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Video : Getting started with HCL Portal 9.5 on Docker . Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Docker image list and Docker image deployment Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments HCL Digital Experience 9.5 Container Deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Container administration 9.5 The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. Troubleshooting cloud container Containers This section lists the basics of troubleshooting the containerized image or your deployment. Parent topic: Digital Experience on containerized platforms","title":"Operator-based deployment"},{"location":"containerization/deploy_dx_components_using_hcl_dx_client_and_dx_connect/","text":"Deploy DX components using HCL DXClient and DXConnect HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process. Important: DXClient version is mostly forward and backward compatible with the DX CF versions, however, in some cases it might not work as expected if the CF versions are different. Hence, ensure that the CF versions of both DXClient and DX Core are the same in your installation. DXClient DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. DXClient Artifact Types This section provides information about the artifact types that are currently supported by the DXClient tool. Troubleshooting DXClient Logs can be enabled and disabled as desired by DX developers and administrators through configuration options in the config.json file of DXClient. The log files can be viewed inside the logs folder within the DXClient installation folder. DXConnect DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions. Sample Pipelines for use with HCL DXClient and Automation servers A CI/CD pipeline can help automate processes in the development and test cycle, including deploying code to test and production environments. HCL DX 9.5 provides sample pipelines for use with the DXClient tooling to demonstrate how the deployment of portlets, Script Applications, Themes, DX Application, export and import of WCM libraries, etc., can be automated. Sample Pipelines for the DXClient Docker image file This sample shows how to pull DXClient docker image from the given artifactory, run the DXClient tool in the docker and then deploy a portlet, theme, and script application. It is designed to be run from a Jenkins job that provides the following parameters: Sample Pipelines for the DXClient node package file This topic describes how to install the DXClient tool in a pipeline, by providing a few sample pipelines such as pipelines for deploying a portlet, theme, and script application. It is designed to be run from a Jenkins job with the following parameters: Parent topic: Digital Experience Application deployment","title":"Deploy DX components using HCL DXClient and DXConnect"},{"location":"containerization/deploy_dx_components_using_hcl_dx_client_and_dx_connect/#deploy-dx-components-using-hcl-dxclient-and-dxconnect","text":"HCL Digital Experience (DX) 9.5 CF19 and later releases include a DXClient toolset, and DXConnect servlet that provides developers and administrators with an approach to deploy changes or improvements to the DX platform, and partially automate the development and delivery process. Important: DXClient version is mostly forward and backward compatible with the DX CF versions, however, in some cases it might not work as expected if the CF versions are different. Hence, ensure that the CF versions of both DXClient and DX Core are the same in your installation. DXClient DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. DXClient Artifact Types This section provides information about the artifact types that are currently supported by the DXClient tool. Troubleshooting DXClient Logs can be enabled and disabled as desired by DX developers and administrators through configuration options in the config.json file of DXClient. The log files can be viewed inside the logs folder within the DXClient installation folder. DXConnect DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions. Sample Pipelines for use with HCL DXClient and Automation servers A CI/CD pipeline can help automate processes in the development and test cycle, including deploying code to test and production environments. HCL DX 9.5 provides sample pipelines for use with the DXClient tooling to demonstrate how the deployment of portlets, Script Applications, Themes, DX Application, export and import of WCM libraries, etc., can be automated. Sample Pipelines for the DXClient Docker image file This sample shows how to pull DXClient docker image from the given artifactory, run the DXClient tool in the docker and then deploy a portlet, theme, and script application. It is designed to be run from a Jenkins job that provides the following parameters: Sample Pipelines for the DXClient node package file This topic describes how to install the DXClient tool in a pipeline, by providing a few sample pipelines such as pipelines for deploying a portlet, theme, and script application. It is designed to be run from a Jenkins job with the following parameters: Parent topic: Digital Experience Application deployment","title":"Deploy DX components using HCL DXClient and DXConnect"},{"location":"containerization/deploy_kubernetes_eks/","text":"Deploy DX Container to Amazon EKS Learn how to deploy different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX CF192 and later release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX CF191 and earlier release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Parent topic: Deploy DX Container to Amazon EKS","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/deploy_kubernetes_eks/#deploy-dx-container-to-amazon-eks","text":"Learn how to deploy different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX CF192 and later release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX CF191 and earlier release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Parent topic: Deploy DX Container to Amazon EKS","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/deploy_openshift/","text":"Deploying DX Container to Red Hat OpenShift Learn how to deploy different release of HCL Digital Experience (DX) 9.5 using the Red Hat OpenShift platform. Deploying DX CF192 and later release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 DX CF192 and later release Containers using the Red Hat OpenShift platform. Deploying DX CF191 and earlier release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 CF191 and earlier release Containers using the Red Hat OpenShift platform. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Deploying DX Container to Red Hat OpenShift"},{"location":"containerization/deploy_openshift/#deploying-dx-container-to-red-hat-openshift","text":"Learn how to deploy different release of HCL Digital Experience (DX) 9.5 using the Red Hat OpenShift platform. Deploying DX CF192 and later release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 DX CF192 and later release Containers using the Red Hat OpenShift platform. Deploying DX CF191 and earlier release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 CF191 and earlier release Containers using the Red Hat OpenShift platform. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Deploying DX Container to Red Hat OpenShift"},{"location":"containerization/deploy_supported_container_platforms/","text":"HCL Digital Experience 9.5 Container Deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Notes: Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Watch this video tutorial: Getting started with HCL Portal 9.5 on Docker . Watch this webinar: Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Image listing and Docker install (This page presents the latest Container Update CF file listings.) Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) Note: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Deploy DX 9.5 Container to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform. Deploy DX Container to Amazon EKS Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE) Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). Parent topic: Operator-based deployment","title":"HCL Digital Experience 9.5 Container Deployment"},{"location":"containerization/deploy_supported_container_platforms/#hcl-digital-experience-95-container-deployment","text":"This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Notes: Prior to deploying on Red Hat OpenShift or Kubernetes, it is recommended that administrators read the Limitations/Requirements section. Additional guidance about storage class and volume is available for HCL Digital Experience 9.5 container administrators. See the topic Sample Storage Class and Volume for HCL Digital Experience 9.5 Container in Amazon EKS or Red Hat OpenShift . Watch this video tutorial: Getting started with HCL Portal 9.5 on Docker . Watch this webinar: Getting started with HCL DX 9.5 on container platforms . The following container platforms are supported. Image listing and Docker install (This page presents the latest Container Update CF file listings.) Red Hat OpenShift Amazon Elastic Kubernetes Service (EKS) Microsoft Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) Note: Initial login credentials for the DX Docker image are: wpsadmin/wpsadmin . Deploy DX 9.5 Container to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform. Deploy DX Container to Amazon EKS Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE) Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and later container release along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). Parent topic: Operator-based deployment","title":"HCL Digital Experience 9.5 Container Deployment"},{"location":"containerization/deployapplication/","text":"Digital Experience applications This section provides information about the deployment of DX application artifacts by using the DXClient tool. Deploy Application The deploy-application command is used to deploy the EAR file into the WebSphere Application Server. Command description This command invokes the deploy-application tool inside DXClient. This command uses the provided files and execute the deploy application task. dxclient deploy-application Required files The following EAR file will be deployed into the WebSphere Application Server: Deployable EAR Help command This command shows the help information for deploy-application command usage: dxclient deploy-application -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered deploy application request> Required options for application deployment: Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR file path that is required while executing the deploy application task \u2013applicationFile <Absolute or relative path to deployable ear file> Use this attribute to specify the application name -applicationName <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> The values passed through the command line command override the default values. Example: dxclient deploy-application -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -applicationFile <application-file-with-path> -applicationName <application name> -dxProfileName <Profile name of the DX core server> Parent topic: DXClient Artifact Types","title":"Digital Experience applications"},{"location":"containerization/deployapplication/#digital-experience-applications","text":"This section provides information about the deployment of DX application artifacts by using the DXClient tool.","title":"Digital Experience applications"},{"location":"containerization/deployapplication/#deploy-application","text":"The deploy-application command is used to deploy the EAR file into the WebSphere Application Server. Command description This command invokes the deploy-application tool inside DXClient. This command uses the provided files and execute the deploy application task. dxclient deploy-application Required files The following EAR file will be deployed into the WebSphere Application Server: Deployable EAR Help command This command shows the help information for deploy-application command usage: dxclient deploy-application -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered deploy application request> Required options for application deployment: Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Enviornment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR file path that is required while executing the deploy application task \u2013applicationFile <Absolute or relative path to deployable ear file> Use this attribute to specify the application name -applicationName <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> The values passed through the command line command override the default values. Example: dxclient deploy-application -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -applicationFile <application-file-with-path> -applicationName <application name> -dxProfileName <Profile name of the DX core server> Parent topic: DXClient Artifact Types","title":"Deploy Application"},{"location":"containerization/deploying_custom_code_openshift/","text":"Deploying Custom Code in OpenShift This section outlines deploying custom code to HCL Digital Experience in OpenShift. Follow these steps to deploy custom code in OpenShift. Along with the benefits of moving HCL Digital Experience to a containerized and orchestrated environment, there also comes the deployment and customization process changes. As containers are short-lived, changes made during the container's runtime are transient. For this reason, a persisted volume is required to retain the HCL Digital Experience configuration settings. The server's profile directory tree, the default and obligatory mount point for persistence, currently stores this information, for example, /opt/HCL/wp_profile. The recommended approach for deploying custom applications to HCL Digital Experience running in the OpenShift Container Platform is through an automated Continuous Integration and Continuous Delivery (CI/CD) pipeline connected to a source code repository containing a pipeline script. For our purposes, we use Jenkins and GitHub. A sample Jenkinsfile and BuildConfig.yaml is provided. In an OpenShift environment, you can deploy a Jenkins instance with either of the following two (2) OpenShift Client commands. oc new-app jenkins-ephemeral oc new-app jenkins-persistent As the commands imply, the Jenkins instance can be persistent or transient. Persistent requiring a persistent volume. Configure the Jenkins server tools as needed for your custom code build requirements. Navigate to Manage Jenkins > Global Tool Configuration and add the tools needed to build your application. For our purposes, we configured Ant and JDK 8. Configure the Jenkins server credentials as needed to access you source code repository. Connection through an SSH key for the application repository and OpenShift is recommended. ssh-keygen -C \"openshift-source-builder/repo@github\" -f repo-at-github -N '' This will generate a public repo-at-github key file and a public repo-at-github.pub key file. Copy the public key contents to your clipboard. clip < ~/.ssh/repo-at-github.pub Navigate to your GitHub repository > Settings > Deploy Keys > Add deploy key . Provide a title for the key, paste the clipboard contents into the key textbox and click Add Key. Copy the private key contents to your clipboard. clip < ~/.ssh/repo-at-github Add the private key to the Jenkins server as an OpenShift secret. oc create secret generic repo-at-github --from-file=ssh-privatekey=~/.ssh/repo-at-github Link the secret to OpenShift builder. oc secrets link builder repo-at-github Create an OpenShift Build Config for building the custom application code stored in GitHub using the provided sample yaml file customized for your environment. oc create -f customized-sample-build-config.yaml kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: <name of your application pipeline> namespace: <namespace-in-openshift> spec: source: git: uri: \"git@github.com:<my custom code directory>/<my custom repository>.git\" strategy: jenkinsPipelineStrategy: jenkinsfilePath: jenkinsfile triggers: - type: GitHub github: secret: <myCustomAppSecret> Link the GitHub repository secret to the OpenShift Build Config. oc set build-secret --source bc/custom-editor-app-pipeline repo-at-github To enable OpenShift to compile and deploy the custom application with each source code commit to GitHub, configure a webhook in the GitHub repository for the OpenShift Build Config. Navigate to your GitHub repository > Settings > Webhooks > Add webhook . To find the value of the Webhook Payload URL, use the OpenShift client describe command for the Build Config: oc describe bc/<name of your application pipeline> For example, your Webhook Payload URL may look something like the following: https://api.hcl-dxdev.hcl-dx-dev.net:6443/apis/build.openshift.io/v1/namespaces/<namespace-in-openshift>/buildconfigs/<nameOfYourApplicationPipeline>/webhooks/<myCustomAppSecretFromTheSampleBuildConfigFile>/github Set the Content type of the Webhook to application/json. If your environment is not using a properly signed certificate, choose to disable SSL. Install and configure the Jenkins plugins required for your application build environment. Navigate to Manage Jenkins > Manage Plugins . From the Available tab, install the following plugins as necessary, as well as others needed based on your requirements. ``` WebSphere-Deployer Plugin - ``` Copy Artifact Plugin - ``` OpenShift Client Jenkins Plugin (if not already installed) ``` Configure the WebSphere-Deployer plugin to connect to the WebSphere Integrated Solutions Console. Restart the Jenkins server. https://<jenkins-in-openshift-url>/restart Note: If in an OpenShift environment, a trusted SSL certificate is required for the WebSphere-Deployer plugin. Both the source repository and the Jenkins server require network access to the WebSphere Integrated Solution Console route. Create the Jenkins deployment job for your application. Use the Copy Artifact plugin to provide the application build output to the deployment job. Note: It is required to configure the deployment job for the application in the Jenkins GUI as the WebSphere-Deployer Plugin is not executable from the command line, and therefore cannot be scripted. Navigate to your OpenShift ProjectName > New Item , provide a name for the application deployment job, select Freestyle Project and click OK . On the General tab of the deployment job's configuration page, check the Restrict where this project can be run check box and enter master in the Label Expression text box. Note: The WebSphere-Deployer plugin only supports deployment from a Jenkins master. Scroll down to the Build Step section of the configuration page, from the Add build step dropdown, choose Copy artifacts from another project . For the Project name , enter the name of the application build project. The default application build project name will be <namespace-in-openshift>-<name of your application pipeline> . In the Which build dropdown, select Upstream build that triggered this job . In the Artifacts to Copy text box, enter the artifacts required for your deployment. Enter any additional parameters relevant to your environment. Scroll down to the Post-Build Actions section of the configuration page, from the Add post-build action dropdown, choose Deploy To IBM WebSphere Application Server . Enter the required WebSphere information for your environment. Check the Connect using Global Security check box and enter the WebSphere administrator username and password . Note: The WebSphere administrator credentials are stored in plain text in the job config.xml files on the Jenkins master. Check the Trust SSL Certificate checkbox. Click on the Advanced... button and enter the KeyStore and TrustStore values. Ensure the Test Connection and/or Show Available Targets buttons show successful returns. Enter any additional deployment options relevant to your application and click Save . Note: The pipeline can be launched from the OpenShift Platform Console Build Configs page, the oc start-build <build-config-name> command, or with a commit to the GitHub repository. The pipeline will copy the application artifact to an HCL Digital Experience instance in OpenShift. The directory where the artifact is copied must already exist and have the appropriate permissions. Sample Jenkins file pipeline{ agent any environment { appName = 'CustomHTMLEditor' earFileName = 'CustomHTMLEditor.ear' antHome = tool 'JenkinsAnt' //Name of the pod to which the application binaries will be copied dx_instance_name = 'dx-deployment-0' //Path location within HCL Digital Experience to where to the applciation binaries will be copied app_binary_path = '/opt/HCL/wp_profile/customApps/' } options { //Value needs to match the name of the downstream application deploy job created in the Jenkins console. copyArtifactPermission('deploy-custom-editor-app') } stages { stage('Log Entry') { steps { script { openshift.withCluster() { openshift.withProject() { echo \"Building: ${appName} in project: ${openshift.project()}.\" } } } } } stage('Checkout') { steps { git 'http://github.com:<my custom code directory>/<my custom repository>.git' } } stage('Build') { steps { sh \"'${antHome}/bin/ant' clean makezips\" } } stage('Archive') { steps { archiveArtifacts 'build/dist/*.ear' } } stage('Deliver') { steps { script { openshift.withCluster() { openshift.withProject() { def result = openshift.raw ( 'cp', '${WORKSPACE}/build/dist/${earFileName} ${dx_instance_name}:${app_binary_path}') echo \"Delivery Status: ${result.out}\" } } } } } stage('Deploy'){ steps { //Value needs to match the name of the downstream application deploy job created in the Jenkins console. build 'deploy-custom-editor-app' } } stage('Log Exit') { steps { script { openshift.withCluster() { openshift.withProject() { echo \"Completed build, delivery and deployment of ${appName} in project: ${openshift.project()}.\" } } } } } } } Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift Parent topic: Digital Experience Application deployment","title":"Deploying Custom Code in OpenShift"},{"location":"containerization/deploying_custom_code_openshift/#deploying-custom-code-in-openshift","text":"This section outlines deploying custom code to HCL Digital Experience in OpenShift. Follow these steps to deploy custom code in OpenShift. Along with the benefits of moving HCL Digital Experience to a containerized and orchestrated environment, there also comes the deployment and customization process changes. As containers are short-lived, changes made during the container's runtime are transient. For this reason, a persisted volume is required to retain the HCL Digital Experience configuration settings. The server's profile directory tree, the default and obligatory mount point for persistence, currently stores this information, for example, /opt/HCL/wp_profile. The recommended approach for deploying custom applications to HCL Digital Experience running in the OpenShift Container Platform is through an automated Continuous Integration and Continuous Delivery (CI/CD) pipeline connected to a source code repository containing a pipeline script. For our purposes, we use Jenkins and GitHub. A sample Jenkinsfile and BuildConfig.yaml is provided. In an OpenShift environment, you can deploy a Jenkins instance with either of the following two (2) OpenShift Client commands. oc new-app jenkins-ephemeral oc new-app jenkins-persistent As the commands imply, the Jenkins instance can be persistent or transient. Persistent requiring a persistent volume. Configure the Jenkins server tools as needed for your custom code build requirements. Navigate to Manage Jenkins > Global Tool Configuration and add the tools needed to build your application. For our purposes, we configured Ant and JDK 8. Configure the Jenkins server credentials as needed to access you source code repository. Connection through an SSH key for the application repository and OpenShift is recommended. ssh-keygen -C \"openshift-source-builder/repo@github\" -f repo-at-github -N '' This will generate a public repo-at-github key file and a public repo-at-github.pub key file. Copy the public key contents to your clipboard. clip < ~/.ssh/repo-at-github.pub Navigate to your GitHub repository > Settings > Deploy Keys > Add deploy key . Provide a title for the key, paste the clipboard contents into the key textbox and click Add Key. Copy the private key contents to your clipboard. clip < ~/.ssh/repo-at-github Add the private key to the Jenkins server as an OpenShift secret. oc create secret generic repo-at-github --from-file=ssh-privatekey=~/.ssh/repo-at-github Link the secret to OpenShift builder. oc secrets link builder repo-at-github Create an OpenShift Build Config for building the custom application code stored in GitHub using the provided sample yaml file customized for your environment. oc create -f customized-sample-build-config.yaml kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: <name of your application pipeline> namespace: <namespace-in-openshift> spec: source: git: uri: \"git@github.com:<my custom code directory>/<my custom repository>.git\" strategy: jenkinsPipelineStrategy: jenkinsfilePath: jenkinsfile triggers: - type: GitHub github: secret: <myCustomAppSecret> Link the GitHub repository secret to the OpenShift Build Config. oc set build-secret --source bc/custom-editor-app-pipeline repo-at-github To enable OpenShift to compile and deploy the custom application with each source code commit to GitHub, configure a webhook in the GitHub repository for the OpenShift Build Config. Navigate to your GitHub repository > Settings > Webhooks > Add webhook . To find the value of the Webhook Payload URL, use the OpenShift client describe command for the Build Config: oc describe bc/<name of your application pipeline> For example, your Webhook Payload URL may look something like the following: https://api.hcl-dxdev.hcl-dx-dev.net:6443/apis/build.openshift.io/v1/namespaces/<namespace-in-openshift>/buildconfigs/<nameOfYourApplicationPipeline>/webhooks/<myCustomAppSecretFromTheSampleBuildConfigFile>/github Set the Content type of the Webhook to application/json. If your environment is not using a properly signed certificate, choose to disable SSL. Install and configure the Jenkins plugins required for your application build environment. Navigate to Manage Jenkins > Manage Plugins . From the Available tab, install the following plugins as necessary, as well as others needed based on your requirements. ``` WebSphere-Deployer Plugin - ``` Copy Artifact Plugin - ``` OpenShift Client Jenkins Plugin (if not already installed) ``` Configure the WebSphere-Deployer plugin to connect to the WebSphere Integrated Solutions Console. Restart the Jenkins server. https://<jenkins-in-openshift-url>/restart Note: If in an OpenShift environment, a trusted SSL certificate is required for the WebSphere-Deployer plugin. Both the source repository and the Jenkins server require network access to the WebSphere Integrated Solution Console route. Create the Jenkins deployment job for your application. Use the Copy Artifact plugin to provide the application build output to the deployment job. Note: It is required to configure the deployment job for the application in the Jenkins GUI as the WebSphere-Deployer Plugin is not executable from the command line, and therefore cannot be scripted. Navigate to your OpenShift ProjectName > New Item , provide a name for the application deployment job, select Freestyle Project and click OK . On the General tab of the deployment job's configuration page, check the Restrict where this project can be run check box and enter master in the Label Expression text box. Note: The WebSphere-Deployer plugin only supports deployment from a Jenkins master. Scroll down to the Build Step section of the configuration page, from the Add build step dropdown, choose Copy artifacts from another project . For the Project name , enter the name of the application build project. The default application build project name will be <namespace-in-openshift>-<name of your application pipeline> . In the Which build dropdown, select Upstream build that triggered this job . In the Artifacts to Copy text box, enter the artifacts required for your deployment. Enter any additional parameters relevant to your environment. Scroll down to the Post-Build Actions section of the configuration page, from the Add post-build action dropdown, choose Deploy To IBM WebSphere Application Server . Enter the required WebSphere information for your environment. Check the Connect using Global Security check box and enter the WebSphere administrator username and password . Note: The WebSphere administrator credentials are stored in plain text in the job config.xml files on the Jenkins master. Check the Trust SSL Certificate checkbox. Click on the Advanced... button and enter the KeyStore and TrustStore values. Ensure the Test Connection and/or Show Available Targets buttons show successful returns. Enter any additional deployment options relevant to your application and click Save . Note: The pipeline can be launched from the OpenShift Platform Console Build Configs page, the oc start-build <build-config-name> command, or with a commit to the GitHub repository. The pipeline will copy the application artifact to an HCL Digital Experience instance in OpenShift. The directory where the artifact is copied must already exist and have the appropriate permissions. Sample Jenkins file pipeline{ agent any environment { appName = 'CustomHTMLEditor' earFileName = 'CustomHTMLEditor.ear' antHome = tool 'JenkinsAnt' //Name of the pod to which the application binaries will be copied dx_instance_name = 'dx-deployment-0' //Path location within HCL Digital Experience to where to the applciation binaries will be copied app_binary_path = '/opt/HCL/wp_profile/customApps/' } options { //Value needs to match the name of the downstream application deploy job created in the Jenkins console. copyArtifactPermission('deploy-custom-editor-app') } stages { stage('Log Entry') { steps { script { openshift.withCluster() { openshift.withProject() { echo \"Building: ${appName} in project: ${openshift.project()}.\" } } } } } stage('Checkout') { steps { git 'http://github.com:<my custom code directory>/<my custom repository>.git' } } stage('Build') { steps { sh \"'${antHome}/bin/ant' clean makezips\" } } stage('Archive') { steps { archiveArtifacts 'build/dist/*.ear' } } stage('Deliver') { steps { script { openshift.withCluster() { openshift.withProject() { def result = openshift.raw ( 'cp', '${WORKSPACE}/build/dist/${earFileName} ${dx_instance_name}:${app_binary_path}') echo \"Delivery Status: ${result.out}\" } } } } } stage('Deploy'){ steps { //Value needs to match the name of the downstream application deploy job created in the Jenkins console. build 'deploy-custom-editor-app' } } stage('Log Exit') { steps { script { openshift.withCluster() { openshift.withProject() { echo \"Completed build, delivery and deployment of ${appName} in project: ${openshift.project()}.\" } } } } } } } Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift Parent topic: Digital Experience Application deployment","title":"Deploying Custom Code in OpenShift"},{"location":"containerization/deployment/","text":"Digital Experience on containerized platforms Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment. Containerization overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Container platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. DX on Docker Helm-based deployment Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Customizing the HCL DX URL for hybrid deployment HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment. Upgrade options for containerized deployments HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path. Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption.","title":"Digital Experience on containerized platforms"},{"location":"containerization/deployment/#digital-experience-on-containerized-platforms","text":"Learn how to deploy HCL Digital Experience as a cloud-native platform and optimize business-critical digital experiences for your customers. The Kubernetes container orchestration platform allows orchestration features for the automated deployment, coordination, scaling, and management of containerized applications. Originally designed by Google, now governed by the Cloud Native Computing Foundation (CNCF), and developed by Google, Red Hat, and many others, Kubernetes is now widely used by organizations of various sizes to run containers in a cloud environment. Containerization overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Container platform support matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. DX on Docker Helm-based deployment Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Operator-based deployment This section outlines the supported container platforms for HCL Digital Experience 9.5, and instructions on how to deploy to supported container environments. Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Customizing the HCL DX URL for hybrid deployment HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment. Upgrade options for containerized deployments HCL Digital Experience on containerized platforms is constantly evolving and incorporating customer feedback. Some of these improvements need extra manual steps to get to the latest version. To make this journey manageable and transparent, this topic shows all possible starting scenarios and their upgrade path. Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption.","title":"Digital Experience on containerized platforms"},{"location":"containerization/dxclient/","text":"DXClient DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. Important: DXClient version is mostly forward and backward compatible with the DX CF versions, however, in some cases it might not work as expected if the CF versions are different. Hence, ensure that the CF versions of both DXClient and DX Core are the same in your installation. Notes: DXClient is enabled in supported Kubernetes platforms from HCL Digital Experience 9.5 CF192 and later releases: DXClient is available as a Docker image from HCL DX 9.5 CF196 and later releases, See the Installation section for more details. DXClient also exists as Node.js -based CLI tool and requires Node.js to be installed as a prerequisite. However, this is deprecated in the HCL Digital Experience Container CF196 release. DXConnect DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later deployments, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Architecture Notes: HCL DX 9.5 CF19 or later version is installed on target servers, on supported on premises platforms (Microsoft Windows, Linux or IBM AIX). Beginning with HCL DX 9.5 Container Update CF192 and later releases, the DXConnect Servlet is pre-configured and started on supported Red Hat OpenShift and Kubernetes platforms that DX 9.5 containers are deployed to. For supported on premises platforms with HCL DX 9.5 CF19 and later releases, the DXConnect application needs to be installed (refer to DXConnect Installation ) and started under the Configuration Wizard ( cw_profile ) on target servers. For more information on starting the Configuration Wizard, refer to Accessing the Configuration Wizard Remember: Configuration Wizard Administrator credentials are required to access the DXConnect application. Installing using the Docker image Prerequisites: You must ensure that Docker is installed on the workstation. Note: When you upgrade to use the Docker image DXClient, you should first uninstall the nodejs DXClient. DXClient docker image comes with a script that you can use to run the docker image. This script creates a store directory, and copies the input files from the absolute path to the shared volume location. See video: CI/CD \u2013 DXClient in Container Navigate to <working-directory> folder where you wish to use DXClient from. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your HCL Digital Experience 9.5 CF196 or higher entitlements on the HCL Software License Portal. Note: If you are upgrading from the node to Docker image version of DXClient, you must first uninstall or unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Extract the DXClient.zip file locally. To work with multiple versions of DXClient, update the IMAGE_TAG reference in the scripts file under the /bin folder. For example, IMAGE_TAG=v95_CF200_20211201-1021 . By default it will be set in the executable script. Run docker load < dxclient.tar.gz. Add the execution shell script to the bin directory to the PATH variable to be able to call dxclient from any directory. export PATH=<working-directory>/bin:$PATH For Microsoft Windows platforms: use dxclient.bat script in the bin directory to the PATH variable to be able to call DXClient from any directory. Set appropriate permission. chmod xxx <working-directory>/bin Run 'dxclient -V' to verify that the dxclient command line is installed. A folder named store will be created in your working directory. This is the shared volume location to your docker container. Configuration, logger, output, and sample files under location - <working-directory>/store. Common command arguments can be pre-configured inside the config.json file available under <working-directory>/store folder. A sample configuration file that could be used on-premises platforms in standalone, cluster platforms is also available under <working-directory>/store/samples/sample-configurations/default-config.json for reference. DXClient installation configuration Common command arguments can be pre-configured inside the config.json file available under dist/src/configuration folder. A sample configuration file that could be used for any of the supported Kubernetes platforms is also available under samples/sample-configurations.json for reference. { \"enableLogger\": true, \"enableBackup\": \"false\", \"dxProtocol\": \"\", \"hostname\": \"\", \"dxPort\": \"\", \"dxContextRoot\":\"/wps\", \"xmlConfigPath\": \"/wps/config\", \"dxUsername\": \"\", \"dxPassword\": \"\", \"dxSoapPort\": \"10033\", \"dxProfileName\": \"wp_profile\", \"dxProfilePath\": \"\", \"dxConnectHostname\": \"\", \"dxConnectUsername\": \"\", \"dxConnectPassword\": \"\", \"dxConnectPort\": \"10202\", \"dxWASUsername\": \"\", \"dxWASPassword\": \"\", \"dxConnectProtocol\": \"https\", \"wcmSiteArea\": \"\", \"wcmContentPath\": \"\", \"wcmContentName\": \"\", \"contenthandlerPath\": \"/wps/mycontenthandler\", \"wcmContentId\": \"\", \"restoreAsPublished\": false, \"wcmLibraryId\": \"\", \"virtualPortalContext\": \"\", \"projectContext\": \"\", \"wcmLibraryName\": \"\", \"lastModifiedAfter\": \"\", \"damAPIPort\": \"\", \"ringAPIPort\": \"\", \"damAPIVersion\": \"\", \"ringAPIVersion\": \"\", \"wcmProjectName\": \"\", \"targetHostname\": \"\", \"targetDxConnectPort\": \"\", \"targetDxConnectUsername\":\"\", \"targetDxConnectPassword\":\"\", \"targetDxProfileName\": \"\" } Installing using the node package file (deprecated in CF196) Prerequisites: Node.js version 12.18.3 is the minimum supported version, and must be installed on the local workstation or automation server. See video: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience Container Update CF194 Note: DXClient node package is deprecated in the HCL Digital Experience Container CF196 release. It might be removed in the future releases. You are encouraged to use the DXClient Docker package from CF Container release CF196 and later. Complete the following steps to install the DXClient tool in your local development workstation or automation server. Note: If you are upgrading from CF19, CF191, or later releases, you should first unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Ensure that Node.js version 12.18.3 or later version is installed to the local workstation. The DXClient tool is supported on Microsoft Windows, Linux, and Apple macOS workstations and automation servers. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your DX 9.5 CF19 or later entitlements on the HCL Software License Portal . Reference the Docker topic for the latest list of HCL DX 9.5 files available for download. Extract the DXClient.zip file locally. From the extracted folder, run the following command. For Linux and Apple macOS platforms: make install For Microsoft Windows platforms: make_install.bat The following commands are run: Run the following command to link your application to the local npm module in your machine. Refer to the following Notes section before you proceed. For Linux and Apple MacOS platforms: make link For Microsoft Windows platforms: make_link.bat Notes: Avoid using this command when scripting deployments from an automation server (for example, in pipelines) as there is a chance of picking up the wrong dependencies during tool version upgrades. If the link command is not used (such as on automation servers), then use the following command to run the application: For Linux and Apple MacOS platforms: ./bin/dxclient For Microsoft Windows platforms: node bin/dxclient DXClient node uninstalling To uninstall the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make clean For Microsoft Windows platforms: make uninstall.bat To unlink the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make unlink For Microsoft Windows platforms: make_unlink.bat Verify the DXClient installation Successful installation of the DXClient tool can be checked by using the \" dxclient -V \" command, which should show the version of the DXClient tool installed. Once installed, commands can be executed using the DXClient tool to perform CI / CD actions on HCL DX 9.5 servers. Notes: Refer to the list of features that were released in the following HCL DX 9.5 Container releases: HCL DX 9.5 CF201 release: An optional parameter requestId added to Deploy theme , Deploy application , Restart DX Core server , and Manage virtual portals . Retrieve feature added to the Resource environment provider . Accessing ConfigWizard in container environment Note that a few parameters are deprecated and replaced with new parameters in the DX Core configuration reports. For information, see DX Core server configuration report HCL DX 9.5 CF200 release: Exporting and Importing WCM libraries DX Core server configuration report HCL DX 9.5 CF199 release: DAM Staging Create credential vault slot Create syndication relation Export and import multiple resource environment providers Specify the context root for exporting and importing personalization rules HCL DX 9.5 CF198 release: List DAM schemas Personalization export and import rules Resource environment provider Manage virtual portals HCL DX 9.5 CF197 release: Undeploy portlets Deploy and undeploy themes Deploy application manage get-syndication report Restart Core Delete DAM schema HCL DX 9.5 CF196 release: Shared library HCL DX 9.5 CF195 release: Undeploy theme MLS export and import of WCM library HCL DX 9.5 CF193 release: Restart DX Core server Deploy Application Managing syndicators Managing subscribers HCL DX 9.5 CF192 release: Undeploy script applications Deploy theme (EAR and WebDAV based) HCL DX 9.5 CF19 release: Deploy Portlets or Undeploy portlets Deploy script applications XML Access Restore Script Application DXClient commands Command syntax conventions: dxclient [command] [options] Use the following command to execute the deploy portlet action: dxclient deploy-portlet [options] Use the following command to execute the undeploy portlet action: dxclient undeploy-portlet [options] Use the following command to execute the xmlaccess action: dxclient xmlaccess [options] Use the following command to execute the pull script application action: dxclient deploy-scriptapplication pull [options] Use the following command to execute the push script application action: dxclient deploy-scriptapplication push [options] Use the following command to execute the undeploy script application action: dxclient undeploy-scriptapplication [options] Use the following command to execute the restore script application action: dxclient restore-scriptapplication [options] Use the following command to execute the deploy application action: dxclient deploy-application [options] Use the following command to execute the DX Core restart action: dxclient restart-dx-core Use the following command to execute manage-subscriber action: dxclient manage-subscriber -h Use the following command to execute manage-syndicator action: dxclient manage-syndicator -h Use the following command to execute the deploy theme action: dxclient deploy-theme [options] Use the following command to execute the undeploy theme action: dxclient undeploy-theme [options] Use the following command to execute the manage-syndicator get-syndication-report action: dxclient manage-syndicator get-syndication-report [options] Use the following command to execute the shared-library action: dxclient shared-library [options] Use the following command to execute the delete DAM schema action: dxclient delete-dam-schema [options] Use the following command to list all DAM schemas present: dxclient list-dam-schemas [options] Use the following command to export the personalization rules from the target server: dxclient pzn-export [options] Use the following command to import the personalization rules into the target server: dxclient pzn-import [options] Use the following command to manage virtual portal tasks in the DX server: dxclient manage-virtual-portal [options] Use the following command to register subscriber: dxclient manage-dam-staging register-dam-subscriber [options] Use the following command to deregister subscriber: dxclient manage-dam-staging deregister-dam-subscriber [options] Use the following command to trigger manual sync: dxclient manage-dam-staging trigger-staging [options] Use the following command to create credential vault slot in the DX server: dxclient create-credential-vault [options] Use the following command to create the syndication relation between syndicator and subscriber in DX server: dxclient create-syndication-relation [options] Use the following command to create, update, delete, export or import a custom property from an existing Resource Environment Provider: dxclient resource-env-provider [options] Use this command to export WCM libraries dxclient wcm-library-export Use this command to import WCM libraries dxclient wcm-library-import Use the dx-core-configuration-reports command to get a summary of the configurations of a single DX server or both source and target DX servers, which users can use to compare. dxclient dx-core-configuration-reports [OPTIONS] DXClient Help commands The following commands show the Help documents for DXClient command usage. Use the following commands to display the Help document for DXClient: dxclient dxclient -h, --help Use the following command to display the DXClient version number: dxclient -V, --version Use the following command to display the detailed help for a specific command: dxclient help [command] Accessing the ConfigWizard admin console in a container environment You can access the ConfigWizard admin console in a container environment from your local system. For more information, refer to Accessing the ConfigWizard admin console in a container environment . Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"DXClient"},{"location":"containerization/dxclient/#dxclient","text":"DXClient is a tool that helps developers and administrators manage tasks, such as uploading one or more portlets or Script Applications, from source development environments to target HCL DX 9.5 deployments. This tool is capable of taking artifacts developed locally and deploying them to DX 9.5 servers deployed to supported on-premises platforms in standalone, cluster, or farm-topologies and supported Kubernetes platforms. Important: DXClient version is mostly forward and backward compatible with the DX CF versions, however, in some cases it might not work as expected if the CF versions are different. Hence, ensure that the CF versions of both DXClient and DX Core are the same in your installation. Notes: DXClient is enabled in supported Kubernetes platforms from HCL Digital Experience 9.5 CF192 and later releases: DXClient is available as a Docker image from HCL DX 9.5 CF196 and later releases, See the Installation section for more details. DXClient also exists as Node.js -based CLI tool and requires Node.js to be installed as a prerequisite. However, this is deprecated in the HCL Digital Experience Container CF196 release. DXConnect DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later deployments, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands.","title":"DXClient"},{"location":"containerization/dxclient/#architecture","text":"Notes: HCL DX 9.5 CF19 or later version is installed on target servers, on supported on premises platforms (Microsoft Windows, Linux or IBM AIX). Beginning with HCL DX 9.5 Container Update CF192 and later releases, the DXConnect Servlet is pre-configured and started on supported Red Hat OpenShift and Kubernetes platforms that DX 9.5 containers are deployed to. For supported on premises platforms with HCL DX 9.5 CF19 and later releases, the DXConnect application needs to be installed (refer to DXConnect Installation ) and started under the Configuration Wizard ( cw_profile ) on target servers. For more information on starting the Configuration Wizard, refer to Accessing the Configuration Wizard Remember: Configuration Wizard Administrator credentials are required to access the DXConnect application.","title":"Architecture"},{"location":"containerization/dxclient/#installing-using-the-docker-image","text":"Prerequisites: You must ensure that Docker is installed on the workstation. Note: When you upgrade to use the Docker image DXClient, you should first uninstall the nodejs DXClient. DXClient docker image comes with a script that you can use to run the docker image. This script creates a store directory, and copies the input files from the absolute path to the shared volume location. See video: CI/CD \u2013 DXClient in Container Navigate to <working-directory> folder where you wish to use DXClient from. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your HCL Digital Experience 9.5 CF196 or higher entitlements on the HCL Software License Portal. Note: If you are upgrading from the node to Docker image version of DXClient, you must first uninstall or unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Extract the DXClient.zip file locally. To work with multiple versions of DXClient, update the IMAGE_TAG reference in the scripts file under the /bin folder. For example, IMAGE_TAG=v95_CF200_20211201-1021 . By default it will be set in the executable script. Run docker load < dxclient.tar.gz. Add the execution shell script to the bin directory to the PATH variable to be able to call dxclient from any directory. export PATH=<working-directory>/bin:$PATH For Microsoft Windows platforms: use dxclient.bat script in the bin directory to the PATH variable to be able to call DXClient from any directory. Set appropriate permission. chmod xxx <working-directory>/bin Run 'dxclient -V' to verify that the dxclient command line is installed. A folder named store will be created in your working directory. This is the shared volume location to your docker container. Configuration, logger, output, and sample files under location - <working-directory>/store. Common command arguments can be pre-configured inside the config.json file available under <working-directory>/store folder. A sample configuration file that could be used on-premises platforms in standalone, cluster platforms is also available under <working-directory>/store/samples/sample-configurations/default-config.json for reference.","title":"Installing using the Docker image"},{"location":"containerization/dxclient/#dxclient-installation-configuration","text":"Common command arguments can be pre-configured inside the config.json file available under dist/src/configuration folder. A sample configuration file that could be used for any of the supported Kubernetes platforms is also available under samples/sample-configurations.json for reference. { \"enableLogger\": true, \"enableBackup\": \"false\", \"dxProtocol\": \"\", \"hostname\": \"\", \"dxPort\": \"\", \"dxContextRoot\":\"/wps\", \"xmlConfigPath\": \"/wps/config\", \"dxUsername\": \"\", \"dxPassword\": \"\", \"dxSoapPort\": \"10033\", \"dxProfileName\": \"wp_profile\", \"dxProfilePath\": \"\", \"dxConnectHostname\": \"\", \"dxConnectUsername\": \"\", \"dxConnectPassword\": \"\", \"dxConnectPort\": \"10202\", \"dxWASUsername\": \"\", \"dxWASPassword\": \"\", \"dxConnectProtocol\": \"https\", \"wcmSiteArea\": \"\", \"wcmContentPath\": \"\", \"wcmContentName\": \"\", \"contenthandlerPath\": \"/wps/mycontenthandler\", \"wcmContentId\": \"\", \"restoreAsPublished\": false, \"wcmLibraryId\": \"\", \"virtualPortalContext\": \"\", \"projectContext\": \"\", \"wcmLibraryName\": \"\", \"lastModifiedAfter\": \"\", \"damAPIPort\": \"\", \"ringAPIPort\": \"\", \"damAPIVersion\": \"\", \"ringAPIVersion\": \"\", \"wcmProjectName\": \"\", \"targetHostname\": \"\", \"targetDxConnectPort\": \"\", \"targetDxConnectUsername\":\"\", \"targetDxConnectPassword\":\"\", \"targetDxProfileName\": \"\" }","title":"DXClient installation configuration"},{"location":"containerization/dxclient/#installing-using-the-node-package-file-deprecated-in-cf196","text":"Prerequisites: Node.js version 12.18.3 is the minimum supported version, and must be installed on the local workstation or automation server. See video: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience Container Update CF194 Note: DXClient node package is deprecated in the HCL Digital Experience Container CF196 release. It might be removed in the future releases. You are encouraged to use the DXClient Docker package from CF Container release CF196 and later. Complete the following steps to install the DXClient tool in your local development workstation or automation server. Note: If you are upgrading from CF19, CF191, or later releases, you should first unlink the current version using the following command before installing the newer version. Syntax for Linux and Apple macOS platforms: make unlink Syntax for Microsoft Windows platforms: make_unlink.bat Ensure that Node.js version 12.18.3 or later version is installed to the local workstation. The DXClient tool is supported on Microsoft Windows, Linux, and Apple macOS workstations and automation servers. Download the DXClient.zip file (DXClient_VX_XXXXXXXX-XXXX.zip) to a local directory on the local workstation from your DX 9.5 CF19 or later entitlements on the HCL Software License Portal . Reference the Docker topic for the latest list of HCL DX 9.5 files available for download. Extract the DXClient.zip file locally. From the extracted folder, run the following command. For Linux and Apple macOS platforms: make install For Microsoft Windows platforms: make_install.bat The following commands are run: Run the following command to link your application to the local npm module in your machine. Refer to the following Notes section before you proceed. For Linux and Apple MacOS platforms: make link For Microsoft Windows platforms: make_link.bat Notes: Avoid using this command when scripting deployments from an automation server (for example, in pipelines) as there is a chance of picking up the wrong dependencies during tool version upgrades. If the link command is not used (such as on automation servers), then use the following command to run the application: For Linux and Apple MacOS platforms: ./bin/dxclient For Microsoft Windows platforms: node bin/dxclient DXClient node uninstalling To uninstall the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make clean For Microsoft Windows platforms: make uninstall.bat To unlink the DXClient tool, perform the following commands: For Linux and Apple MacOS platforms: make unlink For Microsoft Windows platforms: make_unlink.bat","title":"Installing using the node package file (deprecated in CF196)"},{"location":"containerization/dxclient/#verify-the-dxclient-installation","text":"Successful installation of the DXClient tool can be checked by using the \" dxclient -V \" command, which should show the version of the DXClient tool installed. Once installed, commands can be executed using the DXClient tool to perform CI / CD actions on HCL DX 9.5 servers. Notes: Refer to the list of features that were released in the following HCL DX 9.5 Container releases: HCL DX 9.5 CF201 release: An optional parameter requestId added to Deploy theme , Deploy application , Restart DX Core server , and Manage virtual portals . Retrieve feature added to the Resource environment provider . Accessing ConfigWizard in container environment Note that a few parameters are deprecated and replaced with new parameters in the DX Core configuration reports. For information, see DX Core server configuration report HCL DX 9.5 CF200 release: Exporting and Importing WCM libraries DX Core server configuration report HCL DX 9.5 CF199 release: DAM Staging Create credential vault slot Create syndication relation Export and import multiple resource environment providers Specify the context root for exporting and importing personalization rules HCL DX 9.5 CF198 release: List DAM schemas Personalization export and import rules Resource environment provider Manage virtual portals HCL DX 9.5 CF197 release: Undeploy portlets Deploy and undeploy themes Deploy application manage get-syndication report Restart Core Delete DAM schema HCL DX 9.5 CF196 release: Shared library HCL DX 9.5 CF195 release: Undeploy theme MLS export and import of WCM library HCL DX 9.5 CF193 release: Restart DX Core server Deploy Application Managing syndicators Managing subscribers HCL DX 9.5 CF192 release: Undeploy script applications Deploy theme (EAR and WebDAV based) HCL DX 9.5 CF19 release: Deploy Portlets or Undeploy portlets Deploy script applications XML Access Restore Script Application","title":"Verify the DXClient installation"},{"location":"containerization/dxclient/#dxclient-commands","text":"Command syntax conventions: dxclient [command] [options] Use the following command to execute the deploy portlet action: dxclient deploy-portlet [options] Use the following command to execute the undeploy portlet action: dxclient undeploy-portlet [options] Use the following command to execute the xmlaccess action: dxclient xmlaccess [options] Use the following command to execute the pull script application action: dxclient deploy-scriptapplication pull [options] Use the following command to execute the push script application action: dxclient deploy-scriptapplication push [options] Use the following command to execute the undeploy script application action: dxclient undeploy-scriptapplication [options] Use the following command to execute the restore script application action: dxclient restore-scriptapplication [options] Use the following command to execute the deploy application action: dxclient deploy-application [options] Use the following command to execute the DX Core restart action: dxclient restart-dx-core Use the following command to execute manage-subscriber action: dxclient manage-subscriber -h Use the following command to execute manage-syndicator action: dxclient manage-syndicator -h Use the following command to execute the deploy theme action: dxclient deploy-theme [options] Use the following command to execute the undeploy theme action: dxclient undeploy-theme [options] Use the following command to execute the manage-syndicator get-syndication-report action: dxclient manage-syndicator get-syndication-report [options] Use the following command to execute the shared-library action: dxclient shared-library [options] Use the following command to execute the delete DAM schema action: dxclient delete-dam-schema [options] Use the following command to list all DAM schemas present: dxclient list-dam-schemas [options] Use the following command to export the personalization rules from the target server: dxclient pzn-export [options] Use the following command to import the personalization rules into the target server: dxclient pzn-import [options] Use the following command to manage virtual portal tasks in the DX server: dxclient manage-virtual-portal [options] Use the following command to register subscriber: dxclient manage-dam-staging register-dam-subscriber [options] Use the following command to deregister subscriber: dxclient manage-dam-staging deregister-dam-subscriber [options] Use the following command to trigger manual sync: dxclient manage-dam-staging trigger-staging [options] Use the following command to create credential vault slot in the DX server: dxclient create-credential-vault [options] Use the following command to create the syndication relation between syndicator and subscriber in DX server: dxclient create-syndication-relation [options] Use the following command to create, update, delete, export or import a custom property from an existing Resource Environment Provider: dxclient resource-env-provider [options] Use this command to export WCM libraries dxclient wcm-library-export Use this command to import WCM libraries dxclient wcm-library-import Use the dx-core-configuration-reports command to get a summary of the configurations of a single DX server or both source and target DX servers, which users can use to compare. dxclient dx-core-configuration-reports [OPTIONS]","title":"DXClient commands"},{"location":"containerization/dxclient/#dxclient-help-commands","text":"The following commands show the Help documents for DXClient command usage. Use the following commands to display the Help document for DXClient: dxclient dxclient -h, --help Use the following command to display the DXClient version number: dxclient -V, --version Use the following command to display the detailed help for a specific command: dxclient help [command]","title":"DXClient Help commands"},{"location":"containerization/dxclient/#accessing-the-configwizard-admin-console-in-a-container-environment","text":"You can access the ConfigWizard admin console in a container environment from your local system. For more information, refer to Accessing the ConfigWizard admin console in a container environment . Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Accessing the ConfigWizard admin console in a container environment"},{"location":"containerization/dxclientartifacts/","text":"DXClient Artifact Types This section provides information about the artifact types that are currently supported by the DXClient tool. Limitation: If deploying CICD artifacts using DXClient to the Red Hat OpenShift environment, you might receive failure messages while you run the deploy-theme, deploy-application, or restart-dx-core commands. This might happen because of a connection getting closed due to timeout before the response is ready. In such situations, before re-triggering the request, we advise you to check your target server to verify if the application has been deployed or the server is up, as the request was already triggered from the client-side. Portlets This topic provides information about the deployment and undeployment of portlets. Script applications This topic provides information about the deployment, undeployment, and restoration of script applications. XML Access This topic provides information about the xmlaccess command that is used to export or import portlet configurations. Themes This topic provides information about the deployment and undeployment of themes artifacts. Digital Experience applications This section provides information about the deployment of DX application artifacts by using the DXClient tool. DX Core server This topic provides information about restarting the DX Core server and on creating core server configuration report using the DXClient tool. The dx-core-configuration-reports command is used to generate the differential reports on various core configurations between two DX server nodes. Exporting and importing WCM libraries This section provides information about how to export and import WCM libraries using DXClient. Managing Web Content Syndicators and Subscribers using DXClient The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers . Create or update credential vault slot This topic describes the commands that are used to create or update credential vault slot in the DX server. Shared library Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others. DAM artifacts This section contains the commands for working with Digital Asset Management (DAM) schemas, managing DAM staging for subscription, or configuring periodic sync. Personalization rules This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user. Resource environment provider This topic describes the commands that are used to create, update, delete, and retrieve custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers. Managing virtual portals This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals. Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"DXClient Artifact Types"},{"location":"containerization/dxclientartifacts/#dxclient-artifact-types","text":"This section provides information about the artifact types that are currently supported by the DXClient tool. Limitation: If deploying CICD artifacts using DXClient to the Red Hat OpenShift environment, you might receive failure messages while you run the deploy-theme, deploy-application, or restart-dx-core commands. This might happen because of a connection getting closed due to timeout before the response is ready. In such situations, before re-triggering the request, we advise you to check your target server to verify if the application has been deployed or the server is up, as the request was already triggered from the client-side. Portlets This topic provides information about the deployment and undeployment of portlets. Script applications This topic provides information about the deployment, undeployment, and restoration of script applications. XML Access This topic provides information about the xmlaccess command that is used to export or import portlet configurations. Themes This topic provides information about the deployment and undeployment of themes artifacts. Digital Experience applications This section provides information about the deployment of DX application artifacts by using the DXClient tool. DX Core server This topic provides information about restarting the DX Core server and on creating core server configuration report using the DXClient tool. The dx-core-configuration-reports command is used to generate the differential reports on various core configurations between two DX server nodes. Exporting and importing WCM libraries This section provides information about how to export and import WCM libraries using DXClient. Managing Web Content Syndicators and Subscribers using DXClient The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers . Create or update credential vault slot This topic describes the commands that are used to create or update credential vault slot in the DX server. Shared library Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others. DAM artifacts This section contains the commands for working with Digital Asset Management (DAM) schemas, managing DAM staging for subscription, or configuring periodic sync. Personalization rules This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user. Resource environment provider This topic describes the commands that are used to create, update, delete, and retrieve custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers. Managing virtual portals This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals. Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"DXClient Artifact Types"},{"location":"containerization/dxconnect/","text":"DXConnect DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions. Authentication DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in an HCL DX 9.5 CF19 and later deployment, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTPS connection from a client development workstation or automation server to a target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Authentication DXConnect requires the cw_profile Administrator security role to access the application servlet APIs. DXConnect Installation To install DXConnect use the command below: ./ConfigEngine.sh install-dxconnect-application This task will not only install the DxConnect application, but it will create the \"DXC_ConfigSettings\" WAS Resource Environment Provider and will create two custom properties in that REP: DXCONNECT_MAX_MEMORY_SIZE_MB DXCONNECT_MAX_FILE_SIZE_MB To remove DXConnect use the command below: ./ConfigEngine.sh remove-dxconnect-application To re-install DXConnect use the command below: ./ConfigEngine.sh reinstall-dxconnect-application Notes: In Standalone and Cluster setups, the ConfigEngine task should be run under the wp_profile to have DXConnect installed in the correct location, and a restart of the cw_profile server may be required. To verify it is installed on a given HCL DX Server 9.5 with CF19 or later, navigate to the Configuration Wizard Admin console and then under Enterprise Applications . The dxconnect application will appear on the console as shown in the example below. For more information on accessing and working with the Configuration Wizard, refer to Accessing the Configuration Wizard topics. In Red Hat OpenShift, the route for DXConnect is available under the name dx-deployment-service-dxconnect. For the other supported platforms, there is only one route path as usual. Accessing the ConfigWizard admin console in a container environment You can access the ConfigWizard admin console in a container environment from your local system. For more information, refer to Accessing the ConfigWizard admin console in a container environment . Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"DXConnect"},{"location":"containerization/dxconnect/#dxconnect","text":"DXConnect is a servlet-based internal application deployed on top of IBM WebSphere Application Server in the HCL DX 9.5 CF19 and later releases, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTP or HTTPS connection from a client development machine or remote server to a source or target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. This topic covers the DXConnect installation and configuration instructions.","title":"DXConnect"},{"location":"containerization/dxconnect/#authentication","text":"DXConnect is a servlet-based application deployed on top of IBM WebSphere Application Server in an HCL DX 9.5 CF19 and later deployment, under the Configuration Wizard profile - cw_profile . DXConnect enables the DXClient tool to connect over an HTTPS connection from a client development workstation or automation server to a target HCL DX 9.5 server to execute certain tasks requested via DXClient commands. Authentication DXConnect requires the cw_profile Administrator security role to access the application servlet APIs. DXConnect Installation To install DXConnect use the command below: ./ConfigEngine.sh install-dxconnect-application This task will not only install the DxConnect application, but it will create the \"DXC_ConfigSettings\" WAS Resource Environment Provider and will create two custom properties in that REP: DXCONNECT_MAX_MEMORY_SIZE_MB DXCONNECT_MAX_FILE_SIZE_MB To remove DXConnect use the command below: ./ConfigEngine.sh remove-dxconnect-application To re-install DXConnect use the command below: ./ConfigEngine.sh reinstall-dxconnect-application Notes: In Standalone and Cluster setups, the ConfigEngine task should be run under the wp_profile to have DXConnect installed in the correct location, and a restart of the cw_profile server may be required. To verify it is installed on a given HCL DX Server 9.5 with CF19 or later, navigate to the Configuration Wizard Admin console and then under Enterprise Applications . The dxconnect application will appear on the console as shown in the example below. For more information on accessing and working with the Configuration Wizard, refer to Accessing the Configuration Wizard topics. In Red Hat OpenShift, the route for DXConnect is available under the name dx-deployment-service-dxconnect. For the other supported platforms, there is only one route path as usual.","title":"Authentication"},{"location":"containerization/dxconnect/#accessing-the-configwizard-admin-console-in-a-container-environment","text":"You can access the ConfigWizard admin console in a container environment from your local system. For more information, refer to Accessing the ConfigWizard admin console in a container environment . Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Accessing the ConfigWizard admin console in a container environment"},{"location":"containerization/dxcoreserver/","text":"DX Core server This topic provides information about restarting the DX Core server and on creating core server configuration report using the DXClient tool. The dx-core-configuration-reports command is used to generate the differential reports on various core configurations between two DX server nodes. Restart DX Core server Important: Running the restart-dx-core command in the Kubernetes-based deployments might not restart all pods as expected, but this limitation will be addressed in the future releases. For now, if you want to restart all pods, use the Kubernetes interfaces such as kubectl . The restart-dx-core command is used to restart the DX Core server. Command description This command invokes the restart-dx-core tool inside the DXClient and runs the DX Core restart action. dxclient restart-dx-core Help command This command shows the help information for restart-dx-core command usage: dxclient restart-dx-core -h Command options Use this attribute to specify the username that is required for authenticating with the DX Core: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core: -dxPassword <value> Use this attribute to specify the ConfigWizard home that is required for authenticating to cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile : -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile : -dxConnectPassword <value> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> The values that are passed through the command line override the default values. Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered restart request> Example: dxclient restart-dx-core -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server> DX Core server configuration report Command description The dx-core-configuration-reports command shows the summary of the configurations of a single DX server or both source and target DX servers, which users can use to compare. dxclient dx-core-configuration-reports [OPTIONS] Help command This command shows the help information for dx-core-configuration-reports command usage: dxclient dx-core-configuration-reports summary-report -h Command options Use this attribute to specify the ConfigWizard home that is required for authenticating to cw_profile : -hostname <value> Use this attribute to specify the port number of cw_profile : -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to cw_profile : -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the host name of the target DX core server: -targetHostname <value> Use this attribute to specify the port number of the target cw_profile server: -targetDxConnectPort <value> Use this attribute to specify the user name of the target server: -targetDxConnectUsername <value> Use this attribute to specify the password of the target server: -targetDxConnectPassword <value> Use this attribute to specify the profile name of the target server: -targetDxProfileName <Profile name of the DX core server> Notes: The target server details are needed only when the user needs to generate the summary of the configurations of both source and target servers. The following list shows some of the deprecated parameters and the new parameters that replace them in CF201. It is recommended that you start using the new parameters because the old parameters might be removed in the upcoming releases: -dxConnectHostname replaced by -hostname -targetServerHostname replaced by -targetHostname -targetServerPort -> replaced by -targetDxConnectPort -targetServerUsername replaced by -targetDxConnectUsername -targetServerPassword replaced by -targetDxConnectPassword -targetServerProfileName replaced by -targetDxProfileName Example: ``` dxclient dx-core-configuration-reports summary-report -hostname -dxConnectUsername -dxConnectPassword -dxConnectPort -targetHostname -targetDxConnectUsername -targetDxConnectPassword -targetDxConnectPort ``` Parent topic: DXClient Artifact Types","title":"DX Core server"},{"location":"containerization/dxcoreserver/#dx-core-server","text":"This topic provides information about restarting the DX Core server and on creating core server configuration report using the DXClient tool. The dx-core-configuration-reports command is used to generate the differential reports on various core configurations between two DX server nodes.","title":"DX Core server"},{"location":"containerization/dxcoreserver/#restart-dx-core-server","text":"Important: Running the restart-dx-core command in the Kubernetes-based deployments might not restart all pods as expected, but this limitation will be addressed in the future releases. For now, if you want to restart all pods, use the Kubernetes interfaces such as kubectl . The restart-dx-core command is used to restart the DX Core server. Command description This command invokes the restart-dx-core tool inside the DXClient and runs the DX Core restart action. dxclient restart-dx-core Help command This command shows the help information for restart-dx-core command usage: dxclient restart-dx-core -h Command options Use this attribute to specify the username that is required for authenticating with the DX Core: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the DX Core: -dxPassword <value> Use this attribute to specify the ConfigWizard home that is required for authenticating to cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile : -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile : -dxConnectPassword <value> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> The values that are passed through the command line override the default values. Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered restart request> Example: dxclient restart-dx-core -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server>","title":"Restart DX Core server"},{"location":"containerization/dxcoreserver/#dx-core-server-configuration-report","text":"Command description The dx-core-configuration-reports command shows the summary of the configurations of a single DX server or both source and target DX servers, which users can use to compare. dxclient dx-core-configuration-reports [OPTIONS] Help command This command shows the help information for dx-core-configuration-reports command usage: dxclient dx-core-configuration-reports summary-report -h Command options Use this attribute to specify the ConfigWizard home that is required for authenticating to cw_profile : -hostname <value> Use this attribute to specify the port number of cw_profile : -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to cw_profile : -dxConnectPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the host name of the target DX core server: -targetHostname <value> Use this attribute to specify the port number of the target cw_profile server: -targetDxConnectPort <value> Use this attribute to specify the user name of the target server: -targetDxConnectUsername <value> Use this attribute to specify the password of the target server: -targetDxConnectPassword <value> Use this attribute to specify the profile name of the target server: -targetDxProfileName <Profile name of the DX core server> Notes: The target server details are needed only when the user needs to generate the summary of the configurations of both source and target servers. The following list shows some of the deprecated parameters and the new parameters that replace them in CF201. It is recommended that you start using the new parameters because the old parameters might be removed in the upcoming releases: -dxConnectHostname replaced by -hostname -targetServerHostname replaced by -targetHostname -targetServerPort -> replaced by -targetDxConnectPort -targetServerUsername replaced by -targetDxConnectUsername -targetServerPassword replaced by -targetDxConnectPassword -targetServerProfileName replaced by -targetDxProfileName Example: ``` dxclient dx-core-configuration-reports summary-report -hostname -dxConnectUsername -dxConnectPassword -dxConnectPort -targetHostname -targetDxConnectUsername -targetDxConnectPassword -targetDxConnectPort ``` Parent topic: DXClient Artifact Types","title":"DX Core server configuration report"},{"location":"containerization/dxtools_dxctl/","text":"dxctl Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments About this task Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note: Experience API must be enabled to deploy Content Composer and Digital Asset Management. Prerequisites The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl . Creating a deployment Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information. Updating a deployment Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note: When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command. Deleting a deployment There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Main usage Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help dxctl help Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false Parent topic: Operator-based deployment","title":"dxctl"},{"location":"containerization/dxtools_dxctl/#dxctl","text":"Learn how to use dxctl for custom HCL Digital Experience 9.5 container deployments","title":"dxctl"},{"location":"containerization/dxtools_dxctl/#about-this-task","text":"Administrators can use the dxctl tool provided with Container Update CF19 and later releases to define and configure custom DX container deployments. See the following guidance: Video : Using dxctl to Deploy DX Portal on OpenShift General help for the dxctl tool or help related for sub-commands ( create , update , collect , and destroy ) and the command syntax are found with --help . dxctl can be used to deploy DX using a properties file. Sample properties files are included in the dxctl/properties directory. The properties files function as follows: Full deployment config: full-deployment.properties hybrid.enabled: false hybrid.host: onprem_hostname.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/full-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/full-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/full-deployment.properties Hybrid deployment config: hybrid-deployment.properties hybrid.enabled: true hybrid.host: aws-hybrid.sample-dx-deploy.com hybrid.port: 10042 For Mac: ./mac/dxctl --deploy -p properties/hybrid-deployment.properties For Windows: win\\dxctl.exe -\u2013deploy -p properties/hybrid-deployment.properties For Linux: ./linux/dxctl -\u2013deploy -p properties/hybrid-deployment.properties These create a hybrid deployment with Experience API, Content Composer, and Digital Asset Management. You can disable any of these features by making a copy of the hybrid file and setting the value to false to disable it. Example: composer.enabled: false disables Content Composer. Note: Experience API must be enabled to deploy Content Composer and Digital Asset Management.","title":"About this task"},{"location":"containerization/dxtools_dxctl/#prerequisites","text":"The following are the prerequisites for using dxctl . Before running the dxctl tool, you must log in on the targeted cluster using your platform's cloud-specific command-line interface (CLI), such as Azure CLI (az), gcloud CLI, AWS CLI, OpenShift CLI (oc), etc. For example, in Red Hat OpenShift, you must use oc login . dxctl does not deploy the DxDeployment custom resource definition. You must run the ./scripts/deployCrd.sh before using dxctl .","title":"Prerequisites"},{"location":"containerization/dxtools_dxctl/#creating-a-deployment","text":"Follow these steps to create a deployment. You must copy the properties file once a deployment is created. Use the copied file to perform a deployment and maintain and update a deployment. For example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Change the settings. For example, change dx.namespace: to myfirst-dx-deployment . ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: For OpenShift deployments, /linux/dxctl --deploy is all you need. For all other Kubernetes environments (EKS, GKE, etc.), you need to generate a TLS certification and private key. See the Generate TLS Certificate topic for more information.","title":"Creating a deployment"},{"location":"containerization/dxtools_dxctl/#updating-a-deployment","text":"Limitation: If you have a DX-only deployment (a deployment that contains only DX without any other features, such as the Experience API, Content Composer, or Digital Asset Management) installed using the deployment script, the dxctl tool cannot be used to update this deployment. You may continue to use the DX deployment script to update this deployment. Note: When working with HCL Digital Experience 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com or oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Follow these steps to update a deployment. Update the properties file with the new image values and run the update command: For Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties For Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties For Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties With the updated deployment, if you were switching to a next release, you can use the properties file to replace the repository, image, and tag as required and perform the update command.","title":"Updating a deployment"},{"location":"containerization/dxtools_dxctl/#deleting-a-deployment","text":"There are two ways to delete a deployment. Method 1: Remove the deployment but allow for redeployment with the same volumes. ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Method 2: Remove the entire namespace/project . ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If some resources, like services, are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE","title":"Deleting a deployment"},{"location":"containerization/dxtools_dxctl/#main-usage","text":"Usage information for dxctl , for additional information, use --help with an action. Deploy Run to deploy a DX deployment. dxctl --deploy --help Update Run to update a DX deployment. dxctl --update --help Collect Run to collect support data for a given deployment. dxctl --collect --help Destroy Run to destroy a DX deployment. dxctl --destroy --help","title":"Main usage"},{"location":"containerization/dxtools_dxctl/#dxctl-help","text":"Sub-commands, required: deploy , update , collect , or destroy . --deploy or --update action string Update an existing DX deployment. Default: update dx.database string The database type Oracle, DB2, etc. Default: derby dx.image string Required, the DX core image. dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . dx.operator.image string Required, the HCL cloud operator image. dx.operator.tag string Required, the HCL cloud operator tag. dx.repository string Required, the image HCL cloud operator repository. dx.tag string Required, the DX core tag. filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt ingress.image string Required, the ambassador image. Not used in OpenShift deployments. ingress.tag string Required, the ambassador tag. Not used in OpenShift deployments. p string dxctl can be run from a properties file, -p namespace.properties, no default. verbose Display messages on the command line. Default: false --collect action string Collecting deployment information about an existing deployment. Default: collect dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false --destroy action string Destroy a DX deployment. Default: destroy all Delete the project/namespace and all artifacts. Default: false dx.name string Deployment name. Default: dx-deployment dx.namespace string Required, the target namespace/project . filename string File name to write into dx-tests.dx-deployment.txt. This contains the test and deployment logs. By default, the namespace is used as the filename. Example: NAMESPACE.txt verbose Display messages on the command line. Default: false Parent topic: Operator-based deployment","title":"dxctl help"},{"location":"containerization/dynacache_replication/","text":"Replicating the DynaCache service This section describes how to replicate Dynacache service in HCL Digital Experience by customizing timeout properties in the WAS Resource Environment Provider (REP). Previous versions up to HCL Digital Experience 8.5 and 9.0 uses Dynamic Cache (DynaCache), a cluster-aware cache service that allows all HCL Portal cluster members to know any name/value pair changes made by any member. Having a cluster-aware cache does not make sense for HCL Digital Experience 9.5 onwards, as the versions are on a Portal farming or containerization setup. To work around this issue, an administrator can customize the timeout properties in the WAS Resource Environment to replicate the DynaCache service. *Tuning the servers to replicate DynaCache service in your container environment * Note: You need to be on HCL Digital Experience 9.5 CF172 container release to perform this task. Configure the following timeout parameters in the WAS Resource Provider (REP): db.cache.invalidation.read.freq = (timeout in milliseconds) Time between \u201creads\u201d of the database table containing invalidation messages. By default, timeout is 1 minute (60000 milliseconds) . Practically, this means this is the longest delay before any dynacache invalidation is \u201cknown\u201d on each member (e.g. Kubernetes, POD, farm worker). db.cache.invalidation.cleanup.freq = (timeout in milliseconds) Age before an entry in the database table containing invalidation messages is deleted. By default, timeout is 10 minutes (600000 milliseconds) . Generally, the number should be much larger than the \u201cred\u201d frequency to ensure that invalidation messages are read before they are deleted. To replicate DynaCache service in an HCL Portal farm deployment, see Tuning the servers in your environment in Roadmap: Portal farm . Parent topic: Customizing your container deployment","title":"Replicating the DynaCache service"},{"location":"containerization/dynacache_replication/#replicating-the-dynacache-service","text":"This section describes how to replicate Dynacache service in HCL Digital Experience by customizing timeout properties in the WAS Resource Environment Provider (REP). Previous versions up to HCL Digital Experience 8.5 and 9.0 uses Dynamic Cache (DynaCache), a cluster-aware cache service that allows all HCL Portal cluster members to know any name/value pair changes made by any member. Having a cluster-aware cache does not make sense for HCL Digital Experience 9.5 onwards, as the versions are on a Portal farming or containerization setup. To work around this issue, an administrator can customize the timeout properties in the WAS Resource Environment to replicate the DynaCache service. *Tuning the servers to replicate DynaCache service in your container environment * Note: You need to be on HCL Digital Experience 9.5 CF172 container release to perform this task. Configure the following timeout parameters in the WAS Resource Provider (REP): db.cache.invalidation.read.freq = (timeout in milliseconds) Time between \u201creads\u201d of the database table containing invalidation messages. By default, timeout is 1 minute (60000 milliseconds) . Practically, this means this is the longest delay before any dynacache invalidation is \u201cknown\u201d on each member (e.g. Kubernetes, POD, farm worker). db.cache.invalidation.cleanup.freq = (timeout in milliseconds) Age before an entry in the database table containing invalidation messages is deleted. By default, timeout is 10 minutes (600000 milliseconds) . Generally, the number should be much larger than the \u201cred\u201d frequency to ensure that invalidation messages are read before they are deleted. To replicate DynaCache service in an HCL Portal farm deployment, see Tuning the servers in your environment in Roadmap: Portal farm . Parent topic: Customizing your container deployment","title":"Replicating the DynaCache service"},{"location":"containerization/finding_kubernetes_eks_deployment/","text":"Finding the Kubernetes EKS deployment This section describes how to find your HCL DX deployment. Follow this procedure to find your HCL DX deployment. As an administrator, you can easily display a list of the DxDeployment instances in your environment. Access the Kubernetes console and navigate to Custom Resource Definitions and scroll to DxDeployments . Click on the DxDeployment Custom Resource Definition, and select All namespaces. You will then see a list of the instances of DxDeployment. Parent topic: Deploy DX Container to Amazon EKS","title":"Finding the Kubernetes EKS deployment"},{"location":"containerization/finding_kubernetes_eks_deployment/#finding-the-kubernetes-eks-deployment","text":"This section describes how to find your HCL DX deployment. Follow this procedure to find your HCL DX deployment. As an administrator, you can easily display a list of the DxDeployment instances in your environment. Access the Kubernetes console and navigate to Custom Resource Definitions and scroll to DxDeployments . Click on the DxDeployment Custom Resource Definition, and select All namespaces. You will then see a list of the instances of DxDeployment. Parent topic: Deploy DX Container to Amazon EKS","title":"Finding the Kubernetes EKS deployment"},{"location":"containerization/finding_openshift_deployment/","text":"Finding the OpenShift deployment This section describes how to find your HCL Portal deployment. Follow this procedure to find your HCL Portal deployment. As an administrator, you can easily display a list of the DxDeployment instances in your environment. Access the OpenShift console and navigate to Administrator > Custom Resource Definitions and scroll to DxDeployments . Click on the DxDeployment Custom Resource Definition. In the upper right of the previous screen, in the Actions drop down, select view images . You will then see a list of the instances of DxDeployment. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Finding the OpenShift deployment"},{"location":"containerization/finding_openshift_deployment/#finding-the-openshift-deployment","text":"This section describes how to find your HCL Portal deployment. Follow this procedure to find your HCL Portal deployment. As an administrator, you can easily display a list of the DxDeployment instances in your environment. Access the OpenShift console and navigate to Administrator > Custom Resource Definitions and scroll to DxDeployments . Click on the DxDeployment Custom Resource Definition. In the upper right of the previous screen, in the Actions drop down, select view images . You will then see a list of the instances of DxDeployment. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Finding the OpenShift deployment"},{"location":"containerization/google_gke/","text":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE) Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic. Deploying DX CF192 and later release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF192 and later release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Deploying DX CF19 and CF191 release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and CF191 release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine \\(GKE\\)"},{"location":"containerization/google_gke/#deploying-hcl-digital-experience-containers-to-google-kubernetes-engine-gke","text":"Learn how to deploy different releases of HCL Digital Experience (DX) containers, along with the Ambassador, to Kubernetes as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic. Deploying DX CF192 and later release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF192 and later release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Deploying DX CF19 and CF191 release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and CF191 release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)"},{"location":"containerization/google_gke_CF191andearlier/","text":"Deploying DX CF19 and CF191 release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and CF191 release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Release CF19 and CF191 and Update file listings in the Docker deployment topic. About this task Follow these steps to deploy HCL Digital Experience 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. The following installed on your system: Docker kubectl Google Cloud SDK Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Download containers Download files to your local Download and extract the contents of the HCL DX 9.5 CF19 package to the local file system. Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-ambassador-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-redis-image-xxx.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_gke_loading_containers.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Syntax for tagging: docker tag <local image:tag or image id> <destination image:tag> Syntax for pushing: docker push <image:tag> Deployment Unzip and extract Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CFxxx_xxxxxxxx-xxxx.zip Change directory Change to the extracted files directory, ./hcl-dx-cloud-scripts. [root]$ cd ./ hcl-dx-cloud-scripts Modify deploy/operator.yaml Modify deploy/operator.yaml and update the configuration based on your repository image and tag. Note: For deployment of DX 9.5 CF183 and higher, ensure the operator.yaml is updated with names and tags used in your private repository: cloud operator image name, cloud operator tag, Redis repo, Redis image name, and Redis tag. # Replace this with the built image name image:`REPOSITORY_NAME`/`cloud-operator:95_184` ... `` name: REDIS_DOCKER_REPO value: \"myrepo.dx.com\"` - ``` name: `REDIS_IMAGE` value: `\"ambassador-redis\"` - ``` name: REDIS_TAG value: \"95_184\" 4. Custom resource definition Install the DxDeployment custom resource definition. **Notes:** - Do not modify the git\\_v1\\_dxdeployment\\_crd.yaml file. - Customize the ./deploy/crds/git\\_v1\\_dxdeployment\\_cr.yaml file, if required. Use either of the following commands: - ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh ``` Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . Deploy Execute the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment, run the deployDx.sh script. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s) For example: $ ./scripts/customer/deployDx.sh dx11 1 us.gcr.io/dx-gcp-l2com/dxcontainer dxen v95_CF183_20200819 dx-pv11 standard derby ambassador 154 Generate a TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, you can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' Create a TLS certification: $ kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n dx11 Note: The default name is the dx-tls-cert this can be changed in the configuration. In the example, dx11 is the Kubernetes namespace. You can set your preferred namespace, but you must consistently use this namespace in subsequent commands. See the Customizing your Container deployment topic for more information on customizing your deployment. Check the deployment status Check the status of the deployment. The following command can be used from the external IP from the Google GKE load balancer to access the deployment. $ kubectl get all -n <your name space> The deployed system will be available at https://external-ip/wps/portal . For example, https://40.76.169.27/wps/portal/ Install the Experience API, Content Composer, and Digital Asset Management components to GKE Create a config map with the same name as the dx statefulset . By default, the dx statefulset is dx-deployment , as shown in this example: kubectl create configmap dx-deployment -n dx11 Edit the configmap, add the following data, and update the values based on your image tags: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Note: The Digital Asset Manager container requires one volume (specified in the configmap under dx.deploy.dam.volume ). The Digital Asset Manager persistence containers require one self-provisioned volume per pod which by default are set to minimum of 2 minimum and a maximum of 5 pods. You can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n dx11 . After creating the config map, deployment will go into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. You can check the status via the command line using the command (example): kubectl get pods -n dx11 See the following section for additional information: Install Experience API, Content Composer, and Digital Asset Management Update the HCL Digital Experience 9.5 GKE deployment To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: $ ./scripts/updateDx.sh test-mynamespace 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE will need to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment. Delete the HCL Digital Experience 9.5 GKE deployment Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv your_volume Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_volume edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)","title":"Deploying DX CF19 and CF191 release Containers to Google Kubernetes Engine \\(GKE\\)"},{"location":"containerization/google_gke_CF191andearlier/#deploying-dx-cf19-and-cf191-release-containers-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and CF191 release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . Note: Refer to the latest HCL DX 9.5 Container Release CF19 and CF191 and Update file listings in the Docker deployment topic.","title":"Deploying DX CF19 and CF191 release Containers to Google Kubernetes Engine (GKE)"},{"location":"containerization/google_gke_CF191andearlier/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic.","title":"About this task"},{"location":"containerization/google_gke_CF191andearlier/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. The following installed on your system: Docker kubectl Google Cloud SDK Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization.","title":"Prerequisites"},{"location":"containerization/google_gke_CF191andearlier/#download-containers","text":"Download files to your local Download and extract the contents of the HCL DX 9.5 CF19 package to the local file system. Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-ambassador-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-redis-image-xxx.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_gke_loading_containers.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Syntax for tagging: docker tag <local image:tag or image id> <destination image:tag> Syntax for pushing: docker push <image:tag>","title":"Download containers"},{"location":"containerization/google_gke_CF191andearlier/#deployment","text":"Unzip and extract Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CFxxx_xxxxxxxx-xxxx.zip Change directory Change to the extracted files directory, ./hcl-dx-cloud-scripts. [root]$ cd ./ hcl-dx-cloud-scripts Modify deploy/operator.yaml Modify deploy/operator.yaml and update the configuration based on your repository image and tag. Note: For deployment of DX 9.5 CF183 and higher, ensure the operator.yaml is updated with names and tags used in your private repository: cloud operator image name, cloud operator tag, Redis repo, Redis image name, and Redis tag. # Replace this with the built image name image:`REPOSITORY_NAME`/`cloud-operator:95_184` ... `` name: REDIS_DOCKER_REPO value: \"myrepo.dx.com\"` - ``` name: `REDIS_IMAGE` value: `\"ambassador-redis\"` - ``` name: REDIS_TAG value: \"95_184\" 4. Custom resource definition Install the DxDeployment custom resource definition. **Notes:** - Do not modify the git\\_v1\\_dxdeployment\\_crd.yaml file. - Customize the ./deploy/crds/git\\_v1\\_dxdeployment\\_cr.yaml file, if required. Use either of the following commands: - ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh ``` Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . Deploy Execute the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment, run the deployDx.sh script. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s) For example: $ ./scripts/customer/deployDx.sh dx11 1 us.gcr.io/dx-gcp-l2com/dxcontainer dxen v95_CF183_20200819 dx-pv11 standard derby ambassador 154","title":"Deployment"},{"location":"containerization/google_gke_CF191andearlier/#generate-a-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: 'openssl genrsa -out my-key.pem 2048' Using OpenSSL, you can create a certificate signed by the private key: 'openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' Create a TLS certification: $ kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n dx11 Note: The default name is the dx-tls-cert this can be changed in the configuration. In the example, dx11 is the Kubernetes namespace. You can set your preferred namespace, but you must consistently use this namespace in subsequent commands. See the Customizing your Container deployment topic for more information on customizing your deployment.","title":"Generate a TLS Certificate"},{"location":"containerization/google_gke_CF191andearlier/#check-the-deployment-status","text":"Check the status of the deployment. The following command can be used from the external IP from the Google GKE load balancer to access the deployment. $ kubectl get all -n <your name space> The deployed system will be available at https://external-ip/wps/portal . For example, https://40.76.169.27/wps/portal/","title":"Check the deployment status"},{"location":"containerization/google_gke_CF191andearlier/#install-the-experience-api-content-composer-and-digital-asset-management-components-to-gke","text":"Create a config map with the same name as the dx statefulset . By default, the dx statefulset is dx-deployment , as shown in this example: kubectl create configmap dx-deployment -n dx11 Edit the configmap, add the following data, and update the values based on your image tags: ``` data: dx.deploy.openldap.enabled: 'true' dx.deploy.openldap.tag: dx-183 dx.deploy.openldap.image: dx-openldap dx.deploy.experienceapi.enabled: 'true' dx.deploy.experienceapi.tag: dx-183 dx.deploy.experienceapi.image: ring-api dx.deploy.contentui.enabled: 'true' dx.deploy.contentui.tag: dx-183 dx.deploy.contentui.image: content-ui dx.deploy.dam.enabled: 'true' dx.deploy.dam.volume: releaseml dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.dam.persistence.tag: dx-183 dx.deploy.dam.persistence.image: persist dx.deploy.dam.imgprocessor.tag: dx-183 dx.deploy.dam.imgprocessor.image: image-processor dx.deploy.dam.tag: dx-183 dx.deploy.dam.image: dam dx.deploy.dam.operator.tag: dx-183 dx.deploy.dam.operator.image: hcl-dam-operator dx.deploy.host.override: \u201cfalse\u201d ``` Note: The Digital Asset Manager container requires one volume (specified in the configmap under dx.deploy.dam.volume ). The Digital Asset Manager persistence containers require one self-provisioned volume per pod which by default are set to minimum of 2 minimum and a maximum of 5 pods. You can also create the config map in a YAML file and deploy it with the following instructions (example): kubectl create -f my_config_map.yaml -n dx11 . After creating the config map, deployment will go into \u2018 init\u2019 mode, and restart a couple of times after the new options are configured. You can check the status via the command line using the command (example): kubectl get pods -n dx11 See the following section for additional information: Install Experience API, Content Composer, and Digital Asset Management","title":"Install the Experience API, Content Composer, and Digital Asset Management components to GKE"},{"location":"containerization/google_gke_CF191andearlier/#update-the-hcl-digital-experience-95-gke-deployment","text":"To update the deployment, run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for Ambassador (Native K8s). INGRESSTAG - The image tag to use for Ambassador (Native K8s). For example: $ ./scripts/updateDx.sh test-mynamespace 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 Once the database is transferred, the DBTYPE will need to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. See Customizing your Container deployment for more information on customizing your deployment.","title":"Update the HCL Digital Experience 9.5 GKE deployment"},{"location":"containerization/google_gke_CF191andearlier/#delete-the-hcl-digital-experience-95-gke-deployment","text":"Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the removeDx.sh script: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where NAMESPACE is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the Kubernetes command: kubectl edit pv your_volume Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_volume edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)","title":"Delete the HCL Digital Experience 9.5 GKE deployment"},{"location":"containerization/google_gke_cf192andlater/","text":"Deploying DX CF192 and later release Containers to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF192 and later release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . About this task Follow these steps to deploy HCL Digital Experience 9.5 Container Update CF192 and later release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. Google Kubernetes Engine (GKE) Cluster Google Container Registry (Or any other registry configured to use with GKE) The following tools must be installed on your system: Docker kubectl Google Cloud SDK Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For Digital Asset Management, additional volume is required. Notes: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. There are various ways to do this and NFS is one option. For more details, see NFS Server . For more information, see the Detailed System Requirements page. Download HCL Digital Experience 9.5 CF192 or later containers Download container Download HCL DX 9.5 CF192 or later package and extract it to the Local file system. Note: Here local can be a local system, or any other system that the administrator uses to connect to the Kubernetes cluster. Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-ambassador-image-xxx.tar.gz - ``` docker load -i hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-image-processor-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-digital-asset-management-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-postgres-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-ringapi-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-redis-image-xxx.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one of the options, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Syntax for tagging: docker tag <local image:tag or image id> <destination image:tag> Syntax for pushing: docker push <image:tag> Deployment Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . NFS Server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw (Default) sync (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ** (Requires requests originate on ports less than 1024) root_squash ** (Map requests to the nobody user). hard ** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 rsize=8388608 (Avoids dropped packages, default 8192) wsize=8388608 (Avoids dropped packages, default 8192) timeo=600 (60 seconds) retrans=2 (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. This helps to ensure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. Log in to cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (like Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Command-line syntax: gcloud container clusters get-credentials cluster_name --region region_name --project project_name Change directory Change to the extracted files directory, ./hcl-dx-cloud-scripts. [root]$ cd ./ hcl-dx-cloud-scripts Download HCL DX dxctl tool Instructions for downloading the latest packages, see A Step-By-Step Guide to Downloading DX Products and Accessing Customer Support . Using DX Container Update CF192 and later, the directory structure might look as follows: For more information on dxctl tool, see dxctl Configuring the dxctl properties for the DX 95 CF192 or later deployment Copy one of the provided properties files to further modify your deployment. Syntax for copying the properties file: mkdir -p /home/$USER/deployments/ The modified properties file can be used for the deployment, and the same must be used for any further updates. Syntax for deployment using the properties file: cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. Syntax for updating the properties file: dx.namespace: caps-dx-gke dx.image: dxen dx.tag: v95_CF192_20210224-004909_xxxxxxxxx_95_CF192_6035c973 dx.storageclass:dx-deploy-stg dx.volume: jen-core-pv dx.volume.size:60 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploying using HCL DX dxctl tool Run the following command to deploy a HCL DX 9.5 CF192 or later release container on Google Kubernetes Engine using dxctl. Syntax to deploy DX container: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps result in a DX 95 CF192 or later deployment being created. Generate a TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' -new Create a TLS certification: $ kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n caps-dx-gke Note: The default name is the dx-tls-cert , and this can be changed in the configuration. In the example, caps-dx-gke is the Kubernetes namespace. You can set your preferred namespace, but you must consistently use this namespace in subsequent commands. Final Output The external IP from Load balancer in the below example can be used to access PORTAL. Syntax for the output command: $ kubectl get all -n NAMESPACE The deployed system will be available at: https://external-ip/wps/portal, where external-ip is the IP address of your system. Update the HCL Digital Experience 9.5 GKE deployment to a later release To update the deployment to a later HCL DX 9.5 Container Update release, perform the following steps: Update the deployment properties file with the new image values and run the Update command. On Mac ./mac/dxctl --update -p properties/myfirst_deployment.properties On Windows .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: Once the database is transferred, the DBTYPE might need to be updated, so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. Delete the HCL Digital Experience 9.5 GKE deployment Removing the entire deployment requires several steps, this is by design. Remove the deployment but allow for redeployment with the same volumes: ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Remove the entire namespace/project: ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If you still find some resources like services that are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)","title":"Deploying DX CF192 and later release Containers to Google Kubernetes Engine \\(GKE\\)"},{"location":"containerization/google_gke_cf192andlater/#deploying-dx-cf192-and-later-release-containers-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF192 and later release containers along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) .","title":"Deploying DX CF192 and later release Containers to Google Kubernetes Engine (GKE)"},{"location":"containerization/google_gke_cf192andlater/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 Container Update CF192 and later release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Refer to the latest HCL DX 9.5 Container Update Release CF192 and later file listings in the Docker deployment topic.","title":"About this task"},{"location":"containerization/google_gke_cf192andlater/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Use kubectl get {pods, pv, storageclass} to get appropriate information from the artifacts running in the target Kubernetes environment. Google Kubernetes Engine (GKE) Cluster Google Container Registry (Or any other registry configured to use with GKE) The following tools must be installed on your system: Docker kubectl Google Cloud SDK Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For Digital Asset Management, additional volume is required. Notes: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. There are various ways to do this and NFS is one option. For more details, see NFS Server . For more information, see the Detailed System Requirements page.","title":"Prerequisites"},{"location":"containerization/google_gke_cf192andlater/#download-hcl-digital-experience-95-cf192-or-later-containers","text":"Download container Download HCL DX 9.5 CF192 or later package and extract it to the Local file system. Note: Here local can be a local system, or any other system that the administrator uses to connect to the Kubernetes cluster. Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-ambassador-image-xxx.tar.gz - ``` docker load -i hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-image-processor-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-digital-asset-management-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-postgres-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-ringapi-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load -i hcl-dx-redis-image-xxx.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one of the options, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Syntax for tagging: docker tag <local image:tag or image id> <destination image:tag> Syntax for pushing: docker push <image:tag>","title":"Download HCL Digital Experience 9.5 CF192 or later containers"},{"location":"containerization/google_gke_cf192andlater/#deployment","text":"Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . NFS Server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw (Default) sync (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ** (Requires requests originate on ports less than 1024) root_squash ** (Map requests to the nobody user). hard ** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 rsize=8388608 (Avoids dropped packages, default 8192) wsize=8388608 (Avoids dropped packages, default 8192) timeo=600 (60 seconds) retrans=2 (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. This helps to ensure that the EFS file system has uninterrupted availability after a network recovery event. Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608. Log in to cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI (like Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Command-line syntax: gcloud container clusters get-credentials cluster_name --region region_name --project project_name Change directory Change to the extracted files directory, ./hcl-dx-cloud-scripts. [root]$ cd ./ hcl-dx-cloud-scripts Download HCL DX dxctl tool Instructions for downloading the latest packages, see A Step-By-Step Guide to Downloading DX Products and Accessing Customer Support . Using DX Container Update CF192 and later, the directory structure might look as follows: For more information on dxctl tool, see dxctl Configuring the dxctl properties for the DX 95 CF192 or later deployment Copy one of the provided properties files to further modify your deployment. Syntax for copying the properties file: mkdir -p /home/$USER/deployments/ The modified properties file can be used for the deployment, and the same must be used for any further updates. Syntax for deployment using the properties file: cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. Syntax for updating the properties file: dx.namespace: caps-dx-gke dx.image: dxen dx.tag: v95_CF192_20210224-004909_xxxxxxxxx_95_CF192_6035c973 dx.storageclass:dx-deploy-stg dx.volume: jen-core-pv dx.volume.size:60 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false ingress.image:dx-build-output/common/ambassador ingress.tag:1.5.4 ingress.redis.image:redis ingress.redis.tag:5.0.1 dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploying using HCL DX dxctl tool Run the following command to deploy a HCL DX 9.5 CF192 or later release container on Google Kubernetes Engine using dxctl. Syntax to deploy DX container: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps result in a DX 95 CF192 or later deployment being created.","title":"Deployment"},{"location":"containerization/google_gke_cf192andlater/#generate-a-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert' -new Create a TLS certification: $ kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n caps-dx-gke Note: The default name is the dx-tls-cert , and this can be changed in the configuration. In the example, caps-dx-gke is the Kubernetes namespace. You can set your preferred namespace, but you must consistently use this namespace in subsequent commands.","title":"Generate a TLS Certificate"},{"location":"containerization/google_gke_cf192andlater/#final-output","text":"The external IP from Load balancer in the below example can be used to access PORTAL. Syntax for the output command: $ kubectl get all -n NAMESPACE The deployed system will be available at: https://external-ip/wps/portal, where external-ip is the IP address of your system.","title":"Final Output"},{"location":"containerization/google_gke_cf192andlater/#update-the-hcl-digital-experience-95-gke-deployment-to-a-later-release","text":"To update the deployment to a later HCL DX 9.5 Container Update release, perform the following steps: Update the deployment properties file with the new image values and run the Update command. On Mac ./mac/dxctl --update -p properties/myfirst_deployment.properties On Windows .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: Once the database is transferred, the DBTYPE might need to be updated, so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased.","title":"Update the HCL Digital Experience 9.5 GKE deployment to a later release"},{"location":"containerization/google_gke_cf192andlater/#delete-the-hcl-digital-experience-95-gke-deployment","text":"Removing the entire deployment requires several steps, this is by design. Remove the deployment but allow for redeployment with the same volumes: ./linux/dxctl --destroy -p properties/hybrid-deployment.properties Remove the entire namespace/project: ./linux/dxctl --destroy -p properties/hybrid-deployment.properties -all true If you still find some resources like services that are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploying HCL Digital Experience Containers to Google Kubernetes Engine (GKE)","title":"Delete the HCL Digital Experience 9.5 GKE deployment"},{"location":"containerization/hybrid_deployment_HCL_SoFy/","text":"Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption. HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. These include the following: running in docker containers, creation of stand-alone and individually scalable services, enablement for cloud scaling and orchestration, continuous delivery of updates, and opening up capabilities with new REST APIs. Using HCL SoFy to access and deploy HCL Digital Experience 9.5 and other HCL software offerings you can quickly gain hands-on experience working with these cloud-native solutions. Before you begin Before you explore HCL Digital Experience and form a solution on HCL SoFy , you must obtain access to the platform. You can request access through the contact form located at: https://hcltechsw.com/products/sofy/request-access . Once you gain access, it is recommended to read through the provided user guides to get familiar with the SoFy solution builder. Video: HCL Sofy \u2013 A Quick Tour You can also refer to the HCL Software Academy on how to Setup DX Demo in SoFy lab . Parent topic: Digital Experience on containerized platforms","title":"Deploying HCL Digital Experience 9.5 with HCL Solution Factory \\(SoFy\\)"},{"location":"containerization/hybrid_deployment_HCL_SoFy/#deploying-hcl-digital-experience-95-with-hcl-solution-factory-sofy","text":"The HCL Solution Factory (SoFy) platform offers the ability for organizations to quickly prototype, test and deploy HCL Digital Experience and other solutions, and can enable organizations to speed cloud-native adoption. HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. These include the following: running in docker containers, creation of stand-alone and individually scalable services, enablement for cloud scaling and orchestration, continuous delivery of updates, and opening up capabilities with new REST APIs. Using HCL SoFy to access and deploy HCL Digital Experience 9.5 and other HCL software offerings you can quickly gain hands-on experience working with these cloud-native solutions.","title":"Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy)"},{"location":"containerization/hybrid_deployment_HCL_SoFy/#before-you-begin","text":"Before you explore HCL Digital Experience and form a solution on HCL SoFy , you must obtain access to the platform. You can request access through the contact form located at: https://hcltechsw.com/products/sofy/request-access . Once you gain access, it is recommended to read through the provided user guides to get familiar with the SoFy solution builder. Video: HCL Sofy \u2013 A Quick Tour You can also refer to the HCL Software Academy on how to Setup DX Demo in SoFy lab . Parent topic: Digital Experience on containerized platforms","title":"Before you begin"},{"location":"containerization/hybrid_deployment_operator/","text":"Hybrid deployment - Operator This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method. Overview Many organizations are using cloud and containerized deployments as part of their overall systems environments. In parallel, organizations continue to operate software applications and processes on-premises. The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. Notes: The hybrid deployment of cloud-based components HCL Digital Asset Management, Content Composer, and Experience API is supported on Red Hat OpenShift with the HCL DX 9.5 CF19 release. It is not yet supported with an on-premises clustered environment. The HCL DX hybrid deployment model will be supported on additional Kubernetes platforms in later release updates. Virtual Portal support with Content Composer is not yet supported. Support for this component in a virtual portal deployment model will be available in later releases. Prerequisites HCL Digital Experience V9.5 CF19 or higher is deployed to supported on-premises platforms, in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio has been enabled in the Digital Experience 9.5 CF19 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain, using an SSL connection, is established for both the on premise HCL DX 9.5 CF19 and higher on-premises environment, and the target Red Hat Open Shift platform deployment to contain cloud native components HCL DX Experience API, Digital Asset Management and Content Composer. For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF19 or higher on-premises environment. On DMGR or WAS Admin console under Security > Global Security > Web & SIP Security > Single Sign-On , Enabled is checked and Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment, and the target DX Red Hat Open Shift platform deployment. dxctl tool. Volume Requirement : Requires an AccessMode of ReadWriteMany. Reference the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information. Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment Follow the steps below to enable Hybrid deployment support in the HCL Digital Experience 9.5 on-premises environment. Access the latest HCL DX 9.5 CF19 or higher release software from the HCL Software License Portal . The package will include the file hcl-dx-cloud-scripts, which should be downloaded and extracted to your local system. This will present the following deploy, dxctl and scripts directories as shown below: Configure Properties of the dxctl tool. Navigate to the dxctl/properties directory and open the hybrid-deployment.properties file in your favorite editor. Sample properties for deploying Digital Asset Management, Content Composer, and the Experience API for use with an on-premise DX environment are shown below. Use following example to modify the values in the dxctl/properties/hybrid-deployment.properties file to reflect your target Red Hat Open Shift environment to deploy the Content Composer, Digital Asset Management, and the Experience API components. Note: For a Hybrid deployment, the following items should be enabled and/or not included in the deployment: Enable api.enabled(eAPI), composer.enabled(Content Composer) and/or dam.enabled(DAM) to true. Not included **dx.tag** This is not required for a hybrid deployment as DX Portal and WCM is installed on premises. **dx.database** This is not required for a hybrid deployment as it is installed with DX Portal and WCM on premises. See the Reference list below for explanations of the file items. dx.namespace: dxhybwin dx.config.cors: https://portal-dxhybwin.apps.dxdev.dx-dev.net:10042 hybrid.enabled: true hybrid.host: portal-dxhybwin.apps.dxdev.dx-dev.net hybrd.port: 10042 api.enabled: true api.tag: v1.3.0_20201019-1240_develop composer.enabled: true composer.tag: v1.3.0_20201019-1239_develop dam.enabled: true dam.volume: dxhybwin-dam dam.stgclass: dx-deploy-stg dam.tag: v1.3.0_20201019-1259_develop imgproc.tag: v1.3.0_20201015-1133_develop override.ingress.host: dx-hybrid-service-dxhybwin-dxhybwin.apps.hcl-dxdev.dx-dev.net dam.operator.tag: v95_CF19_20201020_dev_build Reference list : dx.namespace - The project or the namespace to create or use for deployment. dx.tag - Tag of the latest DX HCL DX 9.5 Portal and Web Content Manager image. dx.database - By default, and initially, this is Derby. HCL DX 9.5 supports Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . dx.config.cors - URL of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. hybrid.enabled - Boolean value to enable/disable. hybrid.host - URL of On-premises DX 9.5 Portal and Web Content Manager server without port. hybrd.port - Port of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. api.enabled - Boolean value to enable/disable the Experience API. api.tag - Tag of the your Experience API image. composer.enabled - Boolean value to enable/disable Content Composer. composer.tag - Tag of the latest Content Composer image. dam.enabled - Boolean value to enable/disable Digital Asset Management. dam.volume - Volume for Digital Asset Management. dam.stgclass - Storage class for Digital Asset Management. dam.tag - Tag of your Digital Asset Management image. imgproc.tag - Tag of your Image-Processor image. override.ingress.host - Generated base URL of the route. dx.operator.tag - Tag of your hcldx-cloud-operator image. dam.operator.tag - Tag of your Docker hcl-dam-operator tag. Deploy using the dxctl tool Login to the Red Hat OpenShift platform using Kubectl. Log in to the target Red Hat OpenShift platform using the following OpenShift command: $ oc login Enter your username and password in response to system prompts. See the example below: Deploy using dxctl. On your local workstation, navigate to the dxctl folder: $ cd ./hcl-dx-cloud-scripts/dxctl Execute the below command to deploy from the local workstation to the target Red Hat OpenShift platform to contain the HCL DX 9.5 CF19 and higher cloud native components using the dxctl tool based on configured properties: From a MacOS workstation: MacBook-Pro:macuser$ ./mac/dxctl --deploy -p properties/hybrid-deployment.properties From a Windows workstation: Cmd> ./win/dxctl -\u2013deploy -p properties/hybrid-deployment.properties Note: On Windows 10 MD and PowerShell, rename the dxctl file to dxctl.exe and run the deploy command. From a Linux workstation: $ ./linux/dxctl \u2013deploy -p properties/hybrid-deployment.properties Validate the Deployment Login to the Red Hat OpenShift Console. Login to OpenShift Dashboard Console with your credentials: Once logged in, search for the HCL DX 9.5 project: Ensure the deployment and pods are running and in a ready state. Access the HCL Digital Experience 9.5 on-premises server. Access the host URL of the HCL Digital Experience 9.5 on-premises deployment, which is defined in the hybrid-deployment.properties file and add the path below to the URL: /wps/myportal. Login to the HCL DX 9.5 on-premises deployment using your appropriate credentials. Once logged in, click the Open Applications Menu in the right top corner. If the Web Content and Digital Assets menu items do not appear in the menu selections, follow the steps below to enable Content Composer and Digital Asset Management using the Configuration Engine. Enable Content Composer and Digital Asset Management Locate the Content Composer and Digital Asset Management URLs. The URLs will match the location values of the Red Hat OpenShift route of the component. Copy the Content Composer and Digital Asset Management URL from the Red Hat OpenShift dashboard console and add the suffix/static as in the example below: Content Composer \u2013 Content UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/content/static Digital Asset Management UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/dam/static Navigate to the ConfigEngine service. Connect/login to the HCL DX 9.5 on-premises system and open a command line prompt. Navigate to the following path on your deployment: C:\\> cd <path to wp_profile>\\ConfigEngine Enable Content Composer and Digital Asset Management to the Hybrid deployment. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Windows platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Linux platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Once the above steps are completed, log in to the Digital Experience 9.5 deployment and validate the Web Content and Digital Assets menu items appear. Example : Parent topic: Digital Experience on containerized platforms","title":"Hybrid deployment - Operator"},{"location":"containerization/hybrid_deployment_operator/#hybrid-deployment-operator","text":"This section describes how to install HCL Digital Experience 9.5 Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms deployed using the Operator (dxctl) method.","title":"Hybrid deployment - Operator"},{"location":"containerization/hybrid_deployment_operator/#overview","text":"Many organizations are using cloud and containerized deployments as part of their overall systems environments. In parallel, organizations continue to operate software applications and processes on-premises. The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. Notes: The hybrid deployment of cloud-based components HCL Digital Asset Management, Content Composer, and Experience API is supported on Red Hat OpenShift with the HCL DX 9.5 CF19 release. It is not yet supported with an on-premises clustered environment. The HCL DX hybrid deployment model will be supported on additional Kubernetes platforms in later release updates. Virtual Portal support with Content Composer is not yet supported. Support for this component in a virtual portal deployment model will be available in later releases.","title":"Overview"},{"location":"containerization/hybrid_deployment_operator/#prerequisites","text":"HCL Digital Experience V9.5 CF19 or higher is deployed to supported on-premises platforms, in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio has been enabled in the Digital Experience 9.5 CF19 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain, using an SSL connection, is established for both the on premise HCL DX 9.5 CF19 and higher on-premises environment, and the target Red Hat Open Shift platform deployment to contain cloud native components HCL DX Experience API, Digital Asset Management and Content Composer. For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF19 or higher on-premises environment. On DMGR or WAS Admin console under Security > Global Security > Web & SIP Security > Single Sign-On , Enabled is checked and Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment, and the target DX Red Hat Open Shift platform deployment. dxctl tool. Volume Requirement : Requires an AccessMode of ReadWriteMany. Reference the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information.","title":"Prerequisites"},{"location":"containerization/hybrid_deployment_operator/#enabling-hybrid-deployment-support-in-the-hcl-digital-experience-95-on-premises-environment","text":"Follow the steps below to enable Hybrid deployment support in the HCL Digital Experience 9.5 on-premises environment. Access the latest HCL DX 9.5 CF19 or higher release software from the HCL Software License Portal . The package will include the file hcl-dx-cloud-scripts, which should be downloaded and extracted to your local system. This will present the following deploy, dxctl and scripts directories as shown below: Configure Properties of the dxctl tool. Navigate to the dxctl/properties directory and open the hybrid-deployment.properties file in your favorite editor. Sample properties for deploying Digital Asset Management, Content Composer, and the Experience API for use with an on-premise DX environment are shown below. Use following example to modify the values in the dxctl/properties/hybrid-deployment.properties file to reflect your target Red Hat Open Shift environment to deploy the Content Composer, Digital Asset Management, and the Experience API components. Note: For a Hybrid deployment, the following items should be enabled and/or not included in the deployment: Enable api.enabled(eAPI), composer.enabled(Content Composer) and/or dam.enabled(DAM) to true. Not included **dx.tag** This is not required for a hybrid deployment as DX Portal and WCM is installed on premises. **dx.database** This is not required for a hybrid deployment as it is installed with DX Portal and WCM on premises. See the Reference list below for explanations of the file items. dx.namespace: dxhybwin dx.config.cors: https://portal-dxhybwin.apps.dxdev.dx-dev.net:10042 hybrid.enabled: true hybrid.host: portal-dxhybwin.apps.dxdev.dx-dev.net hybrd.port: 10042 api.enabled: true api.tag: v1.3.0_20201019-1240_develop composer.enabled: true composer.tag: v1.3.0_20201019-1239_develop dam.enabled: true dam.volume: dxhybwin-dam dam.stgclass: dx-deploy-stg dam.tag: v1.3.0_20201019-1259_develop imgproc.tag: v1.3.0_20201015-1133_develop override.ingress.host: dx-hybrid-service-dxhybwin-dxhybwin.apps.hcl-dxdev.dx-dev.net dam.operator.tag: v95_CF19_20201020_dev_build Reference list : dx.namespace - The project or the namespace to create or use for deployment. dx.tag - Tag of the latest DX HCL DX 9.5 Portal and Web Content Manager image. dx.database - By default, and initially, this is Derby. HCL DX 9.5 supports Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . dx.config.cors - URL of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. hybrid.enabled - Boolean value to enable/disable. hybrid.host - URL of On-premises DX 9.5 Portal and Web Content Manager server without port. hybrd.port - Port of the on-premises HCL DX 9.5 CF19 or higher Portal and Web Content Manager server. api.enabled - Boolean value to enable/disable the Experience API. api.tag - Tag of the your Experience API image. composer.enabled - Boolean value to enable/disable Content Composer. composer.tag - Tag of the latest Content Composer image. dam.enabled - Boolean value to enable/disable Digital Asset Management. dam.volume - Volume for Digital Asset Management. dam.stgclass - Storage class for Digital Asset Management. dam.tag - Tag of your Digital Asset Management image. imgproc.tag - Tag of your Image-Processor image. override.ingress.host - Generated base URL of the route. dx.operator.tag - Tag of your hcldx-cloud-operator image. dam.operator.tag - Tag of your Docker hcl-dam-operator tag.","title":"Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment"},{"location":"containerization/hybrid_deployment_operator/#deploy-using-the-dxctl-tool","text":"Login to the Red Hat OpenShift platform using Kubectl. Log in to the target Red Hat OpenShift platform using the following OpenShift command: $ oc login Enter your username and password in response to system prompts. See the example below: Deploy using dxctl. On your local workstation, navigate to the dxctl folder: $ cd ./hcl-dx-cloud-scripts/dxctl Execute the below command to deploy from the local workstation to the target Red Hat OpenShift platform to contain the HCL DX 9.5 CF19 and higher cloud native components using the dxctl tool based on configured properties: From a MacOS workstation: MacBook-Pro:macuser$ ./mac/dxctl --deploy -p properties/hybrid-deployment.properties From a Windows workstation: Cmd> ./win/dxctl -\u2013deploy -p properties/hybrid-deployment.properties Note: On Windows 10 MD and PowerShell, rename the dxctl file to dxctl.exe and run the deploy command. From a Linux workstation: $ ./linux/dxctl \u2013deploy -p properties/hybrid-deployment.properties","title":"Deploy using the dxctl tool"},{"location":"containerization/hybrid_deployment_operator/#validate-the-deployment","text":"Login to the Red Hat OpenShift Console. Login to OpenShift Dashboard Console with your credentials: Once logged in, search for the HCL DX 9.5 project: Ensure the deployment and pods are running and in a ready state. Access the HCL Digital Experience 9.5 on-premises server. Access the host URL of the HCL Digital Experience 9.5 on-premises deployment, which is defined in the hybrid-deployment.properties file and add the path below to the URL: /wps/myportal. Login to the HCL DX 9.5 on-premises deployment using your appropriate credentials. Once logged in, click the Open Applications Menu in the right top corner. If the Web Content and Digital Assets menu items do not appear in the menu selections, follow the steps below to enable Content Composer and Digital Asset Management using the Configuration Engine.","title":"Validate the Deployment"},{"location":"containerization/hybrid_deployment_operator/#enable-content-composer-and-digital-asset-management","text":"Locate the Content Composer and Digital Asset Management URLs. The URLs will match the location values of the Red Hat OpenShift route of the component. Copy the Content Composer and Digital Asset Management URL from the Red Hat OpenShift dashboard console and add the suffix/static as in the example below: Content Composer \u2013 Content UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/content/static Digital Asset Management UI URL Example : https://dx-hybrid-service-dxhybwin-dxhybwin.apps.comp-dxdev.hcl-dx-dev.net/dx/ui/dam/static Navigate to the ConfigEngine service. Connect/login to the HCL DX 9.5 on-premises system and open a command line prompt. Navigate to the following path on your deployment: C:\\> cd <path to wp_profile>\\ConfigEngine Enable Content Composer and Digital Asset Management to the Hybrid deployment. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Windows platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.bat enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. If the on-premises HCL DX 9.5 Portal and Web Content Manager deployment is installed to a Linux platform, execute: Enable Content Composer : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-headless-content -Dstatic.ui.url=CONTENT_UI_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the CONTENT_UI_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Enable Digital Asset Management : <Path to wp_profile>\\ConfigEngine\\ConfigEngine.sh enable-media-library -Dstatic.ui.url=DAM_URL -DWasPassword=... -DPortalAdminPwd=... Note: In the command above, the DAM_URL=Route URL is copied from the Red Hat OpenShift dashboard with /static . -DWasPassword=type is the IBM WebSphere Application Server password and -DPortalAdminPwd=type is the DX admin password. Once the above steps are completed, log in to the Digital Experience 9.5 deployment and validate the Web Content and Digital Assets menu items appear. Example : Parent topic: Digital Experience on containerized platforms","title":"Enable Content Composer and Digital Asset Management"},{"location":"containerization/install_config_cc_dam/","text":"Install the HCL Digital Experience 9.5 components This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Video : Install HCL Digital Experience components (Experience API, Content Composer, and Digital Asset Management) on Red Hat OpenShift High-level architecture and topology Prerequisite DX Administrators can choose to install the DX Core containers then proceed to install Content Composer and Digital Asset Management containers to the supported Kubernetes container platforms as outlined in the following steps. See the Deployment section for the latest DX 9.5 container file listings. Deploying the HCL Digital Asset Management or Content Composer components is supported on Kubernetes or OpenShift platforms and is not supported for deployment to Docker platforms. See the System requirements section for more information and the latest updates. Note: For initial deployments, it is recommended to install the HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management) to a non-production (test) HCL Digital Experience 9.5 environment. Installing the HCL Digital Experience 9.5 Container components Follow these steps to install your HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management): Asset Management components If installing in conjunction with HCL Digital Experience 9.5 CF181 or higher, follow the instructions in the Container Deployment topic. This page lists the latest HCL Digital Experience 9.5 CF181 or higher product images available and how to obtain and load the images into your Docker repository before continuing with these instructions. If installing to an existing HCL Digital Experience 9.5 CF181 or higher Kubernetes environment: Verify that you can access the HCL Digital Experience 9.5 CF181 or higher Practitioner Studio by logging in to your HCL Digital Experience 9.5 Practitioner Studio interface. See the HCL Digital Experience 9.5 Practitioner Studio topic for information. Download and extract the HCL Digital Experience 9.5 components from your Digital Experience entitlements from the HCL Software License Portal to the local file system. Sample download package name : hcl-dx-kubernetes-v95-CF181-other.zip or higher, depending on the DX 9.5 Container Update version you are installing. Example : **hcl-dx-kubernetes-v95-CF181-other.zip:** HCL Experience API (Docker image) - hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Postgres - hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Operator) - hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Image processor) - hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Docker image) - hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Content Composer (Docker image) - hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Extract the images to the local file system. Open a terminal window and change to the root directory of the extracted package images. Load the images into your Docker environment. Example: Docker load < hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Note: Either -i or < works for the load command. In case you encounter an error when using one, try running the command using the other. Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Install the HCL Digital Experience 9.5 CF181 or higher Experience API, Content Composer, and Digital Asset Management components by using the following steps. Container Update CF182 or higher is required if deploying to Microsoft Azure Kubernetes Service (AKS). Notes: The config map name value used must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment. The HCL Experience API must be installed to access and use the HCL Content Composer and the HCL Digital Asset Management features. Reminder : If you are currently running an HCL Digital Experience 9.5 CF181 or higher Kubernetes deployment in production, adding new components requires an outage and setup time so plan it carefully. If you are creating the dx-deployment config map, you can use the following content (adjusting the image tag values to match your environment) to create a YAML file and use a command line client to create the config map which is used to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and later components. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management, if either of those combinations is preferred, by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. Confirm your HCL Digital Experience 9.5 CF181 and higher container instance is up and running on Amazon EKS, Microsoft Azure (CF182 or higher), or Red Hat OpenShift platform. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Reminder : For an initial deployment, it is not advisable to deploy these components to a production HCL Digital Experience 9.5 deployment. Update the HCL Digital Experience 9.5 CF181 or higher container deployment configuration map to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and higher components. Note: The config map name value used to support the CF181 or higher components must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment . Create a YAML file with the following config map settings: kind: `ConfigMap` metadata: name: dx-deployment Use the following example YAML ( dx-deploy-config-map.yaml ) to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 or higher components. If deploying HCL Content Composer and HCL Digital Asset Management CF181 components, replace their file names in the sample YAML file services lines used. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. ``` kind: ConfigMap metadata: name: dx-deployment data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Notes: The deployment of HCL Content Composer and HCL Experience API components create: The dx.deploy.contentui.enabled and dx.deploy.experienceapi.enabled configurations tell the operator to deploy HCL Content Composer and HCL Experience API components. This defaults to using the same repository as the HCL Digital Experience 9.5 CF181 or higher container core deployment. Services dx-deployment-service-content-ui and dx-deployment-service-ring-api , and a route for each. Administrators can override the repository by adding the following to the config map entries: dx.deploy.contentui.repository The dx.deploy.dam.enabled tells the operator to deploy the HCL Digital Asset Management component. Note that there are 4 required sets of image/tag parameters: The HCL Digital Asset Management operator component uses prefixdx.deploy.dam.operator . The Postgres datastore component uses dx.deploy.dam.persistence . The HCL Digital Asset Management library services use dx.deploy.dam . The Image processor uses dx.deploy.dam.persistence . To override the repository values for the components above, use dx.deploy.COMPONENT.repository The last two parameters in the example YAML file provide the storage class and volume (must be ReadWriteMany ) for the HCL Digital Asset Management component. This is where the persistence layer maintains the datastore layer. The dx.deploy.dam.volume: volume name setting is optional if the storage class used/specified by dx.deploy.dam.storageclass is self-provisioning. A dx.dam.config.cors config map setting is auto-generated and provides the ability for Cross Origin Resource Sharing across Content Composer and Digital Asset Management resources. In the Digital Experience 9.5 core deployment, the dx.config.cors setting is set in the DX configuration map. Reference the Containerization Deployment pages for additional details. An additional self-provisioning volume is created for each of the HCL Digital Asset Management Persistence (Postgres) pods. The access mode of these self-provisioning persistent volumes must include ReadWriteOnce . If this volume is not present the images are lost and shows blank if/when the HCL Digital Asset Management library is restarted. Administrators can override the repository by adding to: dx.deploy.contentui.repository In addition, the following default settings are configurable: dx.deploy.contentui.resources.cpurequest , the default is 1. dx.deploy.contentui.resources.cpulimit , the default is 3. dx.deploy.contentui.resources.memoryrequest , the default is 2G. dx.deploy.contentui.resources.memorylimit , the default is 4G. dx.deploy.experienceapi.resources.cpurequest , the default is 1. dx.deploy.experienceapi.resources.cpulimit , the default is 3. dx.deploy.experienceapi.resources.memoryrequest , the default is 2G. dx.deploy.experienceapi.resources.memorylimit , the default is 4G. Additional configuration options are currently not supported. Deploy the YAML ( dx-deploy-config-map.yaml ) by issuing the following: Kubernetes command: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace - OpenShift command: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml ``` Stop and restart the HCL Digital Experience 9.5 CF181 and higher container deployment. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/removeDx.sh NAMESPACE Note: This script removes resources from the existing deployment (pods, statefulsets, etc) but does not remove persisted data or existing configmaps. Remove the claimRef from the PersistedVolume. Note: Instructions to re-use the Persistent Volume may also be viewed in the Deploy HCL Digital Experience 9.5 Container to Amazon EKS topic. Open the persistent volume in a visual editor (vi) using the Kubernetes or OpenShift command line client command: kubectl edit pv <pv name> or oc edit pv <pv name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: awseks-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the ' persistentvolume/your_namespace edited ' message. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE or ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG Note: You must restart the deployment with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Access the HCL Content Composer and HCL Digital Asset Management components by navigating to Practitioner Studio > Web Content > Content , or Practitioner Studio > Digital Assets . https://your-portal.net/wps/myportal/Practitioner/Web Content/Content Library https://your-portal.net/wps/myportal/Practitioner/Digital Assets Access the HCL Experience API Explorer at the following URL: http://<HOST>:<PORT>dx/api/core/v1/explorer For example, http://127.0.0.1:3000/dx/api/core/v1/explorer (Optional) Configure Digital Asset Management with a CDN If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your CDN configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' . Parent topic: Operator-based deployment","title":"Install the HCL Digital Experience 9.5 components"},{"location":"containerization/install_config_cc_dam/#install-the-hcl-digital-experience-95-components","text":"This section provides a high-level overview of the architecture and the steps to install, configure, and update the HCL Digital Experience 9.5 components: Experience API, Content Composer, and Digital Asset Management. Video : Install HCL Digital Experience components (Experience API, Content Composer, and Digital Asset Management) on Red Hat OpenShift","title":"Install the HCL Digital Experience 9.5 components"},{"location":"containerization/install_config_cc_dam/#high-level-architecture-and-topology","text":"","title":"High-level architecture and topology"},{"location":"containerization/install_config_cc_dam/#prerequisite","text":"DX Administrators can choose to install the DX Core containers then proceed to install Content Composer and Digital Asset Management containers to the supported Kubernetes container platforms as outlined in the following steps. See the Deployment section for the latest DX 9.5 container file listings. Deploying the HCL Digital Asset Management or Content Composer components is supported on Kubernetes or OpenShift platforms and is not supported for deployment to Docker platforms. See the System requirements section for more information and the latest updates. Note: For initial deployments, it is recommended to install the HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management) to a non-production (test) HCL Digital Experience 9.5 environment.","title":"Prerequisite"},{"location":"containerization/install_config_cc_dam/#installing-the-hcl-digital-experience-95-container-components","text":"Follow these steps to install your HCL Digital Experience 9.5 components (Experience API, Content Composer, and Digital Asset Management): Asset Management components If installing in conjunction with HCL Digital Experience 9.5 CF181 or higher, follow the instructions in the Container Deployment topic. This page lists the latest HCL Digital Experience 9.5 CF181 or higher product images available and how to obtain and load the images into your Docker repository before continuing with these instructions. If installing to an existing HCL Digital Experience 9.5 CF181 or higher Kubernetes environment: Verify that you can access the HCL Digital Experience 9.5 CF181 or higher Practitioner Studio by logging in to your HCL Digital Experience 9.5 Practitioner Studio interface. See the HCL Digital Experience 9.5 Practitioner Studio topic for information. Download and extract the HCL Digital Experience 9.5 components from your Digital Experience entitlements from the HCL Software License Portal to the local file system. Sample download package name : hcl-dx-kubernetes-v95-CF181-other.zip or higher, depending on the DX 9.5 Container Update version you are installing. Example : **hcl-dx-kubernetes-v95-CF181-other.zip:** HCL Experience API (Docker image) - hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Postgres - hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Operator) - hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Image processor) - hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Digital Asset Management (Docker image) - hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz HCL Content Composer (Docker image) - hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Extract the images to the local file system. Open a terminal window and change to the root directory of the extracted package images. Load the images into your Docker environment. Example: Docker load < hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-digital-asset-manager-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-content-composer-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Docker load < hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz Note: Either -i or < works for the load command. In case you encounter an error when using one, try running the command using the other. Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. Install the HCL Digital Experience 9.5 CF181 or higher Experience API, Content Composer, and Digital Asset Management components by using the following steps. Container Update CF182 or higher is required if deploying to Microsoft Azure Kubernetes Service (AKS). Notes: The config map name value used must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment. The HCL Experience API must be installed to access and use the HCL Content Composer and the HCL Digital Asset Management features. Reminder : If you are currently running an HCL Digital Experience 9.5 CF181 or higher Kubernetes deployment in production, adding new components requires an outage and setup time so plan it carefully. If you are creating the dx-deployment config map, you can use the following content (adjusting the image tag values to match your environment) to create a YAML file and use a command line client to create the config map which is used to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and later components. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management, if either of those combinations is preferred, by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. Confirm your HCL Digital Experience 9.5 CF181 and higher container instance is up and running on Amazon EKS, Microsoft Azure (CF182 or higher), or Red Hat OpenShift platform. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Reminder : For an initial deployment, it is not advisable to deploy these components to a production HCL Digital Experience 9.5 deployment. Update the HCL Digital Experience 9.5 CF181 or higher container deployment configuration map to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 and higher components. Note: The config map name value used to support the CF181 or higher components must be the same as the HCL Digital Experience 9.5 CF181 and higher deployment. By default, the config map deployment name value is dx-deployment . Create a YAML file with the following config map settings: kind: `ConfigMap` metadata: name: dx-deployment Use the following example YAML ( dx-deploy-config-map.yaml ) to deploy the HCL Experience API, HCL Content Composer, and HCL Digital Asset Management CF181 or higher components. If deploying HCL Content Composer and HCL Digital Asset Management CF181 components, replace their file names in the sample YAML file services lines used. Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. ``` kind: ConfigMap metadata: name: dx-deployment data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Notes: The deployment of HCL Content Composer and HCL Experience API components create: The dx.deploy.contentui.enabled and dx.deploy.experienceapi.enabled configurations tell the operator to deploy HCL Content Composer and HCL Experience API components. This defaults to using the same repository as the HCL Digital Experience 9.5 CF181 or higher container core deployment. Services dx-deployment-service-content-ui and dx-deployment-service-ring-api , and a route for each. Administrators can override the repository by adding the following to the config map entries: dx.deploy.contentui.repository The dx.deploy.dam.enabled tells the operator to deploy the HCL Digital Asset Management component. Note that there are 4 required sets of image/tag parameters: The HCL Digital Asset Management operator component uses prefixdx.deploy.dam.operator . The Postgres datastore component uses dx.deploy.dam.persistence . The HCL Digital Asset Management library services use dx.deploy.dam . The Image processor uses dx.deploy.dam.persistence . To override the repository values for the components above, use dx.deploy.COMPONENT.repository The last two parameters in the example YAML file provide the storage class and volume (must be ReadWriteMany ) for the HCL Digital Asset Management component. This is where the persistence layer maintains the datastore layer. The dx.deploy.dam.volume: volume name setting is optional if the storage class used/specified by dx.deploy.dam.storageclass is self-provisioning. A dx.dam.config.cors config map setting is auto-generated and provides the ability for Cross Origin Resource Sharing across Content Composer and Digital Asset Management resources. In the Digital Experience 9.5 core deployment, the dx.config.cors setting is set in the DX configuration map. Reference the Containerization Deployment pages for additional details. An additional self-provisioning volume is created for each of the HCL Digital Asset Management Persistence (Postgres) pods. The access mode of these self-provisioning persistent volumes must include ReadWriteOnce . If this volume is not present the images are lost and shows blank if/when the HCL Digital Asset Management library is restarted. Administrators can override the repository by adding to: dx.deploy.contentui.repository In addition, the following default settings are configurable: dx.deploy.contentui.resources.cpurequest , the default is 1. dx.deploy.contentui.resources.cpulimit , the default is 3. dx.deploy.contentui.resources.memoryrequest , the default is 2G. dx.deploy.contentui.resources.memorylimit , the default is 4G. dx.deploy.experienceapi.resources.cpurequest , the default is 1. dx.deploy.experienceapi.resources.cpulimit , the default is 3. dx.deploy.experienceapi.resources.memoryrequest , the default is 2G. dx.deploy.experienceapi.resources.memorylimit , the default is 4G. Additional configuration options are currently not supported. Deploy the YAML ( dx-deploy-config-map.yaml ) by issuing the following: Kubernetes command: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace - OpenShift command: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml ``` Stop and restart the HCL Digital Experience 9.5 CF181 and higher container deployment. Note: If you are adding components for HCL Content Composer, HCL Digital Asset Management, and HCL Experience API to an existing HCL Digital Experience 9.5 environment (must be at level 9.5 CF181 or higher) deployment, you must stop the deployment and restart it with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/removeDx.sh NAMESPACE Note: This script removes resources from the existing deployment (pods, statefulsets, etc) but does not remove persisted data or existing configmaps. Remove the claimRef from the PersistedVolume. Note: Instructions to re-use the Persistent Volume may also be viewed in the Deploy HCL Digital Experience 9.5 Container to Amazon EKS topic. Open the persistent volume in a visual editor (vi) using the Kubernetes or OpenShift command line client command: kubectl edit pv <pv name> or oc edit pv <pv name> Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: awseks-demo resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the ' persistentvolume/your_namespace edited ' message. Change to the extracted hcl-dx-cloud-scripts directory. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE or ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG Note: You must restart the deployment with one (1) replica. Once it is fully started, you can safely scale it to N instances. Reminder : As outlined in this section, adding new components to a production deployment requires an outage and some setup time. It is advisable to plan carefully if you are currently running a Digital Experience container deployment in a supported Kubernetes environment. Access the HCL Content Composer and HCL Digital Asset Management components by navigating to Practitioner Studio > Web Content > Content , or Practitioner Studio > Digital Assets . https://your-portal.net/wps/myportal/Practitioner/Web Content/Content Library https://your-portal.net/wps/myportal/Practitioner/Digital Assets Access the HCL Experience API Explorer at the following URL: http://<HOST>:<PORT>dx/api/core/v1/explorer For example, http://127.0.0.1:3000/dx/api/core/v1/explorer","title":"Installing the HCL Digital Experience 9.5 Container components"},{"location":"containerization/install_config_cc_dam/#optional-configure-digital-asset-management-with-a-cdn","text":"If you are using a content delivery network (CDN) such as Akamai , using Vary: Origin may prevent you from caching content. To bypass this limitation, your CDN configuration must strip the Vary header on the way in, to reinstate your ability to cache content. On the way out, you can append the Origin parameter to the Vary header when serving a response using 'Modify Outgoing Response Header' . Parent topic: Operator-based deployment","title":"(Optional) Configure Digital Asset Management with a CDN"},{"location":"containerization/kubernetes_eks/","text":"Deploy DX Container to Amazon EKS Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX Container to Amazon EKS Learn how to deploy different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Finding the Kubernetes EKS deployment This section describes how to find your HCL DX deployment. Understanding the HCL DX 9.5 container deployment on Amazon EKS This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 container release in on Amazon Elastic Kubernetes Service (EKS). Customizing the Kubernetes EKS deployment This section describes how to customize your HCL Portal deployment. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/kubernetes_eks/#deploy-dx-container-to-amazon-eks","text":"Learn how to deploy, find, understand, and customize the different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Deploy DX Container to Amazon EKS Learn how to deploy different releases of HCL Digital Experience 9.5 containers, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Finding the Kubernetes EKS deployment This section describes how to find your HCL DX deployment. Understanding the HCL DX 9.5 container deployment on Amazon EKS This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 container release in on Amazon Elastic Kubernetes Service (EKS). Customizing the Kubernetes EKS deployment This section describes how to customize your HCL Portal deployment. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploy DX Container to Amazon EKS"},{"location":"containerization/kubernetes_eks_cf191andearlier/","text":"Deploy DX CF191 and earlier release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local (local to cloud or system). If deploying HCL Digital Experience 9.5 Container Update release, the image and package names are as follows: CF183-core.zip files: HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Note: Images included in the \u2018other\u2019 package released with CF183 are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. About this task Follow these steps to deploy HCL Digital Experience 9.5 CF183 and higher container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. DX Container Management Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_eks_load_01.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag> DX Deployment Unzip Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip Change directory Change to the extracted files directory, cd hcl-dx-cloud-scripts Custom resource definition Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml, if required Use either of the following commands: ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF183 and higher Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Update YAML Replace the REPOSITORY NAME , IMAGE TAG , AMBASSADOR , and REDIS values in operator.yaml Deploy Create the DX container deployment. Run the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s). For example: ``` $cd hcl-dx-cloud-scripts $scripts/deployDx.sh dx-11 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 ``` Generate TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment . Final Output External IP from Load balancer in the below example can be used to access PORTAL Output $ kubectl get all -n NAMESPACE https://EXTERNAL_IP/wps/portal Parent topic: Deploy DX Container to Amazon EKS","title":"Deploy DX CF191 and earlier release Containers to Amazon EKS"},{"location":"containerization/kubernetes_eks_cf191andearlier/#deploy-dx-cf191-and-earlier-release-containers-to-amazon-eks","text":"This section describes how to deploy HCL Digital Experience 9.5 CF191 and earlier release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX CF191 and earlier release Containers to Amazon EKS"},{"location":"containerization/kubernetes_eks_cf191andearlier/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, following are some preliminary steps that need to be taken. Setup KUBECONFIG to refer to the target server. This will ensure any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local (local to cloud or system). If deploying HCL Digital Experience 9.5 Container Update release, the image and package names are as follows: CF183-core.zip files: HCL DX notices V9.5 CF183.txt hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Note: Images included in the \u2018other\u2019 package released with CF183 are optional and used to support use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services.","title":"Prerequisites"},{"location":"containerization/kubernetes_eks_cf191andearlier/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF183 and higher container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic.","title":"About this task"},{"location":"containerization/kubernetes_eks_cf191andearlier/#dx-container-management","text":"Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF183_20200819-1159.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF183_20200819-1711.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. ![](../images/container_eks_load_01.png \"Loading containers into your Docker repository\") Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag>","title":"DX Container Management"},{"location":"containerization/kubernetes_eks_cf191andearlier/#dx-deployment","text":"Unzip Extract the HCL DX deployment scripts onto your environment as follows: unzip hcl-dx-cloud-scripts-v95_CF183_20200819-1711.zip Change directory Change to the extracted files directory, cd hcl-dx-cloud-scripts Custom resource definition Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml, if required Use either of the following commands: ``` kubectl create -f hcl-dx-cloud-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml - ``` ./scripts/deployCrd.sh Persistence volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF183 and higher Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, here are the parameters that have been tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Update YAML Replace the REPOSITORY NAME , IMAGE TAG , AMBASSADOR , and REDIS values in operator.yaml Deploy Create the DX container deployment. Run the deployDx.sh script to create the namespace, install the project scoped service account, role, role binding, operator, and deployment. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG NAMESPACE - the project or the namespace to create or use for deployment. This namespace will be used in subsequent commands. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the repository above. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . INGRESSIMAGE - The image name to use for ambassador (Native K8s). INGRESSTAG - The image tag to use for ambassador (Native K8s). For example: ``` $cd hcl-dx-cloud-scripts $scripts/deployDx.sh dx-11 1 REPO_NAME dxen v95_CF183_20200818-1342 dx-pv-11 dx-deploy-stg derby ambassador 154 ```","title":"DX Deployment"},{"location":"containerization/kubernetes_eks_cf191andearlier/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment .","title":"Generate TLS Certificate"},{"location":"containerization/kubernetes_eks_cf191andearlier/#final-output","text":"External IP from Load balancer in the below example can be used to access PORTAL Output $ kubectl get all -n NAMESPACE https://EXTERNAL_IP/wps/portal Parent topic: Deploy DX Container to Amazon EKS","title":"Final Output"},{"location":"containerization/kubernetes_eks_cf192andlater/","text":"Deploy DX CF192 and later release Containers to Amazon EKS This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . Prerequisites Prior to using the following procedures, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, the following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. dxctl tool - If deploying Digital Experience Container Update CF192 or later, the dcxtl tool is used to install and configure the deployment. Documentation resource: Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local file system. This can be a local workstation or a local cloud platform. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. If deploying HCL Digital Experience 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: HCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Note: Images included in the \u2018other\u2019 package released with CF192 are optional (in addition to HCL DX 9.5 core Portal and Web Content Manager) and are used to support the use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services. About this task Follow these steps to deploy HCL Digital Experience 9.5 CF192 and later container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying a Container Update CF191 or earlier, view the instructions to deploy using script-based commands , instead of the dxctl tool commands used in this section. DX Container Management Follow these steps to deploy the HCL Digital Experience 9.5 CF192 and later container release in Amazon EKS Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF192_20210225-004909.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz - ``` docker load < hcl-dx-image-processor-image-v1.6.0_20210226-0014.tar.gz - ``` docker load < hcl-dx-digital-asset-management-operator-image-v95_CF192_20210226-0040.tar.gz - ``` docker load < hcl-dx-postgres-image-v1.6.0_20210226-0009.tar.gz - ``` docker load < hcl-dx-ringapi-image-v1.6.0_20210226-0055.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag> Persistence volume Set the Persistent volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, following are the parameters that are tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Login to the Kubernetes cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform-specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, or Google GKE). Example: aws eks update-kubeconfig --name <eks_cluster> --region <region> Configure and deploy Configure and deploy using HCL DX dxctl tool . Change directory Change to the extracted files directory: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure dxctl Configure the dxctl properties for the HCL DX Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. See the following sample values: ``` dx.namespace: kube-eg-cf192-0225 - ``` dx.image: dxen - ``` dx.tag: v95_CF192_20210225-035822_xxxxxxx_95_CF192_60374773 ``` dx.storageclass:dx-deploy-stg ``` - ``` dx.volume: kube-eg-d2-core-pv - ``` dx.volume.size:60 - ``` remote.search.enabled:false - ``` openldap.enabled:false - ``` api.enabled: false - ``` composer.enabled: false - ``` dam.enabled: false - ``` ingress.image:dx-build-output/common/ambassador - ``` ingress.tag:1.5.4 - ``` ingress.redis.image:redis - ``` ingress.redis.tag:5.0.1 - ``` dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator - ``` dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 **Important:** With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. **Note:** With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: ![](../images/Container_deploy_amazon.png) Deploy using dxctl tool Run the dxctl command to deploy the HCL DX 9.5 CF192 and later release to Amazon EKS. For Windows: ./win/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Linux: ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Mac: ./mac/dxctl -\u2013update -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps create the DX 9.5 CF192 and later deployment. Generate TLS Certificate Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment . Access the deployment Obtain the external IP from the platform load balancer as shown in following example to access the HCL DX 9.5 CF192 and later deployment: Output: $ kubectl get all -n NAMESPACE Example: The deployed system can be accessed at: https://external-ip/wps/portal Update To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with new image values and run the Update command. Example: On Mac: ./mac/dxctl -\u2013update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment. Delete To delete the deployment, follow either of these methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Example: ../linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true If some deployment resources (such as services) are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploy DX Container to Amazon EKS","title":"Deploy DX CF192 and later release Containers to Amazon EKS"},{"location":"containerization/kubernetes_eks_cf192andlater/#deploy-dx-cf192-and-later-release-containers-to-amazon-eks","text":"This section describes how to deploy HCL Digital Experience 9.5 CF192 and later release containers, along with the Ambassador, to Kubernetes as verified in Amazon Elastic Kubernetes Service (Amazon EKS) .","title":"Deploy DX CF192 and later release Containers to Amazon EKS"},{"location":"containerization/kubernetes_eks_cf192andlater/#prerequisites","text":"Prior to using the following procedures, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have the appropriate access to the target environment. If not, the following are some preliminary steps that must be taken. Setup KUBECONFIG to refer to the target server. This ensures any kubectl commands executed locally affect the target environment. Amazon EKS Cluster The following tools must be installed on a machine other than the Portal server: Docker AWS Command Line Interface (CLI) - used to get image details. dxctl tool - If deploying Digital Experience Container Update CF192 or later, the dcxtl tool is used to install and configure the deployment. Documentation resource: Deploy DX Container to Microsoft Azure Kubernetes Service (AKS) Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain For DAM, additional volume is needed. For more details on storage class and volume, see Sample storage class and volume . Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization; NFS is an option for this. Hardware: 4 cores / 7 GB - - - 5 cores / 9 GB Amazon Elastic Container Registry (ECR) or any container registry access, for tagging and pushing. See the support topic for the HCL Digital Experience detailed system requirements . Download and extract the contents of the HCL DX 9.5 package to the local file system. This can be a local workstation or a local cloud platform. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. If deploying HCL Digital Experience 9.5 Container Update CF192 release, the image and package names are as follows: CF192-core.zip files: HCL DX notices V9.5 CF192.txt dxclient_v1.2.0_20210305-1758.zip hcl-dx-ambassador-image-154.tar.gz hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz Note: Images included in the \u2018other\u2019 package released with CF192 are optional (in addition to HCL DX 9.5 core Portal and Web Content Manager) and are used to support the use of OpenLDAP, Remote Search, the Experience API, Content Composer, and Digital Asset Management components and services.","title":"Prerequisites"},{"location":"containerization/kubernetes_eks_cf192andlater/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 CF192 and later container release, along with Ambassador to Kubernetes, as verified in Amazon Elastic Kubernetes Service (Amazon EKS) . This deployment relies heavily on Kubernetes Operators for full functionality. If deploying a Container Update CF191 or earlier, view the instructions to deploy using script-based commands , instead of the dxctl tool commands used in this section.","title":"About this task"},{"location":"containerization/kubernetes_eks_cf192andlater/#dx-container-management","text":"Follow these steps to deploy the HCL Digital Experience 9.5 CF192 and later container release in Amazon EKS Change directory Open a terminal window and change to the root directory of the extracted package. Docker load Load the containers into your Docker repository: ``` docker load < hcl-dx-core-image-v95_CF192_20210225-004909.tar.gz - ``` docker load < hcl-dx-ambassador-image-154.tar.gz - ``` docker load < hcl-dx-cloud-operator-image-v95_CF192_20210225-0546.tar.gz - ``` docker load < hcl-dx-image-processor-image-v1.6.0_20210226-0014.tar.gz - ``` docker load < hcl-dx-digital-asset-management-operator-image-v95_CF192_20210226-0040.tar.gz - ``` docker load < hcl-dx-postgres-image-v1.6.0_20210226-0009.tar.gz - ``` docker load < hcl-dx-ringapi-image-v1.6.0_20210226-0055.tar.gz - ``` docker load < hcl-dx-redis-image-5.0.1.tar.gz **Note:** Either **`-i`** or **`<`** works for the load command. In case you encounter an error when using one, try running the command using the other. Docker tag and push Get the Docker images in your local Docker repository to your target Kubernetes system by tagging and pushing them appropriately. If you used docker load to get your images on the target environment, proceed to the next step. ``` docker tag - ``` docker push <image:tag>","title":"DX Container Management"},{"location":"containerization/kubernetes_eks_cf192andlater/#persistence-volume","text":"Set the Persistent volume Create (or have the Kubernetes Administrator create) a persistent volume and storage class where the AccessMode must be ReadWriteMany and the persistent volume reclaim policy must be Retain . For more details on storage class and volume, see Sample storage class and volume NFS server Provide the HCL DX 9.5 CF192 and later Docker image access to the volume mount created in order to copy the profile. There are various ways to do this and NFS is one option. If NFS is used, following are the parameters that are tested to work: rw, (Default) sync , (Default after NFS 1.0, means that the server does not reply until after the commit) insecure ,** (Requires requests originate on ports less than 1024) root_squash ,** (Map requests to the nobody user). hard ,** (Required because this means the system will keep trying to write until it works.) nfsvers=4.1 , rsize=8388608 , (Avoids dropped packages, default 8192) wsize=8388608 , (Avoids dropped packages, default 8192) timeo=600 , (60 seconds) retrans=2 , (Number of retries after a time out) noresvport ** (Tells the NFS client to use a new Transmission Control Protocol (TCP) source port when a network connection is reestablished. Doing this helps make sure that the EFS file system has uninterrupted availability after a network recovery event.) Note: Those marked with ** are critical and, in many cases, it is recommended to have the rsize and wsize set to 8388608 . Login to the Kubernetes cluster Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform-specific CLI (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, or Google GKE). Example: aws eks update-kubeconfig --name <eks_cluster> --region <region>","title":"Persistence volume"},{"location":"containerization/kubernetes_eks_cf192andlater/#configure-and-deploy","text":"Configure and deploy using HCL DX dxctl tool . Change directory Change to the extracted files directory: cd hcl-dx-cloud-scripts Using DX Container Update CF192 and later, the directory structure appears as follows: Configure dxctl Configure the dxctl properties for the HCL DX Container CF192 and later deployment. Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. See the following sample values: ``` dx.namespace: kube-eg-cf192-0225 - ``` dx.image: dxen - ``` dx.tag: v95_CF192_20210225-035822_xxxxxxx_95_CF192_60374773 ``` dx.storageclass:dx-deploy-stg ``` - ``` dx.volume: kube-eg-d2-core-pv - ``` dx.volume.size:60 - ``` remote.search.enabled:false - ``` openldap.enabled:false - ``` api.enabled: false - ``` composer.enabled: false - ``` dam.enabled: false - ``` ingress.image:dx-build-output/common/ambassador - ``` ingress.tag:1.5.4 - ``` ingress.redis.image:redis - ``` ingress.redis.tag:5.0.1 - ``` dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator - ``` dx.operator.tag: v95_CF192_20210225-0546_xxxxxxxxx_95_CF192 **Important:** With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. **Note:** With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: ![](../images/Container_deploy_amazon.png) Deploy using dxctl tool Run the dxctl command to deploy the HCL DX 9.5 CF192 and later release to Amazon EKS. For Windows: ./win/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Linux: ./linux/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties For Mac: ./mac/dxctl -\u2013update -p /home/$USER/deployments/myfirst_deployment.properties Note: These steps create the DX 9.5 CF192 and later deployment.","title":"Configure and deploy"},{"location":"containerization/kubernetes_eks_cf192andlater/#generate-tls-certificate","text":"Create a TLS certification to be used by the deployment: For development purposes: Using OpenSSL, you can create a private key: openssl genrsa -out my-key.pem 2048 Using OpenSSL, you can create a certificate signed by the private key: openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert Create a TLS certification: kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n aws-mynamespace Note: The default name is the dx-tls-cert this can be changed in the configuration. aws-mynamespace is your Kubernetes namespace. You can set your preferred namespace but you must consistently use this namespace in subsequent commands. See Customizing the Kubernetes deployment .","title":"Generate TLS Certificate"},{"location":"containerization/kubernetes_eks_cf192andlater/#access-the-deployment","text":"Obtain the external IP from the platform load balancer as shown in following example to access the HCL DX 9.5 CF192 and later deployment: Output: $ kubectl get all -n NAMESPACE Example: The deployed system can be accessed at: https://external-ip/wps/portal","title":"Access the deployment"},{"location":"containerization/kubernetes_eks_cf192andlater/#update","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with new image values and run the Update command. Example: On Mac: ./mac/dxctl -\u2013update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Note: If using HCL DX 9.5 Container Update CF192 or later, the dxctl tool can be used to Update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an Update process, administrators should check the DxDeployment custom resource definition ( hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml ) for changes and update accordingly: Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Kubernetes command: kubectl create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment.","title":"Update"},{"location":"containerization/kubernetes_eks_cf192andlater/#delete","text":"To delete the deployment, follow either of these methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Example: ../linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true If some deployment resources (such as services) are not deleted, run the following command: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploy DX Container to Amazon EKS","title":"Delete"},{"location":"containerization/kubernetes_remote_search/","text":"Configure Remote Search in OpenShift and Kubernetes This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Note: Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. Introduction Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL. Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages. EJBs and host names HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs. Defining serverindex.xml on the Remote Search server When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned. Remote Search services configuration The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note: The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note: The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Configure WCM Authoring Portlet search function Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Additional Routing Configuration for supported Kubernetes platforms To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note: Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console Parent topic: Customizing your container deployment","title":"Configure Remote Search in OpenShift and Kubernetes"},{"location":"containerization/kubernetes_remote_search/#configure-remote-search-in-openshift-and-kubernetes","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 environments on supported Red Hat OpenShift and Kubernetes container platforms. Note: Prior to Container Update CF195, the HCL Digital Experience 9.5 Remote Search image is supported for deployment to Red Hat OpenShift. With the Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms.","title":"Configure Remote Search in OpenShift and Kubernetes"},{"location":"containerization/kubernetes_remote_search/#introduction","text":"Using HCL Digital Experience 9.5 Remote Search images in the supported cloud container platforms, such as Red Hat OpenShift, require a different setup and configuration steps than those used to set up Remote Search on a non-Docker or Kubernetes container platform . As information, the serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Follow the guidance in this section to define collections in the core HCL DX 9.5 container environment with respect to JCR text search collections, rather than guidance published for the on-premises (non-Docker or Kubernetes) platforms for the JCR collection URL.","title":"Introduction"},{"location":"containerization/kubernetes_remote_search/#deploying-remote-search-in-hcl-digital-experience-95-openshift-and-kubernetes-platforms","text":"Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and later packages include a core software and Remote search container. Load both of these images into an OpenShift release platform supported by HCL DX 9.5 such as Red Hat OpenShift. Use CF195 and later if you deploy to a Kubernetes platform. See the following Additional Routing Configuration for supported Kubernetes platforms topic for information about deploying to Kubernetes container platforms such as Amazon EKS, Azure AKS, or Google GKE. In this example, the OpenShift load command can be used. Note that if your organization has a corporate OpenShift repository, you might use OpenShift pull instead to put it into your local repository. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz The first one (dx-core-image), is the core HCL DX 9.5 Portal and Web Content Manager image while the second one (dx-dxrs-image) is the remote search image. After the Remote Search images are loaded to the Kubernetes environment that you deploy to, follow deployment steps for that platform presented in the HCL Digital Experience 9.5 Container Deployment topic pages.","title":"Deploying Remote Search in HCL Digital Experience 9.5 OpenShift and Kubernetes platforms"},{"location":"containerization/kubernetes_remote_search/#ejbs-and-host-names","text":"HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force the Kubernetes environment, such as Red Hat OpenShift to write the /etc/hosts file at container initialization time is needed. HCL DX 9.5 Container operators that execute image deployment to Kubernetes platforms such as Red Hat OpenShift create the correct host-name in /etc/hosts for the local container. In addition, these operators execute a DNS resolution on foreign host-names as long as they are on the same Kubernetes deployment. Portal and Portal Remote Search both use WebSphere Application Server as a base. As they are on different hosts (containers), they have to be able to talk to each other via IP. The initial conversation between Portal DX and the Remote Search server take place over IIOP (rmi) which is the internet protocol of EJBs.","title":"EJBs and host names"},{"location":"containerization/kubernetes_remote_search/#defining-serverindexxml-on-the-remote-search-server","text":"When deploying the Remote Search image on supported Kubernetes platforms, additional configuration settings for the serverindex.xml are required. When deployed to Kubernetes, the HCL DX 9.5 container operators are configured to check to ensure that the server name is correct. Note that dx-deployment-service-remotesearch is a DNS resolvable name from the point of view of the HCL Digital Experience 9.5 Server. The remote search server includes the \u201cping\u201d command. You can use this to verify that the host name dx-deployment-service-remotesearch resolves to a valid IP address. Now, when the HCL DX 9.5 server communicates to the Remote Search server over IIOP, the Remote Search Server returns dx-deployment-service-remotesearch as the host name of the Remote Search Server. The HCL DX 9.5 Server has configuration that appends the port to the host name that was just returned.","title":"Defining serverindex.xml on the Remote Search server"},{"location":"containerization/kubernetes_remote_search/#remote-search-services-configuration","text":"The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic must be completed in a Kubernetes container-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX 9.5 image deployment to supported Kubernetes platforms. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use the URL configuration host as defined in the table below (dx-deployment-service-remotesearch) as the host, and the appropriate port for the SSL access. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Kubernetes container deployment. See the section on Creating a new search service for more information. For testing Search Services configuration, the following are used: Item Value IIOP_URL iiop://dx-deployment-service-remotesearch:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Note: The \u201csearch collection location\u201d is relative to the remote search container. Furthermore, one places the collection in the profile of the Remote Search server because the profile of the remote search server is persisted. One obviously wants the search indexes persisted across restarts. Use the following parameters to create a Content Source JCR search collection . The Collect documents linked from this URL is https://dx-deployment-service:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&Action=GetDocuments&Range=100&locale=en-US Note that the host and port are the Kubernetes (for example, Red Hat OpenShift) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 Server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm Complete the following configuration parameters to enable search in the Web Content Manager Authoring i interfaces: Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1","title":"Remote Search services configuration"},{"location":"containerization/kubernetes_remote_search/#jcr-content-source-configuration","text":"Use the following URL for Collect documents linked from this URL : https://dx-deployment-service:10042/wps/seedlist/myserver?Action=GetDocuments&Format=ATOM&Locale=en_US&Range=100&Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&Start=0&SeedlistId=1@OOTB_CRAWLER1 The parsing of the SeedlistId positional parameter in this URL uses an index of the virtual portal being crawled. In this case 1 (in 2 places) represents the base virtual portal. Note: The host and port are the Kubernetes (for example, Red Hat OpenShift ) service host and the port to which 10042 was mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL DX 9.5 server. Note also that one can put this URL in a browser (on the OpenShift host) and confirm that the response is an ATOM feed. On the Security panel, use dx-deployment-service as the host name, along with the username wpsadmin and the associated password for wpsadmin . One can also specify Realm as CrawlerUsersRealm","title":"JCR Content Source Configuration"},{"location":"containerization/kubernetes_remote_search/#configure-wcm-authoring-portlet-search-function","text":"Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dx-deployment-service-remotesearch:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path.","title":"Configure WCM Authoring Portlet search function"},{"location":"containerization/kubernetes_remote_search/#additional-routing-configuration-for-supported-kubernetes-platforms","text":"To configure Remote Search to DX 9.5 container deployments to supported Kubernetes platforms: Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), or Google Kubernetes Engine (GKE) requires specific steps. The HCL DX 9.5 core and Remote Search services each require /ibm/console as the route path when accessing the Admin Console. Due to the overlapping of the path mappings, to configure Remote Search, DX administrators can apply a solution to expose the Remote Search route via an additional Load balancer, as follows: Create a new service for Remote Search with service type as Loadbalancer. Note: Do not alter the Remote Search Service created by the DX-Operator. Sample Yaml: apiVersion: v1 kind: Service metadata: labels: app: dx-deployment-remotesearch release: dx-deployment name: dx-deployment-service-remotesearch-lb spec: ports: - name: was-admin port: 9060 protocol: TCP targetPort: 9060 - name: was-admin-sec port: 9043 protocol: TCP targetPort: 9043 - name: boot-port port: 2809 protocol: TCP targetPort: 2809 - name: rs-port port: 9403 protocol: TCP targetPort: 9403 selector: app: dx-deployment-remotesearch sessionAffinity: None type: LoadBalancer Apply this configuration using the following example command: $ kubectl apply -f filename.yaml Remote Search Routes (example results): Access the Remote Search Admin Console via the external IP address of your DX 9.5 Container deployment: Example: https://35.xxx.174.3:9043/ibm/console Parent topic: Customizing your container deployment","title":"Additional Routing Configuration for supported Kubernetes platforms"},{"location":"containerization/limitations_requirements/","text":"Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels. Requirements and Limitations for Helm-based deployments This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0 Requirements and Limitations for Operator-based deployments Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Containerization requirements and limitations"},{"location":"containerization/limitations_requirements/#containerization-requirements-and-limitations","text":"This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels.","title":"Containerization requirements and limitations"},{"location":"containerization/limitations_requirements/#requirements-and-limitations-for-helm-based-deployments","text":"This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0","title":"Requirements and Limitations for Helm-based deployments"},{"location":"containerization/limitations_requirements/#requirements-and-limitations-for-operator-based-deployments","text":"Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Requirements and Limitations for Operator-based deployments"},{"location":"containerization/maintenance/","text":"Container administration 9.5 The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence. Images include fixes to HCL Digital Experience (comprised of HCL Portal and HCL Web Content Manager), IBM WebSphere Application Server (WAS) Network Deployment, and the Java Development Kit (JDK), and additional components, such as the Experience API, Digital Asset Management, Content Composer, Remote Search, and more. Consult the Digital Experience Deployment topic for the latest list of DX 9.5 Container Images delivered through the Container Update releases. Customers should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Customers should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. Critical hot fixes are built for HCL Digital Experience 9.5 container releases when required outside of the container release process. HCL Digital Experience customers can contact HCL Customer Support if a critical fix is required for their HCL Digital Experience containerized deployment. There is a separation of product and customer code. Customizations should be kept in the wp_profile in the persistent volume and the external and the external database. As HCL Digital Experience 9.5 container development moves forward with the deployment type, an effort to better separate custom code from product code is planned. Video tutorial: HCL Digital Experience - Update an HCL Portal Docker Container Starting with 9.5, under the HCL Digital Experience container update releases, as new HCL Digital Experience 9.5 images become available from HCL, customers can modify the custom resource instance to use 9.5. The initial HCL Digital Experience 9.5 container update release available to DX customers in the HCL Software License Portal page. This pattern is possible because customizations are not made to the image but isolated to the persistent volume and external database, which follows best practices. The procedure for updating to a new release of the HCL Digital Experience 9.5 image is detailed in the Update the Digital Experience 9.5 Core Kubernetes Container Deployment . Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment Update the Digital Experience 9.5 Core Kubernetes container deployment. Update the HCL DX 9.5 Experience API, Content Composer, and Digital Asset Management components This section provides the steps to update the HCL Digital Experience 9.5 Experience API, HCL Digital Experience 9.5 Content Composer, and HCL Digital Experience 9.5 Digital Asset Management components. Optional: Configuration Settings to Manage Digital Asset Management Media upload storage services This section outlines optional configuration steps to tune Digital Asset Management storage services. HCL Digital Experience 9.5 Docker and Container initialization performance Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms. Customizing your container deployment This section outlines the customization options when deploying HCL Digital Experience Container. Backup and recovery procedures Containerization This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments. Parent topic: Operator-based deployment","title":"Container administration 9.5"},{"location":"containerization/maintenance/#container-administration-95","text":"The information in this section enables administrators to manage select operations performance controls, and to update and replace their HCL Digital Experience 9.5 container images with the latest 9.5 container update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence. Images include fixes to HCL Digital Experience (comprised of HCL Portal and HCL Web Content Manager), IBM WebSphere Application Server (WAS) Network Deployment, and the Java Development Kit (JDK), and additional components, such as the Experience API, Digital Asset Management, Content Composer, Remote Search, and more. Consult the Digital Experience Deployment topic for the latest list of DX 9.5 Container Images delivered through the Container Update releases. Customers should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Customers should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. Critical hot fixes are built for HCL Digital Experience 9.5 container releases when required outside of the container release process. HCL Digital Experience customers can contact HCL Customer Support if a critical fix is required for their HCL Digital Experience containerized deployment. There is a separation of product and customer code. Customizations should be kept in the wp_profile in the persistent volume and the external and the external database. As HCL Digital Experience 9.5 container development moves forward with the deployment type, an effort to better separate custom code from product code is planned. Video tutorial: HCL Digital Experience - Update an HCL Portal Docker Container Starting with 9.5, under the HCL Digital Experience container update releases, as new HCL Digital Experience 9.5 images become available from HCL, customers can modify the custom resource instance to use 9.5. The initial HCL Digital Experience 9.5 container update release available to DX customers in the HCL Software License Portal page. This pattern is possible because customizations are not made to the image but isolated to the persistent volume and external database, which follows best practices. The procedure for updating to a new release of the HCL Digital Experience 9.5 image is detailed in the Update the Digital Experience 9.5 Core Kubernetes Container Deployment . Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment Update the Digital Experience 9.5 Core Kubernetes container deployment. Update the HCL DX 9.5 Experience API, Content Composer, and Digital Asset Management components This section provides the steps to update the HCL Digital Experience 9.5 Experience API, HCL Digital Experience 9.5 Content Composer, and HCL Digital Experience 9.5 Digital Asset Management components. Optional: Configuration Settings to Manage Digital Asset Management Media upload storage services This section outlines optional configuration steps to tune Digital Asset Management storage services. HCL Digital Experience 9.5 Docker and Container initialization performance Beginning with from HCL Digital Experience 9.5 Container Update CF192 release, container DX applications initialization performance is improved. Review the following guidance for information, defaults, and options to manage container applications initialization performance when deployed to Docker, Red Hat OpenShift, and Kubernetes platforms. Customizing your container deployment This section outlines the customization options when deploying HCL Digital Experience Container. Backup and recovery procedures Containerization This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments. Parent topic: Operator-based deployment","title":"Container administration 9.5"},{"location":"containerization/openshift/","text":"Deploy DX 9.5 Container to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform. Deploying DX Container to Red Hat OpenShift Learn how to deploy different release of HCL Digital Experience (DX) 9.5 using the Red Hat OpenShift platform. Finding the OpenShift deployment This section describes how to find your HCL Portal deployment. Understanding the OpenShift deployment This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 Docker images to Red Hat OpenShift. Deploying Custom Code in OpenShift This section outlines deploying custom code to HCL Digital Experience in OpenShift. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploy DX 9.5 Container to Red Hat OpenShift"},{"location":"containerization/openshift/#deploy-dx-95-container-to-red-hat-openshift","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 to Red Hat OpenShift platform. Deploying DX Container to Red Hat OpenShift Learn how to deploy different release of HCL Digital Experience (DX) 9.5 using the Red Hat OpenShift platform. Finding the OpenShift deployment This section describes how to find your HCL Portal deployment. Understanding the OpenShift deployment This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 Docker images to Red Hat OpenShift. Deploying Custom Code in OpenShift This section outlines deploying custom code to HCL Digital Experience in OpenShift. Parent topic: HCL Digital Experience 9.5 Container Deployment","title":"Deploy DX 9.5 Container to Red Hat OpenShift"},{"location":"containerization/openshift_cf191andearlier/","text":"Deploying DX CF191 and earlier release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 CF191 and earlier release Containers using the Red Hat OpenShift platform. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have cluster admin access to the OpenShift environment. The following tools must be installed on a machine other than the Portal server: Docker OpenShift Command Line Interface (CLI) or kubectl Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. Video: Getting Started to deploy HCL DX 9.5 in Red Hat OpenShift Procedure Follow these steps to deploy the HCL Digital Experience (DX) 9.5 container release CF191 or earlier to Red Hat OpenShift. Download the HCL Digital Experience Container Update CF191 or earlier container product and extract it to your local file system: -rw- r--r-- 1 hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Note: If using HCL DX 9.5 container update release CF183 and higher, the hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip file is renamed to hcl-dx-cloud-scripts-v95_xxxxxxxx-xxxx.zip. The file name change also affects the directory name for future steps. Open a terminal window and change to the root directory of the extracted package. Load the containers into your Docker repository: ``` docker load < hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz ![](../images/container_openshift_load_01.png \"Loading containers into your Docker repository\") Extract the HCL DX deployment scripts: unzip hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Distribute the Docker images in your local Docker repository to your target OpenShift repository by tagging and pushing them appropriately. If you used docker load to place your images in the target repository, skip this and proceed to the next step. Syntax for tagging: docker tag <image_name>:<image_tag> <openshift_registry>/<image_name>:<tag> Syntax for pushing: docker push <openshift_registry>/<image_name>:<tag> Change to the extracted files directory. ./hcl-dx-openshift-scripts Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml if required. Use either of the following commands: - ./scripts/deployCrd.sh - kubectl create -f hcl-dx-openshift-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml Create (or have the OpenShift administrator create) a persistent volume where the AccessMode is set to ReadWriteMany and the persistent volume reclaim policy set to Retain . See the second item in the Prerequisites . To create the namespace, install the project scoped service account, role, role binding, operator, and deployment, run the deployDx.sh script. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE Note: For the initial 9.5 release, you need to edit the operator.yaml file and ensure the IMAGENAME and IMAGETAG are correct. NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - if you have a self provisioning storage class, you can use the keyword 'create' (or leave it blank) instead of the volume name. STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . Note: For more information, see sections on Understanding the OpenShift deployment and Customizing the container deployment . Update To update the deployment, follow these steps: Run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. For example, once the database is transferred, the DBTYPE will need to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment . Delete Removing the entire deployment requires several steps, this is by design. Run the following to remove the deployment in a specific namespace: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. Use any of the following commands to remove a namespace: OpenShift commands: oc delete \"project project\"_name oc delete -f dxNameSpace_NAMESPACE.yaml where NAMESPACE is the namespace to be removed. Kubernetes command: kubectl delete -f dxNameSpace_NAMESPACE.yaml where NAMESPACE is the namespace to be removed. The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using any of the following commands: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: ``` claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 ``` Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Deploying DX Container to Red Hat OpenShift","title":"Deploying DX CF191 and earlier release Containers to Red Hat OpenShift"},{"location":"containerization/openshift_cf191andearlier/#deploying-dx-cf191-and-earlier-release-containers-to-red-hat-openshift","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF191 and earlier release Containers using the Red Hat OpenShift platform.","title":"Deploying DX CF191 and earlier release Containers to Red Hat OpenShift"},{"location":"containerization/openshift_cf191andearlier/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have cluster admin access to the OpenShift environment. The following tools must be installed on a machine other than the Portal server: Docker OpenShift Command Line Interface (CLI) or kubectl Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. Video: Getting Started to deploy HCL DX 9.5 in Red Hat OpenShift","title":"Prerequisites"},{"location":"containerization/openshift_cf191andearlier/#procedure","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 container release CF191 or earlier to Red Hat OpenShift. Download the HCL Digital Experience Container Update CF191 or earlier container product and extract it to your local file system: -rw- r--r-- 1 hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Note: If using HCL DX 9.5 container update release CF183 and higher, the hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip file is renamed to hcl-dx-cloud-scripts-v95_xxxxxxxx-xxxx.zip. The file name change also affects the directory name for future steps. Open a terminal window and change to the root directory of the extracted package. Load the containers into your Docker repository: ``` docker load < hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz ![](../images/container_openshift_load_01.png \"Loading containers into your Docker repository\") Extract the HCL DX deployment scripts: unzip hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Distribute the Docker images in your local Docker repository to your target OpenShift repository by tagging and pushing them appropriately. If you used docker load to place your images in the target repository, skip this and proceed to the next step. Syntax for tagging: docker tag <image_name>:<image_tag> <openshift_registry>/<image_name>:<tag> Syntax for pushing: docker push <openshift_registry>/<image_name>:<tag> Change to the extracted files directory. ./hcl-dx-openshift-scripts Install the DxDeployment custom resource definition. Do not modify the git_v1_dxdeployment_crd.yaml file. Customize ./deploy/crds/git_v1_dxdeployment_cr.yaml if required. Use either of the following commands: - ./scripts/deployCrd.sh - kubectl create -f hcl-dx-openshift-scripts/deploy/crds/git_v1_dxdeployment_crd.yaml Create (or have the OpenShift administrator create) a persistent volume where the AccessMode is set to ReadWriteMany and the persistent volume reclaim policy set to Retain . See the second item in the Prerequisites . To create the namespace, install the project scoped service account, role, role binding, operator, and deployment, run the deployDx.sh script. ./scripts/deployDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE Note: For the initial 9.5 release, you need to edit the operator.yaml file and ensure the IMAGENAME and IMAGETAG are correct. NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - if you have a self provisioning storage class, you can use the keyword 'create' (or leave it blank) instead of the volume name. STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. HCL DX 9.5 uses Apache Derby, Oracle Database, IBM DB2, or Microsoft SQL Server. Acceptable values are derby , oracle , db2 , or msSql . Note: For more information, see sections on Understanding the OpenShift deployment and Customizing the container deployment .","title":"Procedure"},{"location":"containerization/openshift_cf191andearlier/#update","text":"To update the deployment, follow these steps: Run the updateDx.sh script with updated values: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/Kubernetes. IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. For example, once the database is transferred, the DBTYPE will need to be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment .","title":"Update"},{"location":"containerization/openshift_cf191andearlier/#delete","text":"Removing the entire deployment requires several steps, this is by design. Run the following to remove the deployment in a specific namespace: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. Use any of the following commands to remove a namespace: OpenShift commands: oc delete \"project project\"_name oc delete -f dxNameSpace_NAMESPACE.yaml where NAMESPACE is the namespace to be removed. Kubernetes command: kubectl delete -f dxNameSpace_NAMESPACE.yaml where NAMESPACE is the namespace to be removed. The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using any of the following commands: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: ``` claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 ``` Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Deploying DX Container to Red Hat OpenShift","title":"Delete"},{"location":"containerization/openshift_cf192andlater/","text":"Deploying DX CF192 and later release Containers to Red Hat OpenShift Learn how to deploy HCL Digital Experience (DX) 9.5 DX CF192 and later release Containers using the Red Hat OpenShift platform. Prerequisites Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have cluster admin access to the OpenShift environment. The following tools must be installed on a machine other than the Portal server: Docker Red Hat OpenShift Command Line Interface (CLI) or kubectl If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization. About this task Follow these steps to deploy HCL Digital Experience 9.5 container release in Red Hat OpenShift Container Update CF192 and later. If deploying a Container Update CF191 and earlier, see the instructions to deploy using script commands instead of the dxctl tool in this section. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. Video: Getting Started to deploy HCL DX 9.5 in Red Hat OpenShift Procedure Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release in Red Hat OpenShift. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic . Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system: -rw- r--r-- 1 hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Note: If using HCL DX 9.5 container update release CF183 and later, the hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip file is renamed to hcl-dx-cloud-scripts-v95_xxxxxxxx-xxxx.zip. The file name change also affects the directory name for future steps. Open a terminal window and change to the root directory of the extracted package. Load the containers into your Docker repository: ``` docker load < hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz ![](../images/container_openshift_load_01.png \"Loading containers into your Docker repository\") Extract the HCL DX deployment scripts: unzip hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Distribute the Docker images in your local Docker repository to your target OpenShift repository by tagging and pushing them appropriately. If you used docker load to place your images in the target repository, skip this and proceed to the next step. Syntax for tagging: docker tag <image_name>:<image_tag> <openshift_registry>/<image_name>:<tag> Syntax for pushing: docker push <openshift_registry>/<image_name>:<tag> Create (or have the OpenShift administrator create) a persistent volume where the AccessMode is set to ReadWriteMany and the persistent volume reclaim policy set to Retain . For more details refer to the Volume requirements section above. Deploy using the HCL DX dxctl tool . Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI. (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: Change to the extracted files directory, using DX Container Update CF192 and later, the directory structure appears as follows: ./hcl-dx-openshift-scripts Configuring dxctl properties for the DX 9.5 Container CF192 and later deployment: Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. Sample values: dx.namespace: caps-dx-deploy dx.image: dxen dx.tag: v95_CF192_20210223-004909 _release_95_CF192_603477b7 dx.storageclass:dx-deploy-stg dx.volume: caps-dx-core dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210223-0546 _release_95_CF192 Example: Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl : Run the command below using the dxctl tool to deploy the HCL DX 9.5 CF192 and later container on Red Hat Openshift: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a DX 9.5 CF192 and later deployment being created. Note: For more information, see sections on Understanding the OpenShift deployment and Customizing the container deployment . Update To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with the new image values, and then run the Update command. Examples: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment . Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition (hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Note: The default IBM WebSphere certificate that ships with Digital Experience 9.5 Docker images (only) expired on April 26, 2021. A new certificate is added to Digital Experience 9.5 Container Update CF194 and later docker images. If you are unable to update to CF194 for any reason, see the following HCL Digital Experience support tech note for manual steps to update the expired certificate: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . The updated certificate included with Digital Experience Container Update CF194 updates the Kubernetes secrets used by the DX 9.5 Red Hat OpenShift routes. Following a successful upgrade to DX 95 Container Update CF194, it is necessary to manually delete the secure Digital Experience 9.5 route if deployed on OpenShift. The operator recreates the route using the updated secrets data. If you are using additional custom routes that utilize the Digital Experience 9.5 container secrets, edit or delete/recreate them to use the updated secrets data. For example: oc delete route `<deployment name>`-service-dx-home-sec Delete To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Examples: . ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true NAMESPACE - the project or the namespace created or used for deployment. If some deployment resources such as services are not deleted, the following command can be run: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploying DX Container to Red Hat OpenShift","title":"Deploying DX CF192 and later release Containers to Red Hat OpenShift"},{"location":"containerization/openshift_cf192andlater/#deploying-dx-cf192-and-later-release-containers-to-red-hat-openshift","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 DX CF192 and later release Containers using the Red Hat OpenShift platform.","title":"Deploying DX CF192 and later release Containers to Red Hat OpenShift"},{"location":"containerization/openshift_cf192andlater/#prerequisites","text":"Prior to using the procedure below, it is assumed that the HCL DX Administrator is generally experienced in using Kubernetes. Additionally, the DX Administrator must have cluster admin access to the OpenShift environment. The following tools must be installed on a machine other than the Portal server: Docker Red Hat OpenShift Command Line Interface (CLI) or kubectl If deploying Digital Experience Container Update CF192 and later, the dxctl tool is used to install and configure the deployment Volume requirement: Requires an AccessMode of ReadWriteMany Requires a minimum of 40 GB , with the default request set to 100 GB RECLAIM POLICY = Retain Note: HCL Digital Experience is input-output (I/O) intensive which requires a high performing file system for optimization.","title":"Prerequisites"},{"location":"containerization/openshift_cf192andlater/#about-this-task","text":"Follow these steps to deploy HCL Digital Experience 9.5 container release in Red Hat OpenShift Container Update CF192 and later. If deploying a Container Update CF191 and earlier, see the instructions to deploy using script commands instead of the dxctl tool in this section. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic. Video: Getting Started to deploy HCL DX 9.5 in Red Hat OpenShift","title":"About this task"},{"location":"containerization/openshift_cf192andlater/#procedure","text":"Follow these steps to deploy the HCL Digital Experience (DX) 9.5 CF192 and later container release in Red Hat OpenShift. Note: Reference the latest HCL DX 9.5 Container Release and Update file listings in the Docker deployment topic . Download the HCL Digital Experience Container Update CF192 and later release container product and extract it to your local file system: -rw- r--r-- 1 hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz -rw- r--r-- 1 hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Note: If using HCL DX 9.5 container update release CF183 and later, the hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip file is renamed to hcl-dx-cloud-scripts-v95_xxxxxxxx-xxxx.zip. The file name change also affects the directory name for future steps. Open a terminal window and change to the root directory of the extracted package. Load the containers into your Docker repository: ``` docker load < hcl-dx-cloud-operator-image-v95_xxxxxxxx-xxxx.tar.gz - ``` docker load < hcl-dx-core-image-v95_xxxxxxxx-xxxx.tar.gz ![](../images/container_openshift_load_01.png \"Loading containers into your Docker repository\") Extract the HCL DX deployment scripts: unzip hcl-dx-openshift-scripts-v95_xxxxxxxx-xxxx.zip Distribute the Docker images in your local Docker repository to your target OpenShift repository by tagging and pushing them appropriately. If you used docker load to place your images in the target repository, skip this and proceed to the next step. Syntax for tagging: docker tag <image_name>:<image_tag> <openshift_registry>/<image_name>:<tag> Syntax for pushing: docker push <openshift_registry>/<image_name>:<tag> Create (or have the OpenShift administrator create) a persistent volume where the AccessMode is set to ReadWriteMany and the persistent volume reclaim policy set to Retain . For more details refer to the Volume requirements section above. Deploy using the HCL DX dxctl tool . Before using the dxctl tool to deploy, you must be logged in to the targeted cluster using the cloud platform specific CLI. (Red Hat OpenShift, Amazon EKS, Microsoft Azure AKS, Google GKE). Example: Change to the extracted files directory, using DX Container Update CF192 and later, the directory structure appears as follows: ./hcl-dx-openshift-scripts Configuring dxctl properties for the DX 9.5 Container CF192 and later deployment: Copy one of the provided properties files to further modify for your deployment. The modified properties file can be used for the deployment and the same file must be used for further updates. Example: mkdir -p /home/$USER/deployments/ cp dxctl/properties/full-deployment.properties /home/$USER/deployments/myfirst_deployment.properties Update the dxctl properties file values. Sample values: dx.namespace: caps-dx-deploy dx.image: dxen dx.tag: v95_CF192_20210223-004909 _release_95_CF192_603477b7 dx.storageclass:dx-deploy-stg dx.volume: caps-dx-core dx.volume.size:100 remote.search.enabled:false openldap.enabled:false api.enabled: false composer.enabled: false dam.enabled: false dx.operator.image: dx-build-output/hcldx-cloud-operator/hcldx-cloud-operator dx.operator.tag: v95_CF192_20210223-0546 _release_95_CF192 Example: Important: With HCL DX 9.5 Container Update CF197 and later, dam.features in full-deployment properties is added for use in a future container update release, and should not be modified except with direct guidance from HCL Support. Note: With HCL DX 9.5 Container Update CF193 and later, persist.force.read in full-deployment properties is added to enable a read-only Postgres pod for Digital Asset Management. This enables a failover capability for the Postgres service supporting DAM. Another option to enable a read-only pod is to set the persist.minreplicas: option set to greater than 1. Example: Deploy using dxctl : Run the command below using the dxctl tool to deploy the HCL DX 9.5 CF192 and later container on Red Hat Openshift: ./mac/dxctl --deploy -p /home/$USER/deployments/myfirst_deployment.properties Example: Note: This set of steps result in a DX 9.5 CF192 and later deployment being created. Note: For more information, see sections on Understanding the OpenShift deployment and Customizing the container deployment .","title":"Procedure"},{"location":"containerization/openshift_cf192andlater/#update","text":"To update the deployment to later HCL DX 9.5 Container Update releases, follow these steps: If using HCL DX 9.5 Container Update CF191 and earlier release, update the deployment properties file with the new image values, and then run the Update command. Examples: On Mac: ./mac/dxctl --update -p properties/myfirst_deployment.properties On Windows: .\\win\\dxctl.exe --update -p properties\\myfirst_deployment.properties On Linux: ./linux/dxctl -\u2013update -p properties/myfirst_deployment.properties Additional considerations: For example, once the database is transferred, the DBTYPE must be updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas could be increased. There are additional options to customize the deployment . Note: If using HCL DX 9.5 Container Update CF192 and later, the dxctl tool can be used to update the deployment. The dxctl tool does not deploy or update the DxDeployment custom resource definition. Prior to running an update process, administrators should check the DxDeployment custom resource definition (hcl-dx-cloud-scripts/deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml) for changes and update accordingly: Red Hat OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com CAUTION: Since crd is a cluster-wide resource, the use of oc delete crd dxdeployments.git.cwp.pnp-hcl.com causes a service outage for all the dx-deployment across the cluster. Red Hat OpenShift command: oc create -f deploy/crds/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Note: The default IBM WebSphere certificate that ships with Digital Experience 9.5 Docker images (only) expired on April 26, 2021. A new certificate is added to Digital Experience 9.5 Container Update CF194 and later docker images. If you are unable to update to CF194 for any reason, see the following HCL Digital Experience support tech note for manual steps to update the expired certificate: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . The updated certificate included with Digital Experience Container Update CF194 updates the Kubernetes secrets used by the DX 9.5 Red Hat OpenShift routes. Following a successful upgrade to DX 95 Container Update CF194, it is necessary to manually delete the secure Digital Experience 9.5 route if deployed on OpenShift. The operator recreates the route using the updated secrets data. If you are using additional custom routes that utilize the Digital Experience 9.5 container secrets, edit or delete/recreate them to use the updated secrets data. For example: oc delete route `<deployment name>`-service-dx-home-sec","title":"Update"},{"location":"containerization/openshift_cf192andlater/#delete","text":"To delete the deployment, follow one of two methods: Method 1: Remove the deployment but allow for redeployment with the same volumes: Examples: . ./linux/dxctl --destroy -p properties/myfirst_deployment.properties Method 2: Remove the entire namespace/project: Example: ./linux/dxctl --destroy -p properties/myfirst_deployment.properties -all true NAMESPACE - the project or the namespace created or used for deployment. If some deployment resources such as services are not deleted, the following command can be run: kubectl patch services $(kubectl get services -n $NAMESPACE | grep -v \"NAME\" |awk '{print $1}') -p '{\"metadata\":{\"finalizers\":null}}' -n $NAMESPACE Parent topic: Deploying DX Container to Red Hat OpenShift","title":"Delete"},{"location":"containerization/operator_backup_and_recovery_procedures/","text":"Backup and recovery procedures Containerization This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments. Digital Experience 9.5 Container Deployment Architecture Learn about the HCL DX 9.5 Container deployment architecture to get a better understanding of the backup and recovery options. Note: This topology is also available in the Install the HCL Digital Experience 9.5 components topic. Instructions to back up the Digital Experience 9.5 Container components The following sections describe how the administrators can create and manage backups and recovery of DX 9.5 Container components such as wp_profile, persistence layer - database, and the media in Digital Asset Management. 1. wp-profile backup Backup the file systems in the Digital Experience 9.5 container profile ( Persistent volume claim wp_profile ). Refer to the Backup and Restore topic and component backup guidance for more information. Note: The HCL Digital Asset Management (DAM) component uploads folder and the DAM persistent mount for the primary instance dx-deployment-persistence-0 in the statefulset dx-deployment-persistence (https://console-openshift-console.apps.hcl-dxdev.hcl-dx-dev.net/k8s/ns/master-tests/statefulsets/dx-deployment-persistence) . To create a backup of the profile Persistent volume claim wp_profile , it is recommended that: The DX has only one instance. The DX 9.5 container instance is stopped using the HCL Portal and HCL Web Content Manager command as follows: ``` kubectl exec --stdin --tty -n -- /bin/bash cd /opt/HCL/wp_profile/bin/ /stopServer.sh WebSphere_Portal -username -password ``` The entire /opt/HCL/wp_profile directory is backed up. In the command line interface, run the following command to back up the Digital Experience 9.5 Persistent volume claim wp_profile : Before running the tar command, ensure that the backup file system that you are using has ~50% free profile. ``` cd /opt/HCL/wp_profile tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile/* ``` After the tar backup command is completed, it is recommended that a copy of the backup.tar.gz file is created and placed to alternate long term storage. Recover from wp_profile pervasive volume backup You can extract the backup of the wp_profile volume to recover any files you need to restore. If the version of the backup matches the current fixpack level, you can use the extracted files to populate the original pervasive volume. The procedure to do this depends on how the backup was created. Any changes that occurred after the backup was created will not be recovered. The portal database must be restored to the backup that was created when the backup of wp_profile was created. 2. Persistence layer - database backup Run the following command to back up the container components that are managed through the DX Persistence layer: pg_dump name_of_database > name_of_backup_file To back up the system components on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file After the backup command is completed, it is recommended that a copy of the resulting file is created and placed to an alternate long term storage. See the Backup and restore DAM image topic for more information. 3. Digital Asset Management media backup Use the following commands to back up the Digital Asset Management media uploads volume: A command similar to the backup command outlined in Step 1 to create a backup of wp_profile can be used to back up the two /opt/app/upload and /etc/config Digital Asset Management mount points. Refer to the following examples: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload tar -C/ -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system etc/config/* See the Backup and restore DAM image topic for more information. Alternatively, the Kubernetes documentation pages present additional options to backup and clone persistent volumes. Volume SnapShots: (1.17 [beta] and later) CSI Volume Cloning Note: If either of the methods described in Step 2 or Step 3 is used, it is important to understand fuzzy backups with the wp_profile. A fuzzy backup is a copy of data files or directories that were operating in one state when the backup started, but in a different state by the time the backup completed. In case a volume snapshot or Container Storage Interface (CSI) volume cloning approach is used with the wp_profile , it is important that the snapshot is taken with the Digital Experience instance in shutdown state to ensure that recovery is performed. HCL Digital Experience has successfully tested the volume snapshot and CSI volume cloning methods with HCL Digital Experience 9.5 container deployments. It is recommended that customers perform the additional testing if they are using options Step 2 and Step 3 to manage the wp_profile backup. Restore Digital Asset Management image to previous version This shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in an Operator-based deployment using dxctl . Parent topic: Container administration 9.5","title":"Backup and recovery procedures Containerization"},{"location":"containerization/operator_backup_and_recovery_procedures/#backup-and-recovery-procedures-containerization","text":"This section shows the deployment architecture and provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 environments.","title":"Backup and recovery procedures Containerization"},{"location":"containerization/operator_backup_and_recovery_procedures/#digital-experience-95-container-deployment-architecture","text":"Learn about the HCL DX 9.5 Container deployment architecture to get a better understanding of the backup and recovery options. Note: This topology is also available in the Install the HCL Digital Experience 9.5 components topic.","title":"Digital Experience 9.5 Container Deployment Architecture"},{"location":"containerization/operator_backup_and_recovery_procedures/#instructions-to-back-up-the-digital-experience-95-container-components","text":"The following sections describe how the administrators can create and manage backups and recovery of DX 9.5 Container components such as wp_profile, persistence layer - database, and the media in Digital Asset Management. 1. wp-profile backup Backup the file systems in the Digital Experience 9.5 container profile ( Persistent volume claim wp_profile ). Refer to the Backup and Restore topic and component backup guidance for more information. Note: The HCL Digital Asset Management (DAM) component uploads folder and the DAM persistent mount for the primary instance dx-deployment-persistence-0 in the statefulset dx-deployment-persistence (https://console-openshift-console.apps.hcl-dxdev.hcl-dx-dev.net/k8s/ns/master-tests/statefulsets/dx-deployment-persistence) . To create a backup of the profile Persistent volume claim wp_profile , it is recommended that: The DX has only one instance. The DX 9.5 container instance is stopped using the HCL Portal and HCL Web Content Manager command as follows: ``` kubectl exec --stdin --tty -n -- /bin/bash cd /opt/HCL/wp_profile/bin/ /stopServer.sh WebSphere_Portal -username -password ``` The entire /opt/HCL/wp_profile directory is backed up. In the command line interface, run the following command to back up the Digital Experience 9.5 Persistent volume claim wp_profile : Before running the tar command, ensure that the backup file system that you are using has ~50% free profile. ``` cd /opt/HCL/wp_profile tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile/* ``` After the tar backup command is completed, it is recommended that a copy of the backup.tar.gz file is created and placed to alternate long term storage. Recover from wp_profile pervasive volume backup You can extract the backup of the wp_profile volume to recover any files you need to restore. If the version of the backup matches the current fixpack level, you can use the extracted files to populate the original pervasive volume. The procedure to do this depends on how the backup was created. Any changes that occurred after the backup was created will not be recovered. The portal database must be restored to the backup that was created when the backup of wp_profile was created. 2. Persistence layer - database backup Run the following command to back up the container components that are managed through the DX Persistence layer: pg_dump name_of_database > name_of_backup_file To back up the system components on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file After the backup command is completed, it is recommended that a copy of the resulting file is created and placed to an alternate long term storage. See the Backup and restore DAM image topic for more information. 3. Digital Asset Management media backup Use the following commands to back up the Digital Asset Management media uploads volume: A command similar to the backup command outlined in Step 1 to create a backup of wp_profile can be used to back up the two /opt/app/upload and /etc/config Digital Asset Management mount points. Refer to the following examples: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system /opt/app/upload tar -C/ -cvpzf backupmlcfg.tar.gz --exclude=/backupmlcfg.tar.gz --one-file-system etc/config/* See the Backup and restore DAM image topic for more information. Alternatively, the Kubernetes documentation pages present additional options to backup and clone persistent volumes. Volume SnapShots: (1.17 [beta] and later) CSI Volume Cloning Note: If either of the methods described in Step 2 or Step 3 is used, it is important to understand fuzzy backups with the wp_profile. A fuzzy backup is a copy of data files or directories that were operating in one state when the backup started, but in a different state by the time the backup completed. In case a volume snapshot or Container Storage Interface (CSI) volume cloning approach is used with the wp_profile , it is important that the snapshot is taken with the Digital Experience instance in shutdown state to ensure that recovery is performed. HCL Digital Experience has successfully tested the volume snapshot and CSI volume cloning methods with HCL Digital Experience 9.5 container deployments. It is recommended that customers perform the additional testing if they are using options Step 2 and Step 3 to manage the wp_profile backup. Restore Digital Asset Management image to previous version This shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in an Operator-based deployment using dxctl . Parent topic: Container administration 9.5","title":"Instructions to back up the Digital Experience 9.5 Container components"},{"location":"containerization/optional_digital_asset_management_storage_configuration_settings/","text":"Optional: Configuration Settings to Manage Digital Asset Management Media upload storage services This section outlines optional configuration steps to tune Digital Asset Management storage services. Reference the HCL Digital Experience Deployment topic for the latest list of HCL Digital Experience 9.5 container files available for your implementation, and instructions to install to supported container platforms. As outlined in the steps to install and configure HCL Digital Asset Management, four files are provided for deployment: HCL Digital Asset Management (Operator) HCL Digital Experience Digital Asset Management (Docker image) HCL Digital Experience Digital Asset Management (Image processor) Postgres Persistence Layer There are two independent volume types used in the deployment and runtime of the Digital Asset Management: There is a volume per persistence layer, this volume is self-provisioned and defaults to the platform\u2019s default storage class and therefore the volume has not been made configurable. See the Sample Storage Class and Volume topic for additional information to set storage class and volume using sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. There is a Digital Asset Management Media storage upload folder that is related to the upload performance of assets inside the Digital Asset Management library. This folder is a read/write/many folder and therefore is shared by all instances. There are 2 main settings for the Digital Asset Management upload folder: dam.deploy.dam.storageclass (REQUIRED) dam.deploy.dam.volume (OPTIONAL) There are two choices to configure this volume: DX Administrator specifies volume: In this model, the DX administrator sets both the dam.deploy.dam.storageclass and dam.deploy.dam.volume before initial deployment, and therefore tells the deployment to use a specific storage class and a specific volume provisioned within that storage class. This option provides a bit more control to the DX Administrator. DX Administrator specifies only the dam.deploy.dam.storageclass , and allows either a volume from a pool of volumes to be used or a volume to be provisioned based on the storage class configuration. Note: It is highly recommended that the storage class used have a Reclaim Policy of RETAIN!! This is not enforced and not an issue if the volumes are managed correctly: When a user is done with their volume, they can delete the Persistent Volume Claim (PVC) objects from the API that allows reclamation of the resource. The Reclaim policy for a PersistentVolume tells the cluster what to do with the volume * after it has been released of its claim. Exercise caution when using the DELETE policy.* See the following sections for additional information: Deployment Containerization Limitations/Requirements Parent topic: Container administration 9.5","title":"Optional: Configuration Settings to Manage Digital Asset Management Media upload storage services"},{"location":"containerization/optional_digital_asset_management_storage_configuration_settings/#optional-configuration-settings-to-manage-digital-asset-management-media-upload-storage-services","text":"This section outlines optional configuration steps to tune Digital Asset Management storage services. Reference the HCL Digital Experience Deployment topic for the latest list of HCL Digital Experience 9.5 container files available for your implementation, and instructions to install to supported container platforms. As outlined in the steps to install and configure HCL Digital Asset Management, four files are provided for deployment: HCL Digital Asset Management (Operator) HCL Digital Experience Digital Asset Management (Docker image) HCL Digital Experience Digital Asset Management (Image processor) Postgres Persistence Layer There are two independent volume types used in the deployment and runtime of the Digital Asset Management: There is a volume per persistence layer, this volume is self-provisioned and defaults to the platform\u2019s default storage class and therefore the volume has not been made configurable. See the Sample Storage Class and Volume topic for additional information to set storage class and volume using sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. There is a Digital Asset Management Media storage upload folder that is related to the upload performance of assets inside the Digital Asset Management library. This folder is a read/write/many folder and therefore is shared by all instances. There are 2 main settings for the Digital Asset Management upload folder: dam.deploy.dam.storageclass (REQUIRED) dam.deploy.dam.volume (OPTIONAL) There are two choices to configure this volume: DX Administrator specifies volume: In this model, the DX administrator sets both the dam.deploy.dam.storageclass and dam.deploy.dam.volume before initial deployment, and therefore tells the deployment to use a specific storage class and a specific volume provisioned within that storage class. This option provides a bit more control to the DX Administrator. DX Administrator specifies only the dam.deploy.dam.storageclass , and allows either a volume from a pool of volumes to be used or a volume to be provisioned based on the storage class configuration. Note: It is highly recommended that the storage class used have a Reclaim Policy of RETAIN!! This is not enforced and not an issue if the volumes are managed correctly: When a user is done with their volume, they can delete the Persistent Volume Claim (PVC) objects from the API that allows reclamation of the resource. The Reclaim policy for a PersistentVolume tells the cluster what to do with the volume * after it has been released of its claim. Exercise caution when using the DELETE policy.* See the following sections for additional information: Deployment Containerization Limitations/Requirements Parent topic: Container administration 9.5","title":"Optional: Configuration Settings to Manage Digital Asset Management Media upload storage services"},{"location":"containerization/overview/","text":"Containerization overview Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment. Overview HCL introduced full support for containerization in Digital Experience (DX) 9.5. HCL Digital Experience users can deploy HCL Digital Experience images in Docker, Red Hat OpenShift 4.1 and higher, also Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE) for test, development, staging, and production environments. See the Digital Experience on containerized platforms topic for the latest information on supported platforms and container packages. Whether developing, testing, or running a full production environment, use of Docker images and containers are preferred for the ease of deploying applications, including the latest version of HCL Digital Experience. Deploy in a fraction of the time it takes compared to traditional deployment models. Please see the Containerization Limitations/Requirements for specific versions and Deployment sections of the documentation before you begin. Parent topic: Digital Experience on containerized platforms","title":"Containerization overview"},{"location":"containerization/overview/#containerization-overview","text":"Learn more about the containerization architecture, including the supported container platforms in deploying HCL Digital Experience images for your environment.","title":"Containerization overview"},{"location":"containerization/overview/#overview","text":"HCL introduced full support for containerization in Digital Experience (DX) 9.5. HCL Digital Experience users can deploy HCL Digital Experience images in Docker, Red Hat OpenShift 4.1 and higher, also Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE) for test, development, staging, and production environments. See the Digital Experience on containerized platforms topic for the latest information on supported platforms and container packages. Whether developing, testing, or running a full production environment, use of Docker images and containers are preferred for the ease of deploying applications, including the latest version of HCL Digital Experience. Deploy in a fraction of the time it takes compared to traditional deployment models. Please see the Containerization Limitations/Requirements for specific versions and Deployment sections of the documentation before you begin. Parent topic: Digital Experience on containerized platforms","title":"Overview"},{"location":"containerization/personalization/","text":"Personalization rules This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user. Export PZN rules Command description The pzn-export command is used to export the rules from the source server location specified by the user. dxclient pzn-export Help command This command shows the help information for pzn-export command usage: dxclient pzn-export -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443: -dxPort <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing: -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE'): -targetWorkspace <value> Note: The targetPath and targetWorkspace parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Log files from command execution can be found in the logs directory of the DXClient installation. Example: ``` dxclient pzn-export -dxProtocol -hostname -dxPort -dxUsername -dxPassword ``` The outputfile for pzn export is generated in the following path: store/outputFiles/pznrules Import PZN rules Command description The pzn-import command is used to import the rules into the target server. dxclient pzn-import Required files Rules file: This file should contain the configuration XML representation of all the currently selected personalization objects. Help command This command shows the help information for pzn-import command usage: dxclient pzn-import -h Command options Use this attribute to specify the protocol with which to connect to the DX server -dxProtocol <value> Use this attribute to specify the hostname of the target DX server -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443 -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the import file path that is required while executing the pzn import task -rulesFilePath <Absolute or relative path to import nodes file> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE') -targetWorkspace <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Notes: For Kubernetes environments, dxProtocol should be http , hostname should be localhost, dxPort should be 10039 as DXConnect doesn't support https due to SSL Handshake challenges at this time. The dxProtocol , hostname , dxPort , targetWorkspace , and targetPath parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient pzn-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxConnectPort <dxConnectPort> -rulesFilePath <rulesFilePath> Parent topic: DXClient Artifact Types","title":"Personalization rules"},{"location":"containerization/personalization/#personalization-rules","text":"This topic contains the commands that the administrators can use to export and import the personalization (PZN) rules from the source server to the target server as specified by the user.","title":"Personalization rules"},{"location":"containerization/personalization/#export-pzn-rules","text":"Command description The pzn-export command is used to export the rules from the source server location specified by the user. dxclient pzn-export Help command This command shows the help information for pzn-export command usage: dxclient pzn-export -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443: -dxPort <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing: -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE'): -targetWorkspace <value> Note: The targetPath and targetWorkspace parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Log files from command execution can be found in the logs directory of the DXClient installation. Example: ``` dxclient pzn-export -dxProtocol -hostname -dxPort -dxUsername -dxPassword ``` The outputfile for pzn export is generated in the following path: store/outputFiles/pznrules","title":"Export PZN rules"},{"location":"containerization/personalization/#import-pzn-rules","text":"Command description The pzn-import command is used to import the rules into the target server. dxclient pzn-import Required files Rules file: This file should contain the configuration XML representation of all the currently selected personalization objects. Help command This command shows the help information for pzn-import command usage: dxclient pzn-import -h Command options Use this attribute to specify the protocol with which to connect to the DX server -dxProtocol <value> Use this attribute to specify the hostname of the target DX server -hostname <value> Use this attribute to specify the port on which to connect to the DX server,for Kubernetes Environment dxPort is 443 -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the import file path that is required while executing the pzn import task -rulesFilePath <Absolute or relative path to import nodes file> Use this attribute to specify the location in the target workspace, which is the parent for the published nodes. The target path must exist before publishing -targetPath <value> Use this attribute to specify the name of the workspace containing the rules in DX server (default targetWorkspace is 'ROOTWORKSPACE') -targetWorkspace <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Notes: For Kubernetes environments, dxProtocol should be http , hostname should be localhost, dxPort should be 10039 as DXConnect doesn't support https due to SSL Handshake challenges at this time. The dxProtocol , hostname , dxPort , targetWorkspace , and targetPath parameters are optional. If the user does not pass the respective parameters, then the default values are taken. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient pzn-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxConnectPort <dxConnectPort> -rulesFilePath <rulesFilePath> Parent topic: DXClient Artifact Types","title":"Import PZN rules"},{"location":"containerization/portlets/","text":"Portlets This topic provides information about the deployment and undeployment of portlets. Deploy Portlets The deploy-portlet command is used to deploy one or more new or updated portlets from a source client or server environment to target HCL DX 9.5 CF19 or later server using a provided input XMLAccess file and deployable Portlet WAR file. Note: The synchronization mode of all nodes in a clustered DX environment must be consistently set for a newly deployed portlet to be automatically started; otherwise redeployment or a manual start is required. Required files XMLAccess file This xml file should contain the definition of the web application along with the details of the portlet(s) to be deployed. The web archive file path referred to in this file inside the URL element is ignored, but the URL element itself must exist as it is dynamically replaced when the command is executed. A sample XML file for deploying portlet(s) can be found in the samples directory of DXClient (samples/DeployPortlet.xml) or in DX server located in the following directory: PortalServer_root/doc/xml-samples/DeployPortlet.xml. Portlet Application web archive file This web archive .war file should contain the necessary portlet artifacts for deployment, as per the JSR 286 portlet standard. Refer to Importing WAR files Command dxclient deploy-portlet -xmlFile <path> -warFile <path> Help command This command shows the help document on the deploy-portlet command usage: dxclient deploy-portlet -h Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to XMLAccess input file> Use this attribute to specify the local path to the WAR file to be deployed: -warFile <Absolute or relative path to deployable war file> Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value will typically be the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Log files from command execution can be found in the logs directory of the DXClient installation. Undeploy portlets The undeploy-portlet command is used to undeploy the portlets in the target DX servers. Note: Undeploy-portlet command takes a backup of the XML file of the deployed portlet application and application (EAR) if user has given enableBackup as true. By default, enableBackup is set to true and placed in the store/outputFiles/portlets/backup/undeploy-portlet/ . In case, if the undeployed portlet is required again, then the user can restore the portlet WAR file from the downloaded portlet application EAR file along with the exported deployable portlet application XML file. Command description This command invokes the undeploy-portlet tool inside the DXClient. The undeploy-portlet dxtool uses the provided files and executes the undeploy portlet task. dxclient undeploy-portlet Help command This command shows the help information for undeploy-portlet command usage: dxclient undeploy-portlet -h Required files This file should contain the definition of the web application along with the undeploy portlet. dxclient undeploy-portlet -xmlFile <path> Command options Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/configwps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to take the backup of portlet application before undeploying it: -enableBackup <value> Commands required when enableBackup is set to true Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> The values that are passed through the command line override the default values. Example: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> Example usage when enableBackup is set to true: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -enableBackup true -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server profile> Parent topic: DXClient Artifact Types","title":"Portlets"},{"location":"containerization/portlets/#portlets","text":"This topic provides information about the deployment and undeployment of portlets.","title":"Portlets"},{"location":"containerization/portlets/#deploy-portlets","text":"The deploy-portlet command is used to deploy one or more new or updated portlets from a source client or server environment to target HCL DX 9.5 CF19 or later server using a provided input XMLAccess file and deployable Portlet WAR file. Note: The synchronization mode of all nodes in a clustered DX environment must be consistently set for a newly deployed portlet to be automatically started; otherwise redeployment or a manual start is required. Required files XMLAccess file This xml file should contain the definition of the web application along with the details of the portlet(s) to be deployed. The web archive file path referred to in this file inside the URL element is ignored, but the URL element itself must exist as it is dynamically replaced when the command is executed. A sample XML file for deploying portlet(s) can be found in the samples directory of DXClient (samples/DeployPortlet.xml) or in DX server located in the following directory: PortalServer_root/doc/xml-samples/DeployPortlet.xml. Portlet Application web archive file This web archive .war file should contain the necessary portlet artifacts for deployment, as per the JSR 286 portlet standard. Refer to Importing WAR files Command dxclient deploy-portlet -xmlFile <path> -warFile <path> Help command This command shows the help document on the deploy-portlet command usage: dxclient deploy-portlet -h Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to XMLAccess input file> Use this attribute to specify the local path to the WAR file to be deployed: -warFile <Absolute or relative path to deployable war file> Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value will typically be the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Log files from command execution can be found in the logs directory of the DXClient installation.","title":"Deploy Portlets"},{"location":"containerization/portlets/#undeploy-portlets","text":"The undeploy-portlet command is used to undeploy the portlets in the target DX servers. Note: Undeploy-portlet command takes a backup of the XML file of the deployed portlet application and application (EAR) if user has given enableBackup as true. By default, enableBackup is set to true and placed in the store/outputFiles/portlets/backup/undeploy-portlet/ . In case, if the undeployed portlet is required again, then the user can restore the portlet WAR file from the downloaded portlet application EAR file along with the exported deployable portlet application XML file. Command description This command invokes the undeploy-portlet tool inside the DXClient. The undeploy-portlet dxtool uses the provided files and executes the undeploy portlet task. dxclient undeploy-portlet Help command This command shows the help information for undeploy-portlet command usage: dxclient undeploy-portlet -h Required files This file should contain the definition of the web application along with the undeploy portlet. dxclient undeploy-portlet -xmlFile <path> Command options Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/configwps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to take the backup of portlet application before undeploying it: -enableBackup <value> Commands required when enableBackup is set to true Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> The values that are passed through the command line override the default values. Example: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> Example usage when enableBackup is set to true: dxclient undeploy-portlet -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -enableBackup true -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX core server profile> Parent topic: DXClient Artifact Types","title":"Undeploy portlets"},{"location":"containerization/resourceenvironments/","text":"Resource environment provider This topic describes the commands that are used to create, update, delete, and retrieve custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers. Resource environment commands Command description: The resource-env-provider command is used to create, update or delete a custom property from an existing Resource Environment Provider, and to export or import multiple resource environment providers. dxclient resource-env-provider Help command: This command shows the help information for resource-env-provider command usage: dxclient resource-env-provider -h Help command for creating the resource environment property: ``` dxclient resource-env-provider create-property -h ``` Help command for updating the resource environment property: dxclient resource-env-provider update-property -h Help command for deleting the resource environment property: dxclient resource-env-provider delete-property -h Help command for retrieving the resource environment property: dxclient resource-env-provider retrieve-property -h Help command for exporting the resource environment property: dxclient resource-env-provider export-properties -h Help command for importing the resource environment property: dxclient resource-env-provider import-properties -h Commands: Create a custom property from an existing resource environment: ``` resource-env-provider create-property [OPTIONS] ``` Update a custom property from an existing resource environment: resource-env-provider update-property [OPTIONS] Delete a custom property from an existing resource environment: resource-env-provider delete-property[OPTIONS] Retrieve a custom property from an existing resource environment provider: resource-env-provider retrieve-property [OPTIONS] Export all the existing resource environment providers: resource-env-provider export-properties [OPTIONS] Import all the existing resource environment providers provided in the input file containing the resource environment providers: resource-env-provider import-properties [OPTIONS] Command options required to create, update, delete, and retrieve resource environment providers: Use this attribute to specify the protocol with which to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change is only in case of Open Shift Kubernetes Environment, otherwise it is the same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the name of the Resource Environment Provider: -providerName <value> Use this attribute to specify the name of the Custom Property: -propertyName <value> Use this attribute to specify the value of the Custom Property: -propertyValue <value> Use this attribute to specify the description of the Custom Property: -propertyDesc <value> Command options required to export and import resource environment providers: Use this attribute to specify the configuration wizard home (route change is only in the case of Open Shift Kubernetes Environment, otherwise it is same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the username of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> For importing resource environment properties, use this attribute to specify the File path: -filePath <value> Example: For creating property: ``` dxclient resource-env-provider create-property -providerName -propertyName -propertyValue ``` For updating property: dxclient resource-env-provider update-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For deleting property: dxclient resource-env-provider delete-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For retrieving property: dxclient resource-env-provider retrieve-property -providerName <providerName> -propertyName <propertyName> For exporting property: dxclient resource-env-provider export-properties -dxProfileName <dxProfileName> For importing property: dxclient resource-env-provider import-properties -dxProfileName <dxProfileName> -filePath <filePath> Parent topic: DXClient Artifact Types","title":"Resource environment provider"},{"location":"containerization/resourceenvironments/#resource-environment-provider","text":"This topic describes the commands that are used to create, update, delete, and retrieve custom properties from an existing resource environment provider. It also provides the commands to export or import multiple resource environment providers.","title":"Resource environment provider"},{"location":"containerization/resourceenvironments/#resource-environment-commands","text":"Command description: The resource-env-provider command is used to create, update or delete a custom property from an existing Resource Environment Provider, and to export or import multiple resource environment providers. dxclient resource-env-provider Help command: This command shows the help information for resource-env-provider command usage: dxclient resource-env-provider -h Help command for creating the resource environment property: ``` dxclient resource-env-provider create-property -h ``` Help command for updating the resource environment property: dxclient resource-env-provider update-property -h Help command for deleting the resource environment property: dxclient resource-env-provider delete-property -h Help command for retrieving the resource environment property: dxclient resource-env-provider retrieve-property -h Help command for exporting the resource environment property: dxclient resource-env-provider export-properties -h Help command for importing the resource environment property: dxclient resource-env-provider import-properties -h Commands: Create a custom property from an existing resource environment: ``` resource-env-provider create-property [OPTIONS] ``` Update a custom property from an existing resource environment: resource-env-provider update-property [OPTIONS] Delete a custom property from an existing resource environment: resource-env-provider delete-property[OPTIONS] Retrieve a custom property from an existing resource environment provider: resource-env-provider retrieve-property [OPTIONS] Export all the existing resource environment providers: resource-env-provider export-properties [OPTIONS] Import all the existing resource environment providers provided in the input file containing the resource environment providers: resource-env-provider import-properties [OPTIONS] Command options required to create, update, delete, and retrieve resource environment providers: Use this attribute to specify the protocol with which to connect to the server: -dxProtocol <value> Use this attribute to specify the hostname of the target server: -hostname <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server: -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change is only in case of Open Shift Kubernetes Environment, otherwise it is the same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the name of the Resource Environment Provider: -providerName <value> Use this attribute to specify the name of the Custom Property: -propertyName <value> Use this attribute to specify the value of the Custom Property: -propertyValue <value> Use this attribute to specify the description of the Custom Property: -propertyDesc <value> Command options required to export and import resource environment providers: Use this attribute to specify the configuration wizard home (route change is only in the case of Open Shift Kubernetes Environment, otherwise it is same as hostname) that is required for authenticating to the cw_profile: -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile: -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile: -dxConnectPassword <value> Use this attribute to specify the username of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> For importing resource environment properties, use this attribute to specify the File path: -filePath <value> Example: For creating property: ``` dxclient resource-env-provider create-property -providerName -propertyName -propertyValue ``` For updating property: dxclient resource-env-provider update-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For deleting property: dxclient resource-env-provider delete-property -providerName <providerName> -propertyName <propertyName> -propertyValue <modifiedpropertyValue> For retrieving property: dxclient resource-env-provider retrieve-property -providerName <providerName> -propertyName <propertyName> For exporting property: dxclient resource-env-provider export-properties -dxProfileName <dxProfileName> For importing property: dxclient resource-env-provider import-properties -dxProfileName <dxProfileName> -filePath <filePath> Parent topic: DXClient Artifact Types","title":"Resource environment commands"},{"location":"containerization/run_core_config_engine/","text":"Running DX Core configuration tasks This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments. Running Core Config Engine tasks In an HCL Digital Experience 9.5 Container deployment using Helm , some DX 9.5 Core configuration tasks (such as change of context root) are now performed using the Helm upgrade route. However, others (such as database migration) are performed using a Config Engine task, as they would be in an on-premise or hybrid environment. Steps to run a Config Engine task: Open a shell on a DX Core pod. The following example instructions uses pod \u20180\u2019 as it should always be available: kubectl exec -it -n < namespace > dx-deployment-core-0 -c core -- /bin/bash Create the semaphore file. On the Core pod, use the following command to create a file which tells the Kubernetes probes that a Configuration task is in progress: touch /opt/app/configInProgress Run the Config Engine command. Follow the instructions for the particular configuration task that you need to perform. See DB Transfer Config Engine task for an example. Remove the semaphore file. On the Core pod, use the following command to delete the file which tells the Kubernetes probes that a configuration task is in progress: rm -f /opt/app/configInProgress You can now close the shell on the Core pod: exit (Optional) Restart other Core pods. If you have multiple Core pods running, and if the configuration task you just performed requires a server restart, you should now restart all the Core pods other than the one on which you ran the task. To do this, run the following command for each other Core pod. For example, if you have Core pods dx-deployment-core-0 , dx-deployment-core-1 , and dx-deployment-core-2 and performed the configuration task on pod 0, then run the following command below for pods dx-deployment-core-1 and dx-deployment-core-2 : kubectl delete pod -n < namespace > < pod-name > Note: To reduce the impact on availability, it is recommended that you wait for a pod to be ready again before running the command for the next pod. Parent topic: Update deployment to a later version","title":"Running DX Core configuration tasks"},{"location":"containerization/run_core_config_engine/#running-dx-core-configuration-tasks","text":"This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments.","title":"Running DX Core configuration tasks"},{"location":"containerization/run_core_config_engine/#running-core-config-engine-tasks","text":"In an HCL Digital Experience 9.5 Container deployment using Helm , some DX 9.5 Core configuration tasks (such as change of context root) are now performed using the Helm upgrade route. However, others (such as database migration) are performed using a Config Engine task, as they would be in an on-premise or hybrid environment. Steps to run a Config Engine task: Open a shell on a DX Core pod. The following example instructions uses pod \u20180\u2019 as it should always be available: kubectl exec -it -n < namespace > dx-deployment-core-0 -c core -- /bin/bash Create the semaphore file. On the Core pod, use the following command to create a file which tells the Kubernetes probes that a Configuration task is in progress: touch /opt/app/configInProgress Run the Config Engine command. Follow the instructions for the particular configuration task that you need to perform. See DB Transfer Config Engine task for an example. Remove the semaphore file. On the Core pod, use the following command to delete the file which tells the Kubernetes probes that a configuration task is in progress: rm -f /opt/app/configInProgress You can now close the shell on the Core pod: exit (Optional) Restart other Core pods. If you have multiple Core pods running, and if the configuration task you just performed requires a server restart, you should now restart all the Core pods other than the one on which you ran the task. To do this, run the following command for each other Core pod. For example, if you have Core pods dx-deployment-core-0 , dx-deployment-core-1 , and dx-deployment-core-2 and performed the configuration task on pod 0, then run the following command below for pods dx-deployment-core-1 and dx-deployment-core-2 : kubectl delete pod -n < namespace > < pod-name > Note: To reduce the impact on availability, it is recommended that you wait for a pod to be ready again before running the command for the next pod. Parent topic: Update deployment to a later version","title":"Running Core Config Engine tasks"},{"location":"containerization/sample_pipelines_docker_dxclient/","text":"Sample Pipelines for the DXClient Docker image file This sample shows how to pull DXClient docker image from the given artifactory, run the DXClient tool in the docker and then deploy a portlet, theme, and script application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes DEPLOY_PORTLET Select this to deploy the portlet DEPLOY_THEME Select this to deploy the theme DEPLOY_SCRIPT_APPLICATION Select this to deploy the script application AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run ARTIFACTORY_URL URL to DXClient docker image file Docker image will be pulled out from this URL REPO_PATH Repository path of the DXClient docker image Docker image will be pulled out from this folder IMAGE_TAG docker image tag Docker image will be pulled out using this image tag ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server http or https DX_PORT Port to connect to DX serve Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX config wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server config wizard profile XML_CONFIG_PATH URL path to the config servlet for xmlaccess Defaults to '/wps/config' CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ DX_PROFILE_NAME Profile name of the DX server DX_PROFILE_PATH Profile path of the DX server DX_SOAP_PORT Soap Port number of the DX server PORTLET_WAR_ARTIFACT_NAME Filename of WAR to deploy the portlet Required for deploy portlet PORTLET_XML_ARTIFACT_NAME Filename of xmlaccess script used to deploy the portlet Required for deploy portlet THEME_EAR_APPLICATION_NAME Application name for the EAR file to deploy the theme Required for EAR theme deployment THEME_EAR_ARTIFACT_NAME File name of EAR application to deploy theme Required for EAR theme deployment THEME_REGISTRATION_FILE File name of XML file to register the theme Required for theme registration THEME_NAME WEBDAV theme name Required for WEBDAV theme deployment THEME_ARTIFACT_NAME WEBDAV theme zip file name Required for WEBDAV theme deployment SCRIPT_APP_ARTIFACT_NAME Filename of compressed script application to deploy Required for deploy script application MAIN_HTML_FILE File name of Main HTML file within the script application Required for deploy script application WCM_SITE_AREA SiteArea of the Script Application content Required for deploy script application CONTENT_NAME Name of the Script Application instance to be created or updated Required for deploy script application Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Sample Pipelines for the DXClient Docker image file"},{"location":"containerization/sample_pipelines_docker_dxclient/#sample-pipelines-for-the-dxclient-docker-image-file","text":"This sample shows how to pull DXClient docker image from the given artifactory, run the DXClient tool in the docker and then deploy a portlet, theme, and script application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes DEPLOY_PORTLET Select this to deploy the portlet DEPLOY_THEME Select this to deploy the theme DEPLOY_SCRIPT_APPLICATION Select this to deploy the script application AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run ARTIFACTORY_URL URL to DXClient docker image file Docker image will be pulled out from this URL REPO_PATH Repository path of the DXClient docker image Docker image will be pulled out from this folder IMAGE_TAG docker image tag Docker image will be pulled out using this image tag ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server http or https DX_PORT Port to connect to DX serve Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX config wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server config wizard profile XML_CONFIG_PATH URL path to the config servlet for xmlaccess Defaults to '/wps/config' CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ DX_PROFILE_NAME Profile name of the DX server DX_PROFILE_PATH Profile path of the DX server DX_SOAP_PORT Soap Port number of the DX server PORTLET_WAR_ARTIFACT_NAME Filename of WAR to deploy the portlet Required for deploy portlet PORTLET_XML_ARTIFACT_NAME Filename of xmlaccess script used to deploy the portlet Required for deploy portlet THEME_EAR_APPLICATION_NAME Application name for the EAR file to deploy the theme Required for EAR theme deployment THEME_EAR_ARTIFACT_NAME File name of EAR application to deploy theme Required for EAR theme deployment THEME_REGISTRATION_FILE File name of XML file to register the theme Required for theme registration THEME_NAME WEBDAV theme name Required for WEBDAV theme deployment THEME_ARTIFACT_NAME WEBDAV theme zip file name Required for WEBDAV theme deployment SCRIPT_APP_ARTIFACT_NAME Filename of compressed script application to deploy Required for deploy script application MAIN_HTML_FILE File name of Main HTML file within the script application Required for deploy script application WCM_SITE_AREA SiteArea of the Script Application content Required for deploy script application CONTENT_NAME Name of the Script Application instance to be created or updated Required for deploy script application Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Sample Pipelines for the DXClient Docker image file"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/","text":"Sample Pipelines for use with HCL DXClient and Automation servers A CI/CD pipeline can help automate processes in the development and test cycle, including deploying code to test and production environments. HCL DX 9.5 provides sample pipelines for use with the DXClient tooling to demonstrate how the deployment of portlets, Script Applications, Themes, DX Application, export and import of WCM libraries, etc., can be automated. Note: The sample pipelines referred in this topic are supported only until the HCL Digital Experience Container update version CF195. We recommend that you refer to the following sample pipelines for releases CF196 and later. Sample Pipelines for the DXClient Docker image file Sample Pipelines for the DXClient node package file Sample Pipeline details Sample Pipelines are provided for deploying or updating a portlet application, Script Application, or Themes. The sample pipelines are made available under the samples folder in the DXClient root folder. Developers and administrators can use these pipelines as a basis for Jenkins automation server jobs. Deploy portlet This sample shows how to install the DXClient tool in a pipeline and then deploy or update a portlet. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTEFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl WAR_ARTEFACT_NAME Filename of WAR to deploy XMLACCESS_ARTEFACT_NAME Filename of XMLAccess script used to deploy ARTEFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Hostname or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile XML_CONFIG_PATH URL path to the Configuration servlet for XMLAccess Defaults to /wps/config' DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server Configuration Wizard profile Deploy script application This sample shows how to install the DXClient tool in a pipeline and then deploy a Script Application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_NAME Filename of zipped script application to deploy ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server WCMSITEAREA SiteArea of the Script Application content CONTENTNAME Name of the Script Application instance to be created or updated Restore script application This sample shows how to install the DXClient tool in a pipeline and then restore an existing script application to any of its specified previous version. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server CONTENT_ID WCM content item unique ID Script application will be stored in this content item VERSION_NAME Version name of script application Name of the version that should store the script application RESTORE_AS_PUBLISHED Selected version to restore or draft Restore as a draft or replace the published version Deploy DX application This sample shows how to install the DXClient tool in a pipeline and then deploy or update a DX application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Server Configuration Wizard profile DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PORT Port to connect to DX server Port for the DX main profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 APPLICATION_FILE File name of EAR application to deploy Required for EAR deployment APPLICATION_NAME Application name for the EAR file to deploy Required for EAR deployment DX_SOAP_PORT Soap Port number of the DX server Required for EAR deployment DX_PROFILE_PATH Profile path of the DX server Required for EAR deployment Restart DX Core server This sample shows how to restart the DX Core server using the DXClient tool in a pipeline. It is designed to be run from a Jenkins job that requires the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Core server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Core Server Configuration Wizard profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 DX_PROFILE_PATH Profile path of the DX Core server Required for restarting the DX Core server Manage syndication This sample shows how to install the DXClient tool in a pipeline and then enable or disable the syndicator or subscriber. It is designed to be run from a Jenkins job that requires the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PORT Port to connect to DX server Port for the DX main profile CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ SYNDICATOR_OR_SUBSCRIBER This can be syndicator or subscriber UUID_FOR_SYNDICATION UUID of the syndicator/subscriber instance ENABLE Use true or false to enable or disable the syndicator/subscriber Deploy theme This sample shows how to install the DXClient tool in a pipeline and then deploy or update a theme. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTEFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTEFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server Configuration Wizard profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 APPLICATION_FILE File name of EAR application to deploy Required for EAR deployment APPLICATION_NAME Application name for the EAR file to deploy Required for EAR deployment DX_SOAP_PORT Soap Port number of the DX server Required for EAR deployment DX_PROFILE_PATH Profile path of the DX server Required for EAR deployment XML_FILE File name of XML file to register the theme Required for theme registration XML_CONFIG_PATH URL path to the Configuration endpoint Defaults to /wps/config' THEME_NAME WebDAV theme name Required for WebDAV deployment THEME_PATH WebDAV theme zip file Required for WebDAV deployment CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Sample Pipelines for use with HCL DXClient and Automation servers"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#sample-pipelines-for-use-with-hcl-dxclient-and-automation-servers","text":"A CI/CD pipeline can help automate processes in the development and test cycle, including deploying code to test and production environments. HCL DX 9.5 provides sample pipelines for use with the DXClient tooling to demonstrate how the deployment of portlets, Script Applications, Themes, DX Application, export and import of WCM libraries, etc., can be automated. Note: The sample pipelines referred in this topic are supported only until the HCL Digital Experience Container update version CF195. We recommend that you refer to the following sample pipelines for releases CF196 and later. Sample Pipelines for the DXClient Docker image file Sample Pipelines for the DXClient node package file","title":"Sample Pipelines for use with HCL DXClient and Automation servers"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#sample-pipeline-details","text":"Sample Pipelines are provided for deploying or updating a portlet application, Script Application, or Themes. The sample pipelines are made available under the samples folder in the DXClient root folder. Developers and administrators can use these pipelines as a basis for Jenkins automation server jobs.","title":"Sample Pipeline details"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#deploy-portlet","text":"This sample shows how to install the DXClient tool in a pipeline and then deploy or update a portlet. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTEFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl WAR_ARTEFACT_NAME Filename of WAR to deploy XMLACCESS_ARTEFACT_NAME Filename of XMLAccess script used to deploy ARTEFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Hostname or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile XML_CONFIG_PATH URL path to the Configuration servlet for XMLAccess Defaults to /wps/config' DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server Configuration Wizard profile","title":"Deploy portlet"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#deploy-script-application","text":"This sample shows how to install the DXClient tool in a pipeline and then deploy a Script Application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_NAME Filename of zipped script application to deploy ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server WCMSITEAREA SiteArea of the Script Application content CONTENTNAME Name of the Script Application instance to be created or updated","title":"Deploy script application"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#restore-script-application","text":"This sample shows how to install the DXClient tool in a pipeline and then restore an existing script application to any of its specified previous version. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server CONTENT_ID WCM content item unique ID Script application will be stored in this content item VERSION_NAME Version name of script application Name of the version that should store the script application RESTORE_AS_PUBLISHED Selected version to restore or draft Restore as a draft or replace the published version","title":"Restore script application"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#deploy-dx-application","text":"This sample shows how to install the DXClient tool in a pipeline and then deploy or update a DX application. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Server Configuration Wizard profile DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PORT Port to connect to DX server Port for the DX main profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 APPLICATION_FILE File name of EAR application to deploy Required for EAR deployment APPLICATION_NAME Application name for the EAR file to deploy Required for EAR deployment DX_SOAP_PORT Soap Port number of the DX server Required for EAR deployment DX_PROFILE_PATH Profile path of the DX server Required for EAR deployment","title":"Deploy DX application"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#restart-dx-core-server","text":"This sample shows how to restart the DX Core server using the DXClient tool in a pipeline. It is designed to be run from a Jenkins job that requires the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Core server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX Core Server Configuration Wizard profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 DX_PROFILE_PATH Profile path of the DX Core server Required for restarting the DX Core server","title":"Restart DX Core server"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#manage-syndication","text":"This sample shows how to install the DXClient tool in a pipeline and then enable or disable the syndicator or subscriber. It is designed to be run from a Jenkins job that requires the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PORT Port to connect to DX server Port for the DX main profile CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ SYNDICATOR_OR_SUBSCRIBER This can be syndicator or subscriber UUID_FOR_SYNDICATION UUID of the syndicator/subscriber instance ENABLE Use true or false to enable or disable the syndicator/subscriber","title":"Manage syndication"},{"location":"containerization/sample_pipelines_for_use_with_dx_client_and_automation_servers/#deploy-theme","text":"This sample shows how to install the DXClient tool in a pipeline and then deploy or update a theme. It is designed to be run from a Jenkins job that provides the following parameters: Parameter Value Notes AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access tool package URL ARTEFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTEFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server HTTP or HTTPS DX_PORT Port to connect to DX server Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name/password credentials needed to access DX server DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server Configuration Wizard profile DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX Configuration Wizard profile. Default to 10202 APPLICATION_FILE File name of EAR application to deploy Required for EAR deployment APPLICATION_NAME Application name for the EAR file to deploy Required for EAR deployment DX_SOAP_PORT Soap Port number of the DX server Required for EAR deployment DX_PROFILE_PATH Profile path of the DX server Required for EAR deployment XML_FILE File name of XML file to register the theme Required for theme registration XML_CONFIG_PATH URL path to the Configuration endpoint Defaults to /wps/config' THEME_NAME WebDAV theme name Required for WebDAV deployment THEME_PATH WebDAV theme zip file Required for WebDAV deployment CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Deploy theme"},{"location":"containerization/sample_pipelines_node_dxclient/","text":"Sample Pipelines for the DXClient node package file This topic describes how to install the DXClient tool in a pipeline, by providing a few sample pipelines such as pipelines for deploying a portlet, theme, and script application. It is designed to be run from a Jenkins job with the following parameters: Parameter Value Notes DEPLOY_PORTLET Select this to deploy the portlet DEPLOY_THEME Select this to deploy the theme DEPLOY_SCRIPT_APPLICATION Select this to deploy the script application AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server http or https DX_PORT Port to connect to DX serve Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX config wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server config wizard profile XML_CONFIG_PATH URL path to the config servlet for xmlaccess Defaults to '/wps/config' CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ DX_PROFILE_NAME Profile name of the DX server DX_PROFILE_PATH Profile path of the DX server DX_SOAP_PORT Soap Port number of the DX server PORTLET_WAR_ARTIFACT_NAME Filename of WAR to deploy the portlet Required for deploy portlet PORTLET_XML_ARTIFACT_NAME Filename of xmlaccess script used to deploy the portlet Required for deploy portlet THEME_EAR_APPLICATION_NAME Application name for the EAR file to deploy the theme Required for EAR theme deployment THEME_EAR_ARTIFACT_NAME File name of EAR application to deploy theme Required for EAR theme deployment THEME_REGISTRATION_FILE File name of XML file to register the theme Required for theme registration THEME_NAME WEBDAV theme name Required for WEBDAV theme deployment THEME_ARTIFACT_NAME WEBDAV theme zip file name Required for WEBDAV theme deployment SCRIPT_APP_ARTIFACT_NAME Filename of zipped script application to deploy Required for deploy script application MAIN_HTML_FILE File name of Main HTML file within the script application Required for deploy script application WCM_SITE_AREA SiteArea of the Script Application content Required for deploy script application CONTENT_NAME Name of the Script Application instance to be created or updated Required for deploy script application Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Sample Pipelines for the DXClient node package file"},{"location":"containerization/sample_pipelines_node_dxclient/#sample-pipelines-for-the-dxclient-node-package-file","text":"This topic describes how to install the DXClient tool in a pipeline, by providing a few sample pipelines such as pipelines for deploying a portlet, theme, and script application. It is designed to be run from a Jenkins job with the following parameters: Parameter Value Notes DEPLOY_PORTLET Select this to deploy the portlet DEPLOY_THEME Select this to deploy the theme DEPLOY_SCRIPT_APPLICATION Select this to deploy the script application AGENT_LABEL Jenkins agent label Determines on which agents the pipeline can run TOOL_PACKAGE_URL URL to DXClient zip Fetched via curl TOOL_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access tool package URL ARTIFACT_PATH URL (except filenames) for artifacts to be deployed Artifacts fetched via curl ARTIFACT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access artifact URLs DX_HOST Host name or IP address of DX server Artifacts will be deployed to this server DX_PROTOCOL Protocol to connect to DX server http or https DX_PORT Port to connect to DX serve Port for the DX main profile DX_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server DXCONNECT_HOST Host name or IP address of the DXConnect servlet (route change only in case of Open Shift Kubernetes Environment. For other case, this parameter should be same as DX_HOST) Hostname for the DX Configuration Wizard profile DXCONNECT_PORT Port to connect to DXConnect servlet Port for the DX config wizard profile DXCONNECT_CREDENTIALS_ID Credentials ID in Jenkins store User name / password credentials needed to access DX server config wizard profile XML_CONFIG_PATH URL path to the config servlet for xmlaccess Defaults to '/wps/config' CONTENT_HANDLER_PATH Alternate path for the portal context root or the content handler servlet Default to /wps/mycontenthandler/ DX_PROFILE_NAME Profile name of the DX server DX_PROFILE_PATH Profile path of the DX server DX_SOAP_PORT Soap Port number of the DX server PORTLET_WAR_ARTIFACT_NAME Filename of WAR to deploy the portlet Required for deploy portlet PORTLET_XML_ARTIFACT_NAME Filename of xmlaccess script used to deploy the portlet Required for deploy portlet THEME_EAR_APPLICATION_NAME Application name for the EAR file to deploy the theme Required for EAR theme deployment THEME_EAR_ARTIFACT_NAME File name of EAR application to deploy theme Required for EAR theme deployment THEME_REGISTRATION_FILE File name of XML file to register the theme Required for theme registration THEME_NAME WEBDAV theme name Required for WEBDAV theme deployment THEME_ARTIFACT_NAME WEBDAV theme zip file name Required for WEBDAV theme deployment SCRIPT_APP_ARTIFACT_NAME Filename of zipped script application to deploy Required for deploy script application MAIN_HTML_FILE File name of Main HTML file within the script application Required for deploy script application WCM_SITE_AREA SiteArea of the Script Application content Required for deploy script application CONTENT_NAME Name of the Script Application instance to be created or updated Required for deploy script application Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Sample Pipelines for the DXClient node package file"},{"location":"containerization/sample_storage_class_volume/","text":"Sample storage class and volume for HCL Digital Experience 9.5 containers Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. It is recommended to set a separate storage class and volume for production, especially if you have more than one project in the Kubernetes (Amazon EKS or Red Hat OpenShift) environment. This is a good practice because it prevents projects from overlapping storage volumes. See video: Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Follow these steps to create a new persistent volume and storage class, in either Amazon EKS or OpenShift. Use and save the following as your storage class file: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Use and save the following as your storage volume file: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Copy both files to your local file system. Change at least the server and path in your sample volume .yaml file to an appropriate NFS server and volume. To create the storage class, run the following command: kubectl apply -f subclass.yaml To create the storage volume, run the following command: kubectl apply -f SampleZVolume.yaml Continue with deployment. Note: In these examples, NFS volumes have been used. You can use the following sample yaml to create the volume in Amazon EKS OpenShift with the corrected values: nfs: \u2028 server: your_nfs_server.com \u2028 path: /exports/volume_name Parent topic: Customizing your container deployment","title":"Sample\u00a0storage class and volume for HCL Digital Experience 9.5 containers"},{"location":"containerization/sample_storage_class_volume/#sample-storage-class-and-volume-for-hcl-digital-experience-95-containers","text":"Learn how to set storage class and volume using a sample storage class and volume scripts for HCL Digital Experience 9.5 CF171 and higher container releases deployed to Amazon Elastic Container Service (EKS) or Red Hat OpenShift environment. It is recommended to set a separate storage class and volume for production, especially if you have more than one project in the Kubernetes (Amazon EKS or Red Hat OpenShift) environment. This is a good practice because it prevents projects from overlapping storage volumes. See video: Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Follow these steps to create a new persistent volume and storage class, in either Amazon EKS or OpenShift. Use and save the following as your storage class file: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Use and save the following as your storage volume file: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Copy both files to your local file system. Change at least the server and path in your sample volume .yaml file to an appropriate NFS server and volume. To create the storage class, run the following command: kubectl apply -f subclass.yaml To create the storage volume, run the following command: kubectl apply -f SampleZVolume.yaml Continue with deployment. Note: In these examples, NFS volumes have been used. You can use the following sample yaml to create the volume in Amazon EKS OpenShift with the corrected values: nfs: \u2028 server: your_nfs_server.com \u2028 path: /exports/volume_name Parent topic: Customizing your container deployment","title":"Sample\u00a0storage class and volume for HCL Digital Experience 9.5 containers"},{"location":"containerization/scriptapplications/","text":"Script applications This topic provides information about the deployment, undeployment, and restoration of script applications. Deploy script applications The deploy-scriptapplication command is used with the DXClient tool to push or pull Script Applications between a local development workstation or automation server and DX 9.5 CF19 or later servers. The command will push or pull the files that make up a script application to or from a Script Application instance stored in a Web Content Manager library on the server. Required Files : The script application push command in the DXClient tool requires a Script Application zip file or an extracted folder of the same (identified by the prebuiltZip or contentRoot attributes respectively). For more information on Script Applications, refer to the Script Application topics in the HCL DX Help Center. Command This command invokes the deploy-scriptapplication command inside the DXClient tool to either push or pull a script application: dxclient deploy-scriptapplication Subcommands Use this command to create or update the content of a Script Application on the HCL DX server: push [options] Use this command to download the content of a Script Application from the HCL DX server: pull [options] Help command This command shows the help document on the deploy-scriptapplication command: dxclient deploy-scriptapplication pull -h dxclient deploy-scriptapplication push -h Options for the pull subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example: /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Options for the push subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that should receive the Script Application instance being pushed, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the SiteArea containing the Script Application content item: -wcmSiteArea <value> Use this attribute to specify the name of the Script Application content item to be created or updated: -wcmContentName <value> Use this attribute to specify the full WCM path of the Script Application content item to be created or updated: -wcmContentPath <value> Use this attribute to set or update the title of the Script Application content item: -wcmContentTitle <value> Use this attribute to specify the main HTML file name that is present within the Script Application: -mainHtmlFile <value> Use this attribute to specify the absolute or relative path to the Script Application's content as a ZIP file: -prebuiltZip <value> Use this attribute to specify the absolute or relative path to the Script Application's content in a directory: contentRoot <value> Notes: At least one of (a) wcmContentId , (b) wcmContentPath or (c) both wcmContentName and wcmSiteArea must be specified. If multiple options are provided, then the priority order goes as follows: (a), then (b), and then (c). Use wcmContentId only if you are updating an existing Script Application instance - for new Script Applications specify either (a) wcmContentPath or (b) both wcmContentName and wcmSiteArea . mainHtmlFile is mandatory. The outputfile for pull will be generated inside store/outputFiles/sp-pull-output. When prebuiltZip is specified, the main HTML file path must be relative to the top-level directory in the compressed file. Command options passed through the command line will override values set in the config.json file. Example: For Script Application Pull: dxclient deploy-scriptapplication pull -wcmContentId <wcmContentId> If all required options are configured in config.json of the DX Client tool, then execute: dxclient deploy-scriptapplication pull For Script Application Push, if the Script Application is extracted to a folder named temp at the root of the DXClient machine: dxclient deploy-scriptapplication push -contentRoot /temp -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If the Script Application is available as a .zip file in a folder named temp on the DXClient tool location, execute: dxclient deploy-scriptapplication push -prebuiltZip /temp/DemoScriptApplication.zip -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If all required options are configured in the config.json at the /dist/src/configuration path of the DXClient tool, then execute: dxclient deploy-scriptapplication push Undeploy script applications The undeploy-scriptapplication command is used to remove a script application from a target HCL DX 9.5 CF192 or later servers. Required file This command invokes the undeploy-scriptapplication tool inside the DXClient. The undeploy-scriptapplication dxtool uses the provided files and execute the undeploy scriptapplication task. Command dxclient undeploy-scriptapplication -wcmContentId <value> Help command This command shows the help information for undeploy-scriptapplication command usage: dxclient undeploy-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this tag to forcefully delete the Script Application. -f Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> -f Restore Script Application The restore-scriptapplication command is used to restore a script application into one of its previous versions present in the target HCL DX 9.5 CF 19 or later servers. Required file This command invokes the restore-scriptapplication tool inside the DXClient. The restore-scriptapplication dxtool uses the provided files and execute the restore scriptapplication task. Command dxclient restore-scriptapplication -wcmContentId <value> -versionName <version-name> Help command This command shows the help information for restore-scriptapplication command usage: dxclient restore-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the versionName for the Script Application. -versionName <value> Use this attribute to specify the restore as a draft or replace the published version of Script Application. -restoreAsPublished <value> Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient restore-scriptapplication -wcmContentID <wcm-content-id> -versionName <version-name> -restoreAsPublished <restore-as-published> Parent topic: DXClient Artifact Types","title":"Script applications"},{"location":"containerization/scriptapplications/#script-applications","text":"This topic provides information about the deployment, undeployment, and restoration of script applications.","title":"Script applications"},{"location":"containerization/scriptapplications/#deploy-script-applications","text":"The deploy-scriptapplication command is used with the DXClient tool to push or pull Script Applications between a local development workstation or automation server and DX 9.5 CF19 or later servers. The command will push or pull the files that make up a script application to or from a Script Application instance stored in a Web Content Manager library on the server. Required Files : The script application push command in the DXClient tool requires a Script Application zip file or an extracted folder of the same (identified by the prebuiltZip or contentRoot attributes respectively). For more information on Script Applications, refer to the Script Application topics in the HCL DX Help Center. Command This command invokes the deploy-scriptapplication command inside the DXClient tool to either push or pull a script application: dxclient deploy-scriptapplication Subcommands Use this command to create or update the content of a Script Application on the HCL DX server: push [options] Use this command to download the content of a Script Application from the HCL DX server: pull [options] Help command This command shows the help document on the deploy-scriptapplication command: dxclient deploy-scriptapplication pull -h dxclient deploy-scriptapplication push -h Options for the pull subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example: /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Options for the push subcommand Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that should receive the Script Application instance being pushed, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the SiteArea containing the Script Application content item: -wcmSiteArea <value> Use this attribute to specify the name of the Script Application content item to be created or updated: -wcmContentName <value> Use this attribute to specify the full WCM path of the Script Application content item to be created or updated: -wcmContentPath <value> Use this attribute to set or update the title of the Script Application content item: -wcmContentTitle <value> Use this attribute to specify the main HTML file name that is present within the Script Application: -mainHtmlFile <value> Use this attribute to specify the absolute or relative path to the Script Application's content as a ZIP file: -prebuiltZip <value> Use this attribute to specify the absolute or relative path to the Script Application's content in a directory: contentRoot <value> Notes: At least one of (a) wcmContentId , (b) wcmContentPath or (c) both wcmContentName and wcmSiteArea must be specified. If multiple options are provided, then the priority order goes as follows: (a), then (b), and then (c). Use wcmContentId only if you are updating an existing Script Application instance - for new Script Applications specify either (a) wcmContentPath or (b) both wcmContentName and wcmSiteArea . mainHtmlFile is mandatory. The outputfile for pull will be generated inside store/outputFiles/sp-pull-output. When prebuiltZip is specified, the main HTML file path must be relative to the top-level directory in the compressed file. Command options passed through the command line will override values set in the config.json file. Example: For Script Application Pull: dxclient deploy-scriptapplication pull -wcmContentId <wcmContentId> If all required options are configured in config.json of the DX Client tool, then execute: dxclient deploy-scriptapplication pull For Script Application Push, if the Script Application is extracted to a folder named temp at the root of the DXClient machine: dxclient deploy-scriptapplication push -contentRoot /temp -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If the Script Application is available as a .zip file in a folder named temp on the DXClient tool location, execute: dxclient deploy-scriptapplication push -prebuiltZip /temp/DemoScriptApplication.zip -wcmSiteArea \"Script Application Library/Script Applications/\" -wcmContentName DemoScriptApplication If all required options are configured in the config.json at the /dist/src/configuration path of the DXClient tool, then execute: dxclient deploy-scriptapplication push","title":"Deploy script applications"},{"location":"containerization/scriptapplications/#undeploy-script-applications","text":"The undeploy-scriptapplication command is used to remove a script application from a target HCL DX 9.5 CF192 or later servers. Required file This command invokes the undeploy-scriptapplication tool inside the DXClient. The undeploy-scriptapplication dxtool uses the provided files and execute the undeploy scriptapplication task. Command dxclient undeploy-scriptapplication -wcmContentId <value> Help command This command shows the help information for undeploy-scriptapplication command usage: dxclient undeploy-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this tag to forcefully delete the Script Application. -f Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> dxclient undeploy-scriptapplication -wcmContentId <wcm-content-id> -f","title":"Undeploy script applications"},{"location":"containerization/scriptapplications/#restore-script-application","text":"The restore-scriptapplication command is used to restore a script application into one of its previous versions present in the target HCL DX 9.5 CF 19 or later servers. Required file This command invokes the restore-scriptapplication tool inside the DXClient. The restore-scriptapplication dxtool uses the provided files and execute the restore scriptapplication task. Command dxclient restore-scriptapplication -wcmContentId <value> -versionName <version-name> Help command This command shows the help information for restore-scriptapplication command usage: dxclient restore-scriptapplication -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server: -dxPort <value> Use this attribute to specify the path to the content handler servlet on DX server (example, /wps/mycontenthandler ): -contenthandlerPath <value> Use this attribute to specify the context of the virtual portal that contains the Script Application instance that you want to retrieve, if any: -virtualPortalContext <value> Use this attribute to specify the context of the portal project that manages the publication of changes to the Script Application instance, if any: -projectContext <value> Use this attribute to specify the username to authenticate with the DX server: -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the WCM content ID of the Script Application content item: -wcmContentId <value> Use this attribute to specify the versionName for the Script Application. -versionName <value> Use this attribute to specify the restore as a draft or replace the published version of Script Application. -restoreAsPublished <value> Command options passed through the command line will override values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient restore-scriptapplication -wcmContentID <wcm-content-id> -versionName <version-name> -restoreAsPublished <restore-as-published> Parent topic: DXClient Artifact Types","title":"Restore Script Application"},{"location":"containerization/sharedlibrary/","text":"Shared library Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others. Shared library The shared-library command is used to manage the jar files in the provided default shared library location. Default shared Library: DXCLib Default shared library location: <dx-server-profile>/PortalServer/sharedLibrary Note: For Shared Library artifact, the DX Server needs to be at HCL DX 9.5 CF196 or higher. The default shared library DXCLib is already configured and associated to application server. The shared-library command uses two sub-commands upload and delete to manage files in the DX server. The sub-command upload is used to upload jar files and sub-command delete is used to delete the files from the default shared library location provided below. Command Description This command invokes the shared library upload task inside the DXClient. This is used to upload jar files into the default shared library location. dxclient shared-library upload This command invokes the shared library delete task inside the DXClient. This is used to delete jar files from the default shared library location. dxclient shared-library delete Help command This command shows the help information for shared-library upload command usage: dxclient shared-library upload -h This command shows the help information for shared-library delete command usage: dxclient shared-library delete -h Common Command options Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile. -dxConnectPassword <value> Use this attribute to specify the profile name of the DX Core -dxProfileName <Name of the DX server profile> Command option for upload Use this attribute to specify the path to a jar/zip file or folder containing jars in it. -libFilePath <value> Command option for delete Use this attribute to specify the names of the jar files present in the shared library location on the server. -libFileNames <value> Note: For upload, the folder or zip file should contain only jars files that are to be uploaded to the default shared library location. Example: Use this attribute to specify the path to a jar/zip file or folder containing jars in it. ``` dxclient shared-library upload -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFilePath dxclient shared-library delete -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFileNames -libFilePath ``` Parent topic: DXClient Artifact Types","title":"Shared library"},{"location":"containerization/sharedlibrary/#shared-library","text":"Shared libraries are jar files representing code that is shared across multiple components of the customer, for example, portlets, themes, preprocessors, and others.","title":"Shared library"},{"location":"containerization/sharedlibrary/#shared-library_1","text":"The shared-library command is used to manage the jar files in the provided default shared library location. Default shared Library: DXCLib Default shared library location: <dx-server-profile>/PortalServer/sharedLibrary Note: For Shared Library artifact, the DX Server needs to be at HCL DX 9.5 CF196 or higher. The default shared library DXCLib is already configured and associated to application server. The shared-library command uses two sub-commands upload and delete to manage files in the DX server. The sub-command upload is used to upload jar files and sub-command delete is used to delete the files from the default shared library location provided below. Command Description This command invokes the shared library upload task inside the DXClient. This is used to upload jar files into the default shared library location. dxclient shared-library upload This command invokes the shared library delete task inside the DXClient. This is used to delete jar files from the default shared library location. dxclient shared-library delete Help command This command shows the help information for shared-library upload command usage: dxclient shared-library upload -h This command shows the help information for shared-library delete command usage: dxclient shared-library delete -h Common Command options Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile. -dxConnectPassword <value> Use this attribute to specify the profile name of the DX Core -dxProfileName <Name of the DX server profile> Command option for upload Use this attribute to specify the path to a jar/zip file or folder containing jars in it. -libFilePath <value> Command option for delete Use this attribute to specify the names of the jar files present in the shared library location on the server. -libFileNames <value> Note: For upload, the folder or zip file should contain only jars files that are to be uploaded to the default shared library location. Example: Use this attribute to specify the path to a jar/zip file or folder containing jars in it. ``` dxclient shared-library upload -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFilePath dxclient shared-library delete -dxUsername -dxPassword -dxConnectHostname -dxConnectPort -dxConnectUsername -dxConnectPassword -dxProfileName -libFileNames -libFilePath ``` Parent topic: DXClient Artifact Types","title":"Shared library"},{"location":"containerization/syndicatorsandsubscribers/","text":"Managing Web Content Syndicators and Subscribers using DXClient The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers . Managing syndicators The manage-syndicator command is used to enable or disable the syndicator using the provided input. Command description This command invokes the manage-syndicator tool inside the DXClient. It is used to enable or disable the syndicator. dxclient manage-syndicator Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-syndicator -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (example: /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance. -UUID <value> Use true or false to enable or disable the syndicator. -enable <value> The options that are passed through the command line override the default values. Example: dxclient manage-syndicator -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable> Manage-syndicator get-syndication-report The manage-syndicator get-syndication-report command is used to fetch the failed reports of the syndicator. Command description This command invokes the syndicator-faileditems tool inside the DXClient, which is used to fetch the failed reports. dxclient manage-syndicator get-syndication-report Help command This command shows the help document on the manage-syndicator get-syndication-report command usage: dxclient manage-syndicator get-syndication-report -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance -UUID <value> The options that are passed through the command line override the default values. Example usage with UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> Example usage without UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> Note: If UUID of a syndicator is specified, then the command provides the report for only the particular syndicator that is present in the target DX Server; otherwise, it provides the failure report for all syndicators. Managing subscribers The manage-subscriber command is used to enable or disable the subscriber using the provided input. Command description This command invokes the manage-subscriber tool inside the DXClient. It is used to enable/disable the subscriber. dxclient manage-subscriber Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-subscriber -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443). -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the subscriber instance. -UUID <value> Use this attribute to specify the enable or disable the subscriber instance. Use true or false to enable or disable the subscriber. -enable <value> The options passed through command line overrides the default values. Example: dxclient manage-subscriber -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable> Create Syndication Relation The create-syndication-relation command is used to create the syndication relation between syndicator and subscriber in the DX server. Command description Use the create-syndication-relation to create syndication relation: dxclient create-syndication-relation Help command This command shows the help information for create-syndication-relation command usage: dxclient create-syndication-relation -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> The path to the contenthandler servlet on the Script Application server: -contenthandlerPath <value> Syndicator URL of target server, for example, http(s)://host:port/wps/wcm: -syndicatorUrl <value> Use this attribute to specify the new syndicator name: -syndicatorName <value> Use this attribute to specify the new subscriber name: -subscriberName <value> Use this attribute to specify the Credential Vault Name of source server: -vaultSlotName <value> Whether the syndicator/subscriber pair is enabled on creation: isEnabled (default is true): -isEnabled <value> Whether the syndicator/subscriber pair is updateAfterCreation : updateAfterCreation (default is true): -updateAfterCreation <value> The libraries to syndicate eg. all-items,liveItems,liveProjectsItem,all-items,published-items and all-items-and-versions: -syndicationType <value> Use this attribute to specify the Libraries Name of target Server: -webContentLibraries <value> Use this attribute to specify the Subscriber URL, for example, http(s)://host:port/wps/wcm: -subscriberURL <value> Use this attribute to specify the Syndicator/subscriber mode: -mode <value> Use this attribute to specify the Credential Vault Name: -syndicatorVaultSlotName <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Use this attribute to specify the path to the Virtual portal Context: -virtualPortalContext <value> Log files from command execution can be found in the logs directory of the DXClient installation. Example: ``` dxclient create-syndication-relation -dxProtocol -hostname -dxPort -contenthandlerPath -dxUsername -dxPassword -syndicatorUrl -syndicatorName -subscriberName -vaultSlotName -isEnabled -updateAfterCreation -syndicationType -webContentLibraries -subscriberURL -mode -syndicatorVaultSlotName -dxContextRoot -virtualPortalContext ``` Parent topic: DXClient Artifact Types","title":"Managing Web Content Syndicators and Subscribers using DXClient"},{"location":"containerization/syndicatorsandsubscribers/#managing-web-content-syndicators-and-subscribers-using-dxclient","text":"The section provides information about using the DXClient process to automate the management of Web Content Manager Syndicators, Subscribers, and get-syndication reports. For more information on the process and settings of the Web Content Manager Syndicators and Subscribers, see How to manage syndicators and subscribers .","title":"Managing Web Content Syndicators and Subscribers using DXClient"},{"location":"containerization/syndicatorsandsubscribers/#managing-syndicators","text":"The manage-syndicator command is used to enable or disable the syndicator using the provided input. Command description This command invokes the manage-syndicator tool inside the DXClient. It is used to enable or disable the syndicator. dxclient manage-syndicator Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-syndicator -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (example: /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance. -UUID <value> Use true or false to enable or disable the syndicator. -enable <value> The options that are passed through the command line override the default values. Example: dxclient manage-syndicator -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable>","title":"Managing syndicators"},{"location":"containerization/syndicatorsandsubscribers/#manage-syndicator-get-syndication-report","text":"The manage-syndicator get-syndication-report command is used to fetch the failed reports of the syndicator. Command description This command invokes the syndicator-faileditems tool inside the DXClient, which is used to fetch the failed reports. dxclient manage-syndicator get-syndication-report Help command This command shows the help document on the manage-syndicator get-syndication-report command usage: dxclient manage-syndicator get-syndication-report -h Command options Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443) -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to specify the UUID of the syndicator instance -UUID <value> The options that are passed through the command line override the default values. Example usage with UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> Example usage without UUID: dxclient manage-syndicator get-syndication-report -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> Note: If UUID of a syndicator is specified, then the command provides the report for only the particular syndicator that is present in the target DX Server; otherwise, it provides the failure report for all syndicators.","title":"Manage-syndicator get-syndication-report"},{"location":"containerization/syndicatorsandsubscribers/#managing-subscribers","text":"The manage-subscriber command is used to enable or disable the subscriber using the provided input. Command description This command invokes the manage-subscriber tool inside the DXClient. It is used to enable/disable the subscriber. dxclient manage-subscriber Help command This command shows the help document on the manage-syndicator command usage: dxclient manage-subscriber -h Command options Use this attribute to specify the hostname of the target server. -hostname <value> Use this attribute to specify the protocol with which to connect to the server. -dxProtocol <value> Use this attribute to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443). -dxPort <value> Use this attribute to specify the username that is required for authenticating with the server. -dxUsername <value> Use this attribute to specify the password that is required for authenticating with the server. -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (for example, /wps/mycontenthandler). -contenthandlerPath <value> Use this attribute to specify the UUID of the subscriber instance. -UUID <value> Use this attribute to specify the enable or disable the subscriber instance. Use true or false to enable or disable the subscriber. -enable <value> The options passed through command line overrides the default values. Example: dxclient manage-subscriber -dxProtocol <dxProtocol> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -contenthandlerPath <contenthandlerPath> -UUID <UUID> -enable <enable>","title":"Managing subscribers"},{"location":"containerization/syndicatorsandsubscribers/#create-syndication-relation","text":"The create-syndication-relation command is used to create the syndication relation between syndicator and subscriber in the DX server. Command description Use the create-syndication-relation to create syndication relation: dxclient create-syndication-relation Help command This command shows the help information for create-syndication-relation command usage: dxclient create-syndication-relation -h Command options Use this attribute to specify the protocol with which to connect to the DX server: -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server (for Kubernetes Environment, dxPort is 443): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server -dxUsername <value> Use this attribute to specify the password for the user in the \"dxUsername\" attribute -dxPassword <value> The path to the contenthandler servlet on the Script Application server: -contenthandlerPath <value> Syndicator URL of target server, for example, http(s)://host:port/wps/wcm: -syndicatorUrl <value> Use this attribute to specify the new syndicator name: -syndicatorName <value> Use this attribute to specify the new subscriber name: -subscriberName <value> Use this attribute to specify the Credential Vault Name of source server: -vaultSlotName <value> Whether the syndicator/subscriber pair is enabled on creation: isEnabled (default is true): -isEnabled <value> Whether the syndicator/subscriber pair is updateAfterCreation : updateAfterCreation (default is true): -updateAfterCreation <value> The libraries to syndicate eg. all-items,liveItems,liveProjectsItem,all-items,published-items and all-items-and-versions: -syndicationType <value> Use this attribute to specify the Libraries Name of target Server: -webContentLibraries <value> Use this attribute to specify the Subscriber URL, for example, http(s)://host:port/wps/wcm: -subscriberURL <value> Use this attribute to specify the Syndicator/subscriber mode: -mode <value> Use this attribute to specify the Credential Vault Name: -syndicatorVaultSlotName <value> Use this attribute to specify the path to the context root on the DX server (for example, /wps): -dxContextRoot <value> Use this attribute to specify the path to the Virtual portal Context: -virtualPortalContext <value> Log files from command execution can be found in the logs directory of the DXClient installation. Example: ``` dxclient create-syndication-relation -dxProtocol -hostname -dxPort -contenthandlerPath -dxUsername -dxPassword -syndicatorUrl -syndicatorName -subscriberName -vaultSlotName -isEnabled -updateAfterCreation -syndicationType -webContentLibraries -subscriberURL -mode -syndicatorVaultSlotName -dxContextRoot -virtualPortalContext ``` Parent topic: DXClient Artifact Types","title":"Create Syndication Relation"},{"location":"containerization/t_customize_dx_url/","text":"Customizing the HCL DX URL when deployed to container platforms This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Note: Defining the custom context root URL feature is available in HCL DX 9.5 Container Update CF193 and later. Prerequisites and Notes: The following configuration procedure is supported for DX container deployments to Kubernetes and OpenShift platforms. To change the HCL DX URL hybrid container deployment and on-premise deployment, refer to the following topic: Customizing the HCL DX URL for hybrid deployment . The dxctl tool is used for this configuration process. Before running the dxctl tool, the administrator must log on to the targeted DX cluster using the cloud-specific CLI login commands for the supported Kubernetes and OpenShift platforms; such as Microsoft Azure Kubernetes Services (AKS), Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), or Red Hat OpenShift. For example, to log in to your DX container cluster on the Red Hat OpenShift platform, use the oc login command. Ensure that you have updated the DxDeployment custom resource definition to the HCL DX 9.5 Container Update CF192 and later releases. For more information, see Customizing the container deployment . For more information on the custom URI management for HCL Digital Experience, refer to the following topic: Changing the portal URI after an installation . If you have already defined a custom Digital Experience URL in your existing container deployment, it is mandatory to configure the following properties with the existing values. Otherwise, the properties are updated with the default values. Customize the context root in your Digital Experience container deployment: The following are the default configuration property values for the context root changes. ``` Path dx.path.contextroot: wps dx.path.home: portal dx.path.personalized: myportal ``` To change the default values to your custom requirements, update the following properties. For example: ``` Path dx.path.contextroot: hcl dx.path.home: dx dx.path.personalized: mydx ``` Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX Container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous DX 9.5 Container deployment, then update the properties as specified in Step 1 , and then run the following command: $ dxctl --update -p properties/full-deployment.properties Additional Considerations and Example: Review the following manual, required and optional steps to complete the context root customization updates. Perform the steps that are related to your DX deployment details. (Some optional steps may not apply to your deployment). Optional step: If your DX deployment includes custom themes that use Dojo, update those themes to refer to the correct Dojo context root. The default Dojo context root in HCL Digital Experience is /wps/portal_dojo. After you run the modify-servlet-path and modify-servlet-path-portlets tasks, the Dojo context root is changed to include the new value in the WpsContextRoot parameter as the prefix. For example, if the new WpsContextRoot value is myco, then the new Dojo context root becomes /myco/portal_dojo. If your theme includes hard-coded references to /wps/portal_dojo, update those references to the new context root. If you migrated a custom theme, you might find that it has references to /portal_dojo without the /wps prefix. Look for these references in both the WAR file and in the WebDAV storage for your theme. Required step: Refresh your search collection and select Regather to update the documents. Log on to the Digital Experience platform as the administrator. Navigate to the Practitioner Studio menu. Select Search: Open the Manage Search portlet. Click Search Collections. Click the search collection that you want to update. For example: Default Search Collection. Start the Digital Experience search collection crawler service for each content collection source: Notes: If the documents are not stored in the search collection but a schedule is defined for the crawler, then the crawler automatically runs at the scheduled time. You can also start the crawler manually. If the documents are already collected, then select Regather documents to update the documents with the new context root information. Click Collections from All Services in the breadcrumb trail and select the next search collection to modify. Optional step: From the Web Content interface of Practitioner Studio, update the Web Content Manager syndicator and subscriber servers that reference your modified DX Container site URL. If you do not use syndication, skip this step. Log on to the site that syndicates to this instance. Open the Syndicators page. Click the edit icon by the syndicator that you want to edit. Update the URL with the new context root information. Log on to the site that subscribes to this instance. Open the Subscribers page. Click the edit icon of the subscriber that you want to edit. Update the URL with the new context root information. Configure no context root in your Digital Experience container deployment To configure no context root, update the following property values. For example: ``` Path dx.path.contextroot: \" \" dx.path.home: \" \" dx.path.personalized: mydx dx.ready.path: / dx.live.path: / ``` Note: If the context root is removed, the home path must be removed as well. It may take more time than usual for the DX-Core pod to get to a running state during the update process. Before log in (no context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e After log in (personalized context root): After log in (personalized context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/mydx/woodburnstudio/home/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous HCL DX 9.5 container deployment, update the property values as specified in step 1, and run the following command: $ dxctl --update -p properties/full-deployment.properties Parent topic: Customizing your container deployment","title":"Customizing the HCL DX URL when deployed to container platforms"},{"location":"containerization/t_customize_dx_url/#customizing-the-hcl-dx-url-when-deployed-to-container-platforms","text":"This section describes the procedures to define custom context root URLs, or no context root URL definitions, when deploying your HCL DX 9.5 software to the supported container platforms. Note: Defining the custom context root URL feature is available in HCL DX 9.5 Container Update CF193 and later. Prerequisites and Notes: The following configuration procedure is supported for DX container deployments to Kubernetes and OpenShift platforms. To change the HCL DX URL hybrid container deployment and on-premise deployment, refer to the following topic: Customizing the HCL DX URL for hybrid deployment . The dxctl tool is used for this configuration process. Before running the dxctl tool, the administrator must log on to the targeted DX cluster using the cloud-specific CLI login commands for the supported Kubernetes and OpenShift platforms; such as Microsoft Azure Kubernetes Services (AKS), Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), or Red Hat OpenShift. For example, to log in to your DX container cluster on the Red Hat OpenShift platform, use the oc login command. Ensure that you have updated the DxDeployment custom resource definition to the HCL DX 9.5 Container Update CF192 and later releases. For more information, see Customizing the container deployment . For more information on the custom URI management for HCL Digital Experience, refer to the following topic: Changing the portal URI after an installation . If you have already defined a custom Digital Experience URL in your existing container deployment, it is mandatory to configure the following properties with the existing values. Otherwise, the properties are updated with the default values. Customize the context root in your Digital Experience container deployment: The following are the default configuration property values for the context root changes. ```","title":"Customizing the HCL DX URL when deployed to container platforms"},{"location":"containerization/t_customize_dx_url/#path","text":"dx.path.contextroot: wps dx.path.home: portal dx.path.personalized: myportal ``` To change the default values to your custom requirements, update the following properties. For example: ```","title":"Path"},{"location":"containerization/t_customize_dx_url/#path_1","text":"dx.path.contextroot: hcl dx.path.home: dx dx.path.personalized: mydx ``` Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX Container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous DX 9.5 Container deployment, then update the properties as specified in Step 1 , and then run the following command: $ dxctl --update -p properties/full-deployment.properties","title":"Path"},{"location":"containerization/t_customize_dx_url/#additional-considerations-and-example","text":"Review the following manual, required and optional steps to complete the context root customization updates. Perform the steps that are related to your DX deployment details. (Some optional steps may not apply to your deployment). Optional step: If your DX deployment includes custom themes that use Dojo, update those themes to refer to the correct Dojo context root. The default Dojo context root in HCL Digital Experience is /wps/portal_dojo. After you run the modify-servlet-path and modify-servlet-path-portlets tasks, the Dojo context root is changed to include the new value in the WpsContextRoot parameter as the prefix. For example, if the new WpsContextRoot value is myco, then the new Dojo context root becomes /myco/portal_dojo. If your theme includes hard-coded references to /wps/portal_dojo, update those references to the new context root. If you migrated a custom theme, you might find that it has references to /portal_dojo without the /wps prefix. Look for these references in both the WAR file and in the WebDAV storage for your theme. Required step: Refresh your search collection and select Regather to update the documents. Log on to the Digital Experience platform as the administrator. Navigate to the Practitioner Studio menu. Select Search: Open the Manage Search portlet. Click Search Collections. Click the search collection that you want to update. For example: Default Search Collection. Start the Digital Experience search collection crawler service for each content collection source: Notes: If the documents are not stored in the search collection but a schedule is defined for the crawler, then the crawler automatically runs at the scheduled time. You can also start the crawler manually. If the documents are already collected, then select Regather documents to update the documents with the new context root information. Click Collections from All Services in the breadcrumb trail and select the next search collection to modify. Optional step: From the Web Content interface of Practitioner Studio, update the Web Content Manager syndicator and subscriber servers that reference your modified DX Container site URL. If you do not use syndication, skip this step. Log on to the site that syndicates to this instance. Open the Syndicators page. Click the edit icon by the syndicator that you want to edit. Update the URL with the new context root information. Log on to the site that subscribes to this instance. Open the Subscribers page. Click the edit icon of the subscriber that you want to edit. Update the URL with the new context root information. Configure no context root in your Digital Experience container deployment To configure no context root, update the following property values. For example: ```","title":"Additional Considerations and Example:"},{"location":"containerization/t_customize_dx_url/#path_2","text":"dx.path.contextroot: \" \" dx.path.home: \" \" dx.path.personalized: mydx dx.ready.path: / dx.live.path: / ``` Note: If the context root is removed, the home path must be removed as well. It may take more time than usual for the DX-Core pod to get to a running state during the update process. Before log in (no context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e After log in (personalized context root): After log in (personalized context root): https://dx-cr-demo-service-dx-cr-01-dx-cr-01.apps.sample.domain.net/mydx/woodburnstudio/home/!ut/p/z1/04_Sj9CPykssy0xPLMnMz0vMAfljo8ziDVCAo4FTkJGTsYGBu7OJfjhYgbmHi7u7oYFhgL-bu4BoJmrt7e Important: Do not use the same value for the dx.path.home and dx.path.personalized properties. To apply the HCL Digital Experience custom URI during a new DX container deployment, run the following command: $ dxctl --deploy -p properties/full-deployment.properties To change the custom URI of a previous HCL DX 9.5 container deployment, update the property values as specified in step 1, and run the following command: $ dxctl --update -p properties/full-deployment.properties Parent topic: Customizing your container deployment","title":"Path"},{"location":"containerization/t_customize_dx_url_hybrid_deployment/","text":"Customizing the HCL DX URL for hybrid deployment HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment. Prerequisites Refer to the following topic documentation for hybrid deployment prerequisites: Hybrid deployment - Operator Hybrid Deployment - Helm Customize the context root of the portal and hybrid deployment The following steps refer to a current running state of the HCL DX Experience on-premise: Stop the HCL Digital Experience server. Navigate to wp_profile inside your portal installed location and run the following command: ./bin/stopServer.sh WebSphere_Portal -user <username> -password <password> For example: ./bin/stopServer.sh WebSphere_Portal -user wpsadmin -password wpsadmin Update context root in the config engine property files. Locate the wkplc.properties and wkplc_comp.properties files in the wp_profile/ConfigEngine/properties directory. For example: cd wp_profile/ConfigEngine/properties Edit wkplc.properties file in the wp_profile/ConfigEngine/properties directory and create a backup copy before you change any values. Enter the appropriate value for your environment in the WpsContextRoot property. For example: Property Default value New values for Context-root New values for No-context-root WpsContextRoot wps hcl Edit wkplc_comp.properties file in the wp_profile/ConfigEngine/properties directory and create backup copy before you change any values. Enter the appropriate value for your environment in the following properties: Property Default value New values for Context-root New values for No-context-root WpsContextRoot wsp/wsrp hcl/wsrp /wsrp WpsPersonalisedPath myportal mydx mydx WpsDefaultHome portal dx Save and close the file. Note: Do not enter the same value for WpsPersonalizedHome and WpsDefaultHome . Open a command prompt and change to the wp_profile/ConfigEngine directory. To change the context root for the values that you entered in the WpsContextRoot , WsrpContextRoot , WpsPersonalizedHome , and or WpsDefaultHome properties, run the below task: For AIX\u00ae, HP-UX, and Linux\u2122 Solaris: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For Windows: ConfigEngine.bat modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For IBM i ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For z/OS: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For example: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin -DWpsContextRoot=hcl -DWpsDefaultHome=dx -DWpsPersonalizedHome=mydx Note: Check the output for any error messages before you proceed with the next task. If any of the configuration tasks fail, verify the values in the wkplc.properties and wkplc_comp.properties files. Start the HCL Digital Experience server. Navigate to wp_profile inside your portal installed location and run below command: ./bin/startServer.sh WebSphere_Portal -user <username> -password <password> For example: ./bin/startServer.sh WebSphere_Portal -user wpsadmin -password wpsadmin Now, access the portal with the changed context root. Follow these steps for hybrid deployment based on changed context root. Download the dxctl tool . Find to edit the hybrid-deployment.properties file located under hcl-dx-cloud-scripts/dxctl/properties directory. Put the values for the following properties based on the DX portal context root changes made in previous steps: dx.path.contextroot dx.path.personalized dx.path.home To change the other properties, refer to Step 2 of Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment . Note: When updating deployment, set the following properties if DX context root is configured in your existing deployment: Property Values for context-root Values for no context-root dx.path.contextroot hcl dx.path.personalized mydx mydx dx.path.home dx Once the hybrid-deployment.properties file is ready, process the deployment by following below command: ./os/dxctl --deploy -p ./properties/hybrid-deployment.properties For example: Once hybrid deployment is successful, then DAM and CC from the DX portal. Note: Make sure DAM and CC are enabled and the correct on-prem host is configured under Manage Pages . If DAM and CC are not enabled, refer to Step 2 of Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment . Now, access HCL DX with no context root. Refer to Ste 5-e for hybrid deployment and update the below properties in the hybrid-deployment.properties file as per no context root changes for DX portal. Once the deployment is successful, update the DAM and CC path from Site Management > Manage Pages in DX Portal. Now, access DAM and CC in DX Portal. Parent topic: Digital Experience on containerized platforms","title":"Customizing the HCL DX URL for hybrid deployment"},{"location":"containerization/t_customize_dx_url_hybrid_deployment/#customizing-the-hcl-dx-url-for-hybrid-deployment","text":"HCL Digital Experience and Web Services for Remote Portlets are installed with a default URI or context root. This section describes how to change default URI or context root of the portal and hybrid deployment.","title":"Customizing the HCL DX URL for hybrid deployment"},{"location":"containerization/t_customize_dx_url_hybrid_deployment/#prerequisites","text":"Refer to the following topic documentation for hybrid deployment prerequisites: Hybrid deployment - Operator Hybrid Deployment - Helm","title":"Prerequisites"},{"location":"containerization/t_customize_dx_url_hybrid_deployment/#customize-the-context-root-of-the-portal-and-hybrid-deployment","text":"The following steps refer to a current running state of the HCL DX Experience on-premise: Stop the HCL Digital Experience server. Navigate to wp_profile inside your portal installed location and run the following command: ./bin/stopServer.sh WebSphere_Portal -user <username> -password <password> For example: ./bin/stopServer.sh WebSphere_Portal -user wpsadmin -password wpsadmin Update context root in the config engine property files. Locate the wkplc.properties and wkplc_comp.properties files in the wp_profile/ConfigEngine/properties directory. For example: cd wp_profile/ConfigEngine/properties Edit wkplc.properties file in the wp_profile/ConfigEngine/properties directory and create a backup copy before you change any values. Enter the appropriate value for your environment in the WpsContextRoot property. For example: Property Default value New values for Context-root New values for No-context-root WpsContextRoot wps hcl Edit wkplc_comp.properties file in the wp_profile/ConfigEngine/properties directory and create backup copy before you change any values. Enter the appropriate value for your environment in the following properties: Property Default value New values for Context-root New values for No-context-root WpsContextRoot wsp/wsrp hcl/wsrp /wsrp WpsPersonalisedPath myportal mydx mydx WpsDefaultHome portal dx Save and close the file. Note: Do not enter the same value for WpsPersonalizedHome and WpsDefaultHome . Open a command prompt and change to the wp_profile/ConfigEngine directory. To change the context root for the values that you entered in the WpsContextRoot , WsrpContextRoot , WpsPersonalizedHome , and or WpsDefaultHome properties, run the below task: For AIX\u00ae, HP-UX, and Linux\u2122 Solaris: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For Windows: ConfigEngine.bat modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For IBM i ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For z/OS: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=<password> -DPortalAdminPwd=<password> -DWpsContextRoot=<value> -DWpsDefaultHome=<value> -DWpsPersonalizedHome=<value> For example: ./ConfigEngine.sh modify-servlet-path modify-servlet-path-portlets -DWasPassword=wpsadmin -DPortalAdminPwd=wpsadmin -DWpsContextRoot=hcl -DWpsDefaultHome=dx -DWpsPersonalizedHome=mydx Note: Check the output for any error messages before you proceed with the next task. If any of the configuration tasks fail, verify the values in the wkplc.properties and wkplc_comp.properties files. Start the HCL Digital Experience server. Navigate to wp_profile inside your portal installed location and run below command: ./bin/startServer.sh WebSphere_Portal -user <username> -password <password> For example: ./bin/startServer.sh WebSphere_Portal -user wpsadmin -password wpsadmin Now, access the portal with the changed context root. Follow these steps for hybrid deployment based on changed context root. Download the dxctl tool . Find to edit the hybrid-deployment.properties file located under hcl-dx-cloud-scripts/dxctl/properties directory. Put the values for the following properties based on the DX portal context root changes made in previous steps: dx.path.contextroot dx.path.personalized dx.path.home To change the other properties, refer to Step 2 of Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment . Note: When updating deployment, set the following properties if DX context root is configured in your existing deployment: Property Values for context-root Values for no context-root dx.path.contextroot hcl dx.path.personalized mydx mydx dx.path.home dx Once the hybrid-deployment.properties file is ready, process the deployment by following below command: ./os/dxctl --deploy -p ./properties/hybrid-deployment.properties For example: Once hybrid deployment is successful, then DAM and CC from the DX portal. Note: Make sure DAM and CC are enabled and the correct on-prem host is configured under Manage Pages . If DAM and CC are not enabled, refer to Step 2 of Enabling Hybrid Deployment support in the HCL Digital Experience 9.5 on-premises environment . Now, access HCL DX with no context root. Refer to Ste 5-e for hybrid deployment and update the below properties in the hybrid-deployment.properties file as per no context root changes for DX portal. Once the deployment is successful, update the DAM and CC path from Site Management > Manage Pages in DX Portal. Now, access DAM and CC in DX Portal. Parent topic: Digital Experience on containerized platforms","title":"Customize the context root of the portal and hybrid deployment"},{"location":"containerization/themes/","text":"Themes This topic provides information about the deployment and undeployment of themes artifacts. Deploy theme The deploy-theme command is used to deploy a theme (EAR and WebDAV based) from a source client or server environment to a target HCL DX 9.5 CF192 or later server using the provided theme registration XML file, deployable EAR file, and WebDAV theme collection. Required files Theme Registration XML file: This XML file is required to register the theme into DX Server. Theme deployable EAR file: This EAR file containing theme data is used for deploying into the WebSphere Application Server. WebDAV theme collection: The theme collection folder/zip is used to create or update the collection in WebDAV file store of the DX Server. Notes: This command can execute below one or more tasks together: Theme Registration Theme EAR deployment WebDAV theme collection Command dxclient deploy-theme -xmlFile <path> -applicationFile <path> -applicationName <application name> -themeName <theme collection name> -themePath <folder/zip path of theme collection> Help command This command shows the help document on the deploy-theme command usage: dxclient deploy-theme -h Common Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute and re-trigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered deploy theme request> Required options for Theme Registration Use this attribute to specify the local path to the theme registration XML file: -xmlFile <Absolute or relative path to theme registration xml input file> Use this attribute to specify the path to DX configuration endpoint (for example, /wps/config): -xmlConfigPath <value> Note: For theme registration, a backup of the complete DX configuration export (not including users) is taken and placed in store/outputFiles/themes/backup/foldername folder. Required options for Theme EAR deployment Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value is typically the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server: -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the theme EAR file path that is required while executing the deploy theme task: \u2013applicationFile <Absolute or relative path to deployable theme ear file> Use this attribute to specify the theme application name: -applicationName <value> Required options for creating/updating WebDAV theme collection Use this attribute to specify the theme name of the collection created under WebDAV server in DX: -themeName <value> Use this attribute to specify the theme file path that contains all static files to be pushed into DX theme, it accepts either folder or zip file path of the WebDAV theme collection: -themePath <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Notes: For new WebDAV theme collection, DXClient tool adds the provided collection (folder/zip) to the WebDAV file store. For existing WebDAV theme collection, the existing theme collection is replaced by the provided theme collection during the update. To get the latest theme collection from the DX server, see Exporting content from the filestore and make modifications on the same folder to get it updated in the DX Server WebDAV file store. For WebDAV theme collection update, a backup of the existing theme collection is taken and placed in store/outputFiles/themes/backup/foldername folder. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient deploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationFile <application-file-with-path> -applicationName <application name> -themeName <theme-name> -themePath <theme-path> -dxProfileName <Profile name of the DX core server> Undeploy theme The undeploy-theme command can be used to undeploy a theme, including the EAR application and WebDAV files, and it also unregisters the registered theme from the target DX server. Note: If enableBackup is set to true, then undeploy-theme takes a backup of the deployed EAR theme, WebDAV theme collection, and completes DX configuration export (without users) and place it in the backup folder. When the user is downloading EAR, WeDAV, and XML to backup, we must separate it by the folder names store/outputFiles/themes/backup/foldername . The backup of EAR is placed in store/outputFiles/themes/backup/application . Users can restore the theme by using the backup files. Note: Pages might lose the applied theme references in the restored themes. Command description This command invokes the undeploy-theme tool inside the DXClient. This command uses the unregistered theme XML file, theme EAR application name and WebDAV theme collection name, and executes the undeploy theme task. ``` dxclient undeploy-theme ``` Help command This command shows the help document on the undeploy-theme command usage: dxclient undeploy-theme -h Required files Theme Unregistration XML file: This XML file is required to unregister the theme from target DX Server and must contain the details of the theme. The XML file must be provided when executing the undeploy theme task. This command can execute one or more of the following tasks at the same time: Theme unregistration Undeploy theme EAR application Undeploy WebDAV theme collection Common commands Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443 ) -dxPort <value> Use this attribute to specify the username that is required for server authentication -dxUsername <value> Use this attribute to specify the password that is required for server authentication -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to take the backup before undeploying theme -enableBackup <value> Note: User can set the enableBackup parameter as true to take backup before undeploying theme. The value is set to false by default. The options passed through command line override the default values. Required options for Theme Unregistration: Use this attribute to specify the theme registration xml file that is used while executing the undeploy theme task. For example, see the Theme-registration.xml file in the directory dxclient/samples/ -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to specify the path to DX configuration endpoint -xmlConfigPath <value> Required options for undeploying theme EAR application: Use this attribute to specify the configuration wizard home (change of route is only in case of Open Shift Kubernetes Enviornment, otherwise the route remains the same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR application name -applicationName <value> Required options for undeploying WebDAV theme collection: Use this attribute to specify the theme name of the collection created under WebDAV -themeName <value> Example: dxclient undeploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationName <application name> -themeName <theme-name> -enableBackup <enable-backup> -dxProfileName <Profile name of the DX core server profile> Parent topic: DXClient Artifact Types","title":"Themes"},{"location":"containerization/themes/#themes","text":"This topic provides information about the deployment and undeployment of themes artifacts.","title":"Themes"},{"location":"containerization/themes/#deploy-theme","text":"The deploy-theme command is used to deploy a theme (EAR and WebDAV based) from a source client or server environment to a target HCL DX 9.5 CF192 or later server using the provided theme registration XML file, deployable EAR file, and WebDAV theme collection. Required files Theme Registration XML file: This XML file is required to register the theme into DX Server. Theme deployable EAR file: This EAR file containing theme data is used for deploying into the WebSphere Application Server. WebDAV theme collection: The theme collection folder/zip is used to create or update the collection in WebDAV file store of the DX Server. Notes: This command can execute below one or more tasks together: Theme Registration Theme EAR deployment WebDAV theme collection Command dxclient deploy-theme -xmlFile <path> -applicationFile <path> -applicationName <application name> -themeName <theme collection name> -themePath <folder/zip path of theme collection> Help command This command shows the help document on the deploy-theme command usage: dxclient deploy-theme -h Common Command options Use this attribute to specify the protocol with which to connect to the DX server ( wp_profile ): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute and re-trigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered deploy theme request> Required options for Theme Registration Use this attribute to specify the local path to the theme registration XML file: -xmlFile <Absolute or relative path to theme registration xml input file> Use this attribute to specify the path to DX configuration endpoint (for example, /wps/config): -xmlConfigPath <value> Note: For theme registration, a backup of the complete DX configuration export (not including users) is taken and placed in store/outputFiles/themes/backup/foldername folder. Required options for Theme EAR deployment Use this attribute to specify the Configuration Wizard Console port number: -dxConnectPort <value> Use this attribute to specify the config wizard home (change to the appropriate route in the case of an OpenShift Kubernetes Environment, otherwise the value is typically the same as the hostname) that is required for authenticating with the DXConnect application: -dxConnectHostname <value> Use this attribute to specify the Configuration Wizard Administrator username that is required for authenticating with the DXConnect application: -dxConnectUsername <value> Use this attribute to specify the Configuration Wizard Administrator password that is required for authenticating with the DXConnect application: -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server: -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the theme EAR file path that is required while executing the deploy theme task: \u2013applicationFile <Absolute or relative path to deployable theme ear file> Use this attribute to specify the theme application name: -applicationName <value> Required options for creating/updating WebDAV theme collection Use this attribute to specify the theme name of the collection created under WebDAV server in DX: -themeName <value> Use this attribute to specify the theme file path that contains all static files to be pushed into DX theme, it accepts either folder or zip file path of the WebDAV theme collection: -themePath <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler): -contenthandlerPath <value> Notes: For new WebDAV theme collection, DXClient tool adds the provided collection (folder/zip) to the WebDAV file store. For existing WebDAV theme collection, the existing theme collection is replaced by the provided theme collection during the update. To get the latest theme collection from the DX server, see Exporting content from the filestore and make modifications on the same folder to get it updated in the DX Server WebDAV file store. For WebDAV theme collection update, a backup of the existing theme collection is taken and placed in store/outputFiles/themes/backup/foldername folder. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient deploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationFile <application-file-with-path> -applicationName <application name> -themeName <theme-name> -themePath <theme-path> -dxProfileName <Profile name of the DX core server>","title":"Deploy theme"},{"location":"containerization/themes/#undeploy-theme","text":"The undeploy-theme command can be used to undeploy a theme, including the EAR application and WebDAV files, and it also unregisters the registered theme from the target DX server. Note: If enableBackup is set to true, then undeploy-theme takes a backup of the deployed EAR theme, WebDAV theme collection, and completes DX configuration export (without users) and place it in the backup folder. When the user is downloading EAR, WeDAV, and XML to backup, we must separate it by the folder names store/outputFiles/themes/backup/foldername . The backup of EAR is placed in store/outputFiles/themes/backup/application . Users can restore the theme by using the backup files. Note: Pages might lose the applied theme references in the restored themes. Command description This command invokes the undeploy-theme tool inside the DXClient. This command uses the unregistered theme XML file, theme EAR application name and WebDAV theme collection name, and executes the undeploy theme task. ``` dxclient undeploy-theme ``` Help command This command shows the help document on the undeploy-theme command usage: dxclient undeploy-theme -h Required files Theme Unregistration XML file: This XML file is required to unregister the theme from target DX Server and must contain the details of the theme. The XML file must be provided when executing the undeploy theme task. This command can execute one or more of the following tasks at the same time: Theme unregistration Undeploy theme EAR application Undeploy WebDAV theme collection Common commands Use this attribute to specify the hostname of the target server -hostname <value> Use this attribute to specify the protocol with which to connect to the server -dxProtocol <value> Use this attribute to specify the port on which to connect to the server (for Kubernetes Environment, dxPort is 443 ) -dxPort <value> Use this attribute to specify the username that is required for server authentication -dxUsername <value> Use this attribute to specify the password that is required for server authentication -dxPassword <value> Use this attribute to specify the path to the contenthandler servlet on the DX server (e.g. /wps/mycontenthandler) -contenthandlerPath <value> Use this attribute to take the backup before undeploying theme -enableBackup <value> Note: User can set the enableBackup parameter as true to take backup before undeploying theme. The value is set to false by default. The options passed through command line override the default values. Required options for Theme Unregistration: Use this attribute to specify the theme registration xml file that is used while executing the undeploy theme task. For example, see the Theme-registration.xml file in the directory dxclient/samples/ -xmlFile <xml file name with absolute path of the xmlaccess input file> Use this attribute to specify the path to DX configuration endpoint -xmlConfigPath <value> Required options for undeploying theme EAR application: Use this attribute to specify the configuration wizard home (change of route is only in case of Open Shift Kubernetes Enviornment, otherwise the route remains the same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this attribute to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this attribute to specify Soap port of the DX server -dxSoapPort <Soap port of the DX server> Specify either the dxProfileName or dxProfilePath of the DX core server: Use this attribute to specify the profile name of the DX core server (for example: wp_profile ): -dxProfileName <Profile name of the DX core server> OR Use this attribute to specify the profile path of the DX server (for example: /opt/HCL/wp_profile ): -dxProfilePath <Path of the DX core server profile> Use this attribute to specify the EAR application name -applicationName <value> Required options for undeploying WebDAV theme collection: Use this attribute to specify the theme name of the collection created under WebDAV -themeName <value> Example: dxclient undeploy-theme -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxSoapPort <dxSoapPort> -dxConnectHostname <hostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -xmlFile <xml-file-with-path> -applicationName <application name> -themeName <theme-name> -enableBackup <enable-backup> -dxProfileName <Profile name of the DX core server profile> Parent topic: DXClient Artifact Types","title":"Undeploy theme"},{"location":"containerization/troubleshooting/","text":"Troubleshooting cloud container Containers This section lists the basics of troubleshooting the containerized image or your deployment. If the issue is suspected to be specific to the containerized image or deployment, please collect the following information. Otherwise, please follow the standard Mustgather for the reported problem. General Provide the proper context regarding the issue. What is failing? What events occurred leading up to the failure? What are the expected results if unclear? Include error messages, logs, screen captures, and other information that may help describe the problem. Include the results of ' docker inspect *IMAGEID* '. Kubernetes Include the following information: ``` kubectl version - ``` kubectl get nodes - ``` kubectl describe customresourcedefinitions - ``` kubectl describe DxDeployment -n <your_namespace> - ``` kubectl api-resources - ``` kubectl api-versions - Command or procedure used for deployment. **Note:** See the section for the recommended deployment method, [Deploy HCL Digital Experience 9.5 Container to Amazon EKS](kubernetes_eks.md). - The files generated at the time of the deployment and documented at the end of the section on [Understanding the HCL 9.5 Container deployment on Amazon Elastic Kubernetes Service](understanding_kubernetes_eks_deployment.md). OpenShift Include the following information: ``` oc version - ``` oc get nodes - ``` oc describe customresourcedefinitions - ``` oc describe DxDeployment -n <your_namespace> - To set project: ``` oc project <your_namespace> ``` - ``` oc api-resources - ``` oc api-versions - Command or procedure used for deployment. **Note:** See the section for the recommended deployment method, [Red Hat OpenShift](openshift.md). - The files generated at the time of the deployment and documented at the end of the section on [Understanding the OpenShift deployment](understanding_openshift_deployment.md). Docker What is the version of the Docker engine? What is the operating system? What is the exact command used to start (or attempt to start) the container? Include results of docker images . Include results of docker ps -a Issue Solution - Out of memory |Ensure that the memory request and the limit defined in the custom resource are high enough for the specified heap size in the Portal configuration| |- DX pod will not start - Unschedulable pods - Pod has unbound immediate PersistentVolumeClaims |Ensure that the specified volume exists, and meets the deployment requirements. To reuse a volume, see the Delete topic.| Logging and tracing for HCL Digital Experience Containers and services - Access the tracing options that can be used to capture logging and tracing for HCL Digital Experience (DX) 9.5 container based services with container update CF181 and higher releases. Parent topic: Operator-based deployment","title":"Troubleshooting cloud container Containers"},{"location":"containerization/troubleshooting/#troubleshooting-cloud-container-containers","text":"This section lists the basics of troubleshooting the containerized image or your deployment. If the issue is suspected to be specific to the containerized image or deployment, please collect the following information. Otherwise, please follow the standard Mustgather for the reported problem. General Provide the proper context regarding the issue. What is failing? What events occurred leading up to the failure? What are the expected results if unclear? Include error messages, logs, screen captures, and other information that may help describe the problem. Include the results of ' docker inspect *IMAGEID* '. Kubernetes Include the following information: ``` kubectl version - ``` kubectl get nodes - ``` kubectl describe customresourcedefinitions - ``` kubectl describe DxDeployment -n <your_namespace> - ``` kubectl api-resources - ``` kubectl api-versions - Command or procedure used for deployment. **Note:** See the section for the recommended deployment method, [Deploy HCL Digital Experience 9.5 Container to Amazon EKS](kubernetes_eks.md). - The files generated at the time of the deployment and documented at the end of the section on [Understanding the HCL 9.5 Container deployment on Amazon Elastic Kubernetes Service](understanding_kubernetes_eks_deployment.md). OpenShift Include the following information: ``` oc version - ``` oc get nodes - ``` oc describe customresourcedefinitions - ``` oc describe DxDeployment -n <your_namespace> - To set project: ``` oc project <your_namespace> ``` - ``` oc api-resources - ``` oc api-versions - Command or procedure used for deployment. **Note:** See the section for the recommended deployment method, [Red Hat OpenShift](openshift.md). - The files generated at the time of the deployment and documented at the end of the section on [Understanding the OpenShift deployment](understanding_openshift_deployment.md). Docker What is the version of the Docker engine? What is the operating system? What is the exact command used to start (or attempt to start) the container? Include results of docker images . Include results of docker ps -a Issue Solution - Out of memory |Ensure that the memory request and the limit defined in the custom resource are high enough for the specified heap size in the Portal configuration| |- DX pod will not start - Unschedulable pods - Pod has unbound immediate PersistentVolumeClaims |Ensure that the specified volume exists, and meets the deployment requirements. To reuse a volume, see the Delete topic.| Logging and tracing for HCL Digital Experience Containers and services - Access the tracing options that can be used to capture logging and tracing for HCL Digital Experience (DX) 9.5 container based services with container update CF181 and higher releases. Parent topic: Operator-based deployment","title":"Troubleshooting cloud container Containers"},{"location":"containerization/troubleshooting_dxclient/","text":"Troubleshooting DXClient Logs can be enabled and disabled as desired by DX developers and administrators through configuration options in the config.json file of DXClient. The log files can be viewed inside the logs folder within the DXClient installation folder. Enable or disable logs Enable logger The DXClient tool Logs can be enabled by setting the parameter enableLogger: true in the config.json file. Disable logger The DXClient tool Logs can be disabled by setting the parameter enableLogger: false in the config.json file. Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Troubleshooting DXClient"},{"location":"containerization/troubleshooting_dxclient/#troubleshooting-dxclient","text":"Logs can be enabled and disabled as desired by DX developers and administrators through configuration options in the config.json file of DXClient. The log files can be viewed inside the logs folder within the DXClient installation folder.","title":"Troubleshooting DXClient"},{"location":"containerization/troubleshooting_dxclient/#enable-or-disable-logs","text":"Enable logger The DXClient tool Logs can be enabled by setting the parameter enableLogger: true in the config.json file. Disable logger The DXClient tool Logs can be disabled by setting the parameter enableLogger: false in the config.json file. Parent topic: Deploy DX components using HCL DXClient and DXConnect","title":"Enable or disable logs"},{"location":"containerization/understanding_kubernetes_eks_deployment/","text":"Understanding the HCL DX 9.5 container deployment on Amazon EKS This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 container release in on Amazon Elastic Kubernetes Service (EKS). Note: All modifications must be done on the custom resource instance and not the individual pieces of the deployment. See Customizing the Kubernetes deployment for details. Deploy with the following command: ./scripts/deployDx.sh dx14 1 AWS_OWNER_ID.dkr.ecr.us-east-2.amazonaws.com dxen v95_CF184_20200916-2009 dxh1-dam dx deploy-stg derby ambassador 154 Results in the following output: Namespace: dx14 - REPLICAS: 1 - Repository: AWS_OWNER_ID.dkr.ecr.us-east-2.amazonaws.com - Image Name: dxen - Image Tag: v95_CF184_20200916-2009 - Volume Name: dxh1-dam - Storage Class Name: dx-deploy-stg - Database Type: derby - namespace/awseks-demo created serviceaccount/hcldx-cloud-operator created role.rbac.authorization.k8s.io/hcldx-cloud-operator created rolebinding.rbac.authorization.k8s.io/hcldx-cloud-operator created deployment.apps/hcldx-cloud-operator created dxdeployment.git.cwp.pnp-hcl.com/dx-deployment created The following artifacts are created during deployment: Artifacts Project/Namespace The Project / Namespace gets created if it does not already exist. The Kubernetes Dashboard can be installed on your local using the following link: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ kubectl get namespaces Service Account Service Accounts for the operator and ambassador deployments. kubectl get serviceaccounts -n dx14 Role A Role that defines the access required by the operator. kubectl get role -n dx14 For more detailed information, use the describe command. Role Binding A Role Binding that binds the operator service account to the operator role. kubectl describe rolebinding -n dx14 Deployment A Deployment that defines/describes the operator Replica Set. kubectl get deployment -n dx14 Replica Sets Replica Sets based on the deployments of the ambassador and the operator, that maintains a stable set of replica pods. kubectl get replicaset -n dx14 Metrics and Services Services expose HCL Digital Experience running as part of the Stateful Set and provide metrics for the operator created. kubectl get service -n dx14 Secrets There are several secrets that are created. kubectl get secrets -n dx14 kubectl describe secret dx-deployment-wps -n dx14 Kubernetes command to update existing secrets: kubectl edit secret dx-deployment-wps -n dx14 Use base64 encoded values when updating secrets. Most Linux operating systems will allow you to generate base64 values with this command: echo 'wpsadmin' | base64 Config Map A Config Map is created to handle clustering in scenarios where multiple operators are deployed. kubectl get configmap -n dx14 Persistent Volume Claims One or more Persistent Volume Claims are created, one for the shared profile, and another, if configured, for the logs of each pod. kubectl get pvc -n dx14 Stateful Set A Stateful Set is created for the HCL Digital Experience core. A Stateful Set manages pods that are based on an identical container specification. kubectl get statefulset -n dx14 Pods Pods are part of the Stateful Set, running HCL Portal. kubectl get pods -l app=dx-deployment -n dx14' (limits to pods in the StatefulSetSpec) kubectl get pods -n dx14 Ambassador To expose the DX server, HCL is leveraging Ambassador . Ambassador is deployed and configured by default. There are many artifacts included. Custom resources that help define Ambassador: AuthService , ConsulResolver , KubernetesEndpointResolver , KubernetesServiceResolver , Mapping , Module , RateLimitService , TCPMapping , TLSContext , TracingService . Ambassador deployment Ambassador replica set Ambassador pods (by default, 3) Ambassador service and Ambassador admin service An instance of TLS context kubectl describe TLSContext -n dx14 Mapping An instance of Mapping for each target: DX Home Secure, WAS Home Secure, etc. kubectl get mapping -n dx14 Extras There are several files created for each deployment. It is recommended to keep these files. File 1 dxNameSpace_NAMESPACE.yaml can be used to delete the namespace / project if needed. File 2 git_v1_dxdeployment_cr_NAMESPACE.yaml is a representation of the last deployed deployment for the given namespace / project . Note: With multiple instances of Digital Experience 9.5 containers writing to a shared Transaction log (tranlog) directory, there is a possibility of data corruption which could lead to DX server startup issues. To avoid this possibility, beginning with CF192, for each DX 9.5 instance the deployment creates an additional dynamic Persistent Volume (pv) and Persistent Volume Claim (pvc) to contain the tranlog data of the individual instances. Parent topic: Deploy DX Container to Amazon EKS","title":"Understanding the HCL DX 9.5 container deployment on Amazon EKS"},{"location":"containerization/understanding_kubernetes_eks_deployment/#understanding-the-hcl-dx-95-container-deployment-on-amazon-eks","text":"This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 container release in on Amazon Elastic Kubernetes Service (EKS). Note: All modifications must be done on the custom resource instance and not the individual pieces of the deployment. See Customizing the Kubernetes deployment for details. Deploy with the following command: ./scripts/deployDx.sh dx14 1 AWS_OWNER_ID.dkr.ecr.us-east-2.amazonaws.com dxen v95_CF184_20200916-2009 dxh1-dam dx deploy-stg derby ambassador 154 Results in the following output: Namespace: dx14 - REPLICAS: 1 - Repository: AWS_OWNER_ID.dkr.ecr.us-east-2.amazonaws.com - Image Name: dxen - Image Tag: v95_CF184_20200916-2009 - Volume Name: dxh1-dam - Storage Class Name: dx-deploy-stg - Database Type: derby - namespace/awseks-demo created serviceaccount/hcldx-cloud-operator created role.rbac.authorization.k8s.io/hcldx-cloud-operator created rolebinding.rbac.authorization.k8s.io/hcldx-cloud-operator created deployment.apps/hcldx-cloud-operator created dxdeployment.git.cwp.pnp-hcl.com/dx-deployment created The following artifacts are created during deployment:","title":"Understanding the HCL DX 9.5 container deployment on Amazon EKS"},{"location":"containerization/understanding_kubernetes_eks_deployment/#artifacts","text":"Project/Namespace The Project / Namespace gets created if it does not already exist. The Kubernetes Dashboard can be installed on your local using the following link: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ kubectl get namespaces Service Account Service Accounts for the operator and ambassador deployments. kubectl get serviceaccounts -n dx14 Role A Role that defines the access required by the operator. kubectl get role -n dx14 For more detailed information, use the describe command. Role Binding A Role Binding that binds the operator service account to the operator role. kubectl describe rolebinding -n dx14 Deployment A Deployment that defines/describes the operator Replica Set. kubectl get deployment -n dx14 Replica Sets Replica Sets based on the deployments of the ambassador and the operator, that maintains a stable set of replica pods. kubectl get replicaset -n dx14 Metrics and Services Services expose HCL Digital Experience running as part of the Stateful Set and provide metrics for the operator created. kubectl get service -n dx14 Secrets There are several secrets that are created. kubectl get secrets -n dx14 kubectl describe secret dx-deployment-wps -n dx14 Kubernetes command to update existing secrets: kubectl edit secret dx-deployment-wps -n dx14 Use base64 encoded values when updating secrets. Most Linux operating systems will allow you to generate base64 values with this command: echo 'wpsadmin' | base64 Config Map A Config Map is created to handle clustering in scenarios where multiple operators are deployed. kubectl get configmap -n dx14 Persistent Volume Claims One or more Persistent Volume Claims are created, one for the shared profile, and another, if configured, for the logs of each pod. kubectl get pvc -n dx14 Stateful Set A Stateful Set is created for the HCL Digital Experience core. A Stateful Set manages pods that are based on an identical container specification. kubectl get statefulset -n dx14 Pods Pods are part of the Stateful Set, running HCL Portal. kubectl get pods -l app=dx-deployment -n dx14' (limits to pods in the StatefulSetSpec) kubectl get pods -n dx14 Ambassador To expose the DX server, HCL is leveraging Ambassador . Ambassador is deployed and configured by default. There are many artifacts included. Custom resources that help define Ambassador: AuthService , ConsulResolver , KubernetesEndpointResolver , KubernetesServiceResolver , Mapping , Module , RateLimitService , TCPMapping , TLSContext , TracingService . Ambassador deployment Ambassador replica set Ambassador pods (by default, 3) Ambassador service and Ambassador admin service An instance of TLS context kubectl describe TLSContext -n dx14 Mapping An instance of Mapping for each target: DX Home Secure, WAS Home Secure, etc. kubectl get mapping -n dx14 Extras There are several files created for each deployment. It is recommended to keep these files. File 1 dxNameSpace_NAMESPACE.yaml can be used to delete the namespace / project if needed. File 2 git_v1_dxdeployment_cr_NAMESPACE.yaml is a representation of the last deployed deployment for the given namespace / project . Note: With multiple instances of Digital Experience 9.5 containers writing to a shared Transaction log (tranlog) directory, there is a possibility of data corruption which could lead to DX server startup issues. To avoid this possibility, beginning with CF192, for each DX 9.5 instance the deployment creates an additional dynamic Persistent Volume (pv) and Persistent Volume Claim (pvc) to contain the tranlog data of the individual instances. Parent topic: Deploy DX Container to Amazon EKS","title":"Artifacts"},{"location":"containerization/understanding_openshift_deployment/","text":"Understanding the OpenShift deployment This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 Docker images to Red Hat OpenShift. Note: All modifications must be done on the custom resource instance and not the individual pieces of the deployment. See Customizing the container deployment for details. Deploy with the following command: ./scripts/deployDx.sh your_namespace 1 your_repository dxen v95_CF184_20200916-2009 dxh1-dam dx deploy-stg derby Results in the following output: Namespace: your_namespace - REPLICAS: 1 - Repository: your_repository - Image Name: dxen - Image Tag: v95_CF184_20200916-2009 - Volume Name: dxh1-dam - Storage Class Name: dx-deploy-stg - Database Type: derby - namespace/your_namespace created serviceaccount/hcldx-cloud-operator created role.rbac.authorization.k8s.io/hcldx-cloud-operator created rolebinding.rbac.authorization.k8s.io/hcldx-cloud-operator created deployment.apps/hcldx-cloud-operator created dxdeployment.git.cwp.pnp-hcl.com/dx-deployment created The following artifacts are created during deployment: Artifacts Note: Before executing OpenShift CLI, we need to set the project: oc project project_name Project/Namespace The Project / Namespace gets created if it does not already exist. Example Kubernetes command: kubectl get namespaces your_namespace Example OpenShift CLI command: oc get project your_namespace Note: dx-latest is the project name in this example. Service Account The operator is scoped to the same namespace as the project namespace. It is also possible to implement multiple DX 9.5 deployments in one namespace, sharing a single operator. Note: The Operator is used to deploy, configure, and manage the HCL DX9.5 deployment to Red Hat OpenShift. Consult the Red Hat OpenShift Operator documentation for more information. Example Kubernetes command: kubectl get serviceaccounts -n your_namespace Example OpenShift CLI command: oc get serviceaccount Role A Role that defines the access required by the operator. Example Kubernetes command: kubectl get role -n your_namespace Example OpenShift CLI command: oc get roles Role Binding A Role Binding that binds the operator service account to the operator role. Example Kubernetes command: kubectl get rolebinding -n your_namespace Example OpenShift CLI command: oc get rolebinding Deployment A Deployment that defines/describes the operator Replica Set. Example Kubernetes command: kubectl get deployment -n your_namespace Example OpenShift CLI command: oc get deployment Replica Sets A Replica Set based on the deployment. This maintains a stable set of replica pods. Example Kubernetes command: kubectl get replicaset -n your_namespace Example OpenShift CLI command: oc get replicasets Metrics and Services Services are created to expose HCL Digital Experience running as part of the Stateful Set and to provide metrics for the operator. Example Kubernetes command: kubectl get service -n your_namespace Example OpenShift CLI command: oc get service Secrets There are several secrets that are created. Example Kubernetes command: kubectl get secrets -n your_namespace Example OpenShift CLI command: oc get secret Example Kubernetes details command: kubectl describe secret dx-deployment-wps -n your_namespace Example Kubernetes update existing secrets command: kubectl edit secret dx-deployment-wps -n your_namespace Use double base64 encoded values when updating secrets. Most Linux operating systems will allow you to generate double base64 values with this command: echo 'wpsadmin' | base64 | base64 Config Map A Config Map is created to handle clustering in scenarios where multiple operators are deployed. Example Kubernetes command: kubectl get configmap -n your_namespace Example OpenShift CLI command: oc get configmap Persistent Volume Claims One or more Persistent Volume Claims are created, one for the shared profile, and another, if configured, for the logs of each pod. Example Kubernetes command: kubectl get pvc -n your_namespace Example OpenShift command: oc get pvc Note: Logs are shown when logging is configured. Stateful Set A Stateful Set is created for the HCL Digital Experience core. A Stateful Set manages pods that are based on an identical container specification. Example Kubernetes command: kubectl get statefulset -n your_namespace Example OpenShift CLI command: oc get statefulset Pods Pods are part of the Stateful Set, running HCL Portal. Example Kubernetes commands: kubectl get pods -n your_namespace kubectl get pods -l app=app_name -n your_namespace Example OpenShift CLI command: oc get pods oc get pods -l app=app_name Routes These are used to expose various ports in the HCL Digital Experience service, including those used to access HCL Portal, the WAS console, and the Config Wizard. Example Kubernetes command: kubectl get routes Example OpenShift CLI command: oc get routes Extras There are several files created for each deployment. It is recommended to keep these files. File 1 'dxNameSpace_NAMESPACE.yaml' can be used to delete the namespace / project if needed. File 2 'git_v1_dxdeployment_cr_NAMESPACE.yaml' is a representation of the last deployed deployment for the given namespace / project . Note: With multiple instances of Digital Experience 9.5 containers writing to a shared Transaction log (tranlog) directory, there is a possibility of data corruption which could lead to DX server startup issues. To avoid this possibility, beginning with CF192, for each DX 9.5 instance the deployment creates an additional dynamic Persistent Volume (pv) and Persistent Volume Claim (pvc) to contain the tranlog data of the individual instances. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Understanding the OpenShift deployment"},{"location":"containerization/understanding_openshift_deployment/#understanding-the-openshift-deployment","text":"This section describes the output and artifacts created when deploying HCL Digital Experience 9.5 Docker images to Red Hat OpenShift. Note: All modifications must be done on the custom resource instance and not the individual pieces of the deployment. See Customizing the container deployment for details. Deploy with the following command: ./scripts/deployDx.sh your_namespace 1 your_repository dxen v95_CF184_20200916-2009 dxh1-dam dx deploy-stg derby Results in the following output: Namespace: your_namespace - REPLICAS: 1 - Repository: your_repository - Image Name: dxen - Image Tag: v95_CF184_20200916-2009 - Volume Name: dxh1-dam - Storage Class Name: dx-deploy-stg - Database Type: derby - namespace/your_namespace created serviceaccount/hcldx-cloud-operator created role.rbac.authorization.k8s.io/hcldx-cloud-operator created rolebinding.rbac.authorization.k8s.io/hcldx-cloud-operator created deployment.apps/hcldx-cloud-operator created dxdeployment.git.cwp.pnp-hcl.com/dx-deployment created The following artifacts are created during deployment:","title":"Understanding the OpenShift deployment"},{"location":"containerization/understanding_openshift_deployment/#artifacts","text":"Note: Before executing OpenShift CLI, we need to set the project: oc project project_name Project/Namespace The Project / Namespace gets created if it does not already exist. Example Kubernetes command: kubectl get namespaces your_namespace Example OpenShift CLI command: oc get project your_namespace Note: dx-latest is the project name in this example. Service Account The operator is scoped to the same namespace as the project namespace. It is also possible to implement multiple DX 9.5 deployments in one namespace, sharing a single operator. Note: The Operator is used to deploy, configure, and manage the HCL DX9.5 deployment to Red Hat OpenShift. Consult the Red Hat OpenShift Operator documentation for more information. Example Kubernetes command: kubectl get serviceaccounts -n your_namespace Example OpenShift CLI command: oc get serviceaccount Role A Role that defines the access required by the operator. Example Kubernetes command: kubectl get role -n your_namespace Example OpenShift CLI command: oc get roles Role Binding A Role Binding that binds the operator service account to the operator role. Example Kubernetes command: kubectl get rolebinding -n your_namespace Example OpenShift CLI command: oc get rolebinding Deployment A Deployment that defines/describes the operator Replica Set. Example Kubernetes command: kubectl get deployment -n your_namespace Example OpenShift CLI command: oc get deployment Replica Sets A Replica Set based on the deployment. This maintains a stable set of replica pods. Example Kubernetes command: kubectl get replicaset -n your_namespace Example OpenShift CLI command: oc get replicasets Metrics and Services Services are created to expose HCL Digital Experience running as part of the Stateful Set and to provide metrics for the operator. Example Kubernetes command: kubectl get service -n your_namespace Example OpenShift CLI command: oc get service Secrets There are several secrets that are created. Example Kubernetes command: kubectl get secrets -n your_namespace Example OpenShift CLI command: oc get secret Example Kubernetes details command: kubectl describe secret dx-deployment-wps -n your_namespace Example Kubernetes update existing secrets command: kubectl edit secret dx-deployment-wps -n your_namespace Use double base64 encoded values when updating secrets. Most Linux operating systems will allow you to generate double base64 values with this command: echo 'wpsadmin' | base64 | base64 Config Map A Config Map is created to handle clustering in scenarios where multiple operators are deployed. Example Kubernetes command: kubectl get configmap -n your_namespace Example OpenShift CLI command: oc get configmap Persistent Volume Claims One or more Persistent Volume Claims are created, one for the shared profile, and another, if configured, for the logs of each pod. Example Kubernetes command: kubectl get pvc -n your_namespace Example OpenShift command: oc get pvc Note: Logs are shown when logging is configured. Stateful Set A Stateful Set is created for the HCL Digital Experience core. A Stateful Set manages pods that are based on an identical container specification. Example Kubernetes command: kubectl get statefulset -n your_namespace Example OpenShift CLI command: oc get statefulset Pods Pods are part of the Stateful Set, running HCL Portal. Example Kubernetes commands: kubectl get pods -n your_namespace kubectl get pods -l app=app_name -n your_namespace Example OpenShift CLI command: oc get pods oc get pods -l app=app_name Routes These are used to expose various ports in the HCL Digital Experience service, including those used to access HCL Portal, the WAS console, and the Config Wizard. Example Kubernetes command: kubectl get routes Example OpenShift CLI command: oc get routes Extras There are several files created for each deployment. It is recommended to keep these files. File 1 'dxNameSpace_NAMESPACE.yaml' can be used to delete the namespace / project if needed. File 2 'git_v1_dxdeployment_cr_NAMESPACE.yaml' is a representation of the last deployed deployment for the given namespace / project . Note: With multiple instances of Digital Experience 9.5 containers writing to a shared Transaction log (tranlog) directory, there is a possibility of data corruption which could lead to DX server startup issues. To avoid this possibility, beginning with CF192, for each DX 9.5 instance the deployment creates an additional dynamic Persistent Volume (pv) and Persistent Volume Claim (pvc) to contain the tranlog data of the individual instances. Parent topic: Deploy DX 9.5 Container to Red Hat OpenShift","title":"Artifacts"},{"location":"containerization/update_config_cc_dam/","text":"Update the HCL DX 9.5 Experience API, Content Composer, and Digital Asset Management components This section provides the steps to update the HCL Digital Experience 9.5 Experience API, HCL Digital Experience 9.5 Content Composer, and HCL Digital Experience 9.5 Digital Asset Management components. Update the HCL Digital Experience 9.5 Experience API, Content Composer, and Digital Asset Management images New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container Images that are available. Customers should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Customers should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. To update the Experience API, Content Composer, and Digital Asset Management images to a later container update version, follow these steps: Note: All images must be moved to the target release. Create a backup of the Digital Asset Management components. Follow instructions to do so outlined in the HCL Digital Experience Containerization Backup and recovery procedures topic. Update the Experience API, Content Composer, and Digital Asset Management component images to a later container update by modifying the image tag for each component to be updated with the later versions tag, as in this example: Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. kind: ConfigMap metadata: name: dx-deployment ``` data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Deploy the YAML ( dx-deploy-config-map.yaml ) with the updated Experience API, Content Composer and Digital Asset Management file names listed by issuing the command: Kubernetes: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace or - ``` kubectl edit configmap name-of-map -n project - OpenShift: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml to perform the update. The deployment will automatically restart a few times and make appropriate configuration changes during these restarts. Once complete, the deployment is upgraded. HCL Digital Experience Solution Feedback HCL Digital Experience is interested in your experience and feedback working with HCL Digital Experience 9.5 release software. To offer comments or issues on your findings, please access the HCL Digital Experience 9.5 Feedback Reporting application . Parent topic: Container administration 9.5","title":"Update the HCL DX 9.5 Experience API, Content Composer, and Digital Asset Management components"},{"location":"containerization/update_config_cc_dam/#update-the-hcl-dx-95-experience-api-content-composer-and-digital-asset-management-components","text":"This section provides the steps to update the HCL Digital Experience 9.5 Experience API, HCL Digital Experience 9.5 Content Composer, and HCL Digital Experience 9.5 Digital Asset Management components.","title":"Update the HCL DX 9.5 Experience API, Content Composer, and Digital Asset Management components"},{"location":"containerization/update_config_cc_dam/#update-the-hcl-digital-experience-95-experience-api-content-composer-and-digital-asset-management-images","text":"New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container Images that are available. Customers should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Customers should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. To update the Experience API, Content Composer, and Digital Asset Management images to a later container update version, follow these steps: Note: All images must be moved to the target release. Create a backup of the Digital Asset Management components. Follow instructions to do so outlined in the HCL Digital Experience Containerization Backup and recovery procedures topic. Update the Experience API, Content Composer, and Digital Asset Management component images to a later container update by modifying the image tag for each component to be updated with the later versions tag, as in this example: Note: It is possible to deploy the services for the HCL Experience API and HCL Content Composer and/or Digital Asset Management by removing either the HCL Content Composer or HCL Digital Asset Management service lines from the YAML file. kind: ConfigMap metadata: name: dx-deployment ``` data: dx.deploy.dam.persistence.tag: v1.0.0_20200622-1806 dx.deploy.dam.persistence.image: portal/persistence/postgres dx.deploy.dam.volume: volume name dx.deploy.dam.imgprocessor.tag: v95_CF181_20200622-1550 dx.deploy.remotesearch.tag: v95_CF181_20200622-1550 dx.deploy.dam.imgprocessor.image: portal/image-processor dx.deploy.dam.storageclass: dx-deploy-stg dx.deploy.remotesearch.image: dxrs dx.deploy.openldap.tag: v1.0.0-release_20200622_1592846796 dx.deploy.openldap.image: dx-openldap dx.deploy.contentui.tag: v1.0.0_20200622-1709 dx.deploy.contentui.image: portal/content-ui dx.deploy.remotesearch.enabled: 'true' dx.deploy.dam.tag: v1.0.0_20200622-1718 dx.deploy.experienceapi.tag: v1.0.0_20200622-1719 dx.deploy.experienceapi.image: portal/api/ringapi dx.deploy.dam.image: portal/media-library dx.deploy.openldap.enabled: 'true' dx.deploy.contentui.enabled: 'true' dx.deploy.experienceapi.enabled: 'true' dx.deploy.dam.enabled: 'true' dx.deploy.dam.operator.tag: v95_CF181_20200622-1756 dx.deploy.dam.operator.image: hcl-medialibrary-operator dx.deploy.remotesearch.volume.storageclass: gp2 ``` Deploy the YAML ( dx-deploy-config-map.yaml ) with the updated Experience API, Content Composer and Digital Asset Management file names listed by issuing the command: Kubernetes: ``` kubectl apply -f dx-deploy-config-map.yaml -n your-namespace or - ``` kubectl edit configmap name-of-map -n project - OpenShift: - ``` oc project your-namespace followed by - ``` oc apply -f dx-deploy-config-map.yaml to perform the update. The deployment will automatically restart a few times and make appropriate configuration changes during these restarts. Once complete, the deployment is upgraded.","title":"Update the HCL Digital Experience 9.5 Experience API, Content Composer, and Digital Asset Management images"},{"location":"containerization/update_config_cc_dam/#hcl-digital-experience-solution-feedback","text":"HCL Digital Experience is interested in your experience and feedback working with HCL Digital Experience 9.5 release software. To offer comments or issues on your findings, please access the HCL Digital Experience 9.5 Feedback Reporting application . Parent topic: Container administration 9.5","title":"HCL Digital Experience Solution Feedback"},{"location":"containerization/update_dx_core_kubernetes_container_deployment/","text":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment Update the Digital Experience 9.5 Core Kubernetes container deployment. Follow the processes below to create a backup, then update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift Container Deployment to a later Container Update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container images that are available. HCL DX administrators should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Note: HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only . DX administrators should not upgrade DX 9.5 container deployments to this release. HCL DX administrators should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. It is recommended you create a backup of your DX 9.5 deployment before managing the update processes using the following steps: Create a backup of the wp_profile . To backup the wp_profile , it is recommended that the number of instances is 1 instance, and your Digital Experience 9.5 Container deployment is stopped using the following command: /opt/HCL/wp_profile/stopServer.sh Next, ensure the entire /opt/HCL/wp_profile directory is backed up. A method to generate this backup is shown using the following commands: cd /opt/HCL/wp_profile\u2028 tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile Note: Before starting the tar ensure that your file system has ~50% free capacity. \u2028Once complete, it is recommended that you copy the backup.tar.gz file that is generated to alternate long term storage. In addition, as outlined in the Backup and Recovery topic, the Digital Experience 9.5 database should be backed up at the same time as the wp_profile . Create a backup of the Persistence layer using the following example commands: pg_dump name_of_database > name_of_backup_file. We recommend backing up the system on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file. If it is done locally, you need to execute into the POD. Once you have completed the command, it is recommended that a copy of the resulting file is created and placed to an alternate long-term storage. Update the HCL Digital Experience 9.5 core deployment files: Note: Beginning with HCL DX 9.5 Container Update CF192, the dxctl process is used to manage updates to later update releases. See the following deployment topics below for the update instructions details. Documentation resource: Deploy HCL Digital Experience 9.5 Container to Red Hat OpenShift Documentation resource: Deploy HCL Digital Experience 9.5 Container to Amazon EKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Microsoft Azure AKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Video: Using dxctl to update HCL DX 9.5 on Red Hat OpenShift to Container Update CF192 Video: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Download the later version of the HCL DX 9.5 Container Update packages to update from the HCL Software License Portal. Consult the Digital Experience 9.5 Container Deployment topic for the latest list. Load, tag, and push the later version DX 9.5 Container images to your supported Kubernetes or OpenShift platform (similar to steps followed with your original HCL DX 9.5 image detailed in the HCL DX 9.5 Help Center Deployment topics). Use the following guidance and steps if managing an update to Container Update versions prior to CF192: To upgrade the deployment to a new version, update the IMAGETAG value that was used in the original deployDx.sh execution to the new IMAGETAG . Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Update the tag in the operator.yaml file to the later HCL DX 9.5 Container Update versions tag. Note: If updating your DX 9.5 container deployment to CF19, complete the following steps before proceeding to step 7. Delete the DxDeployment CRD. This terminates all deployments. OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com Release claim on the wp_profile persistent volume to make it available: OpenShift command: oc edit pv VOLUME Kubernetes command: kubectl edit pv VOLUME Delete the claimRef section: ``` Example: claimRef: kind: PersistentVolumeClaim namespace: dx-ns name: dx-deployment-pvc uid: 633c67f9-89fe-4ac8-8db1-929ccbb8a657 apiVersion: v1 resourceVersion: '658831' ``` Create the CF19 DxDeployment CRD: OpenShift command: oc create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the HCL Digital Experience 9.5 core deployment files to the later Container Update version, run the updateDx.sh script with updated values, as shown in the following examples: Kubernetes command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG OpenShift command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/ or supported Kubernetes platforms, Amazon EKS or MS Azure AKs IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. INGRESSIMAGE - The image name to use for ambassador. INGRESSTAG - The image tag to use for ambassador. After the updateDx.sh script has completed, DX administrators are encouraged to check the following log files to ensure the CF update completed successfully. Check the ConfigTrace.log located under the /opt/HCL/wp_profile/ConfigEngine/log/ directory to ensure the update task was successful, as shown in this example: [2020-08-26 19:53:28.658] Target finished: action-apply-cf BUILD SUCCESSFUL 2. Review for any exceptions in the SystemOut.log located under the /opt/HCL/wp\\_profile/logs/WebSphere\\_Portal directory. **Additional steps required for HCL DX 9.5 deployments to supported Kubernetes platforms: Amazon EKS or Microsoft Azure AKS.** Administrators must perform delete/deploy or redeploy the ambassador definitions. This can be done by performing these commands, operating on an MS Azure AKS environment in these examples: kubectl delete crd tracingservices.getambassador.io -n az-demo kubectl delete crd tlscontexts.getambassador.io -n az-demo kubectl delete crd tcpmappings.getambassador.io -n az-demo kubectl delete crd ratelimitservices.getambassador.io -n az-demo kubectl delete crd ratelimits.getambassador.io -n az-demo kubectl delete crd projectversions.getambassador.io -n az-demo kubectl delete crd projects.getambassador.io -n az-demo kubectl delete crd projectsrevisions.getambassador.io -n az-demo kubectl delete crd modules.getambassador.io -n az-demo kubectl delete crd mappings.getambassador.io -n az-demo kubectl delete crd logservices.getambassador.io -n az-demo kubectl delete crd authservices.getambassador.io -n az-demo kubectl delete crd consulresolvers.getambassador.io -n az-demo kubectl delete crd hosts.getambassador.io -n az-demo kubectl delete crd kubernetesserviceresolvers.getambassador.io -n az-demo kubectl delete crd kubernetesendpointresolvers.getambassador.io -n az-demo Upon completion, these are automatically redeployed at version 1 and version 2, provided you have an active deployment. If not, they are redeployed once the HCL DX 9.5 DX is deployed. The previous ambassador version, prior to CF183 at level 0.85.0, is deployed and uses the ambassador version 1 APIs. There are additional options to customize the deployment . For example, once the database is transferred to a non-Derby database, the DBTYPE must updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas can be increased. During the Update process, the deployment automatically restarts a few times and make appropriate configuration changes during these restarts. Once complete the deployment is upgraded. For instructions to update the Content Composer, Digital Asset Management, and Experience API container deployment images, see the following topics. Documentation resource: Install the Experience API, Content Composer, and DAM Components Documentation resource: Update the Experience API, Content Composer, and DAM Components Instructions to Delete a DX 9.5 Container Deployment Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the following: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: OpenShift commands: 'oc delete project **<project\\_name\\>**' 'oc delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Container administration 9.5","title":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment"},{"location":"containerization/update_dx_core_kubernetes_container_deployment/#update-the-digital-experience-95-core-kubernetes-or-red-hat-openshift-container-deployment","text":"Update the Digital Experience 9.5 Core Kubernetes container deployment. Follow the processes below to create a backup, then update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift Container Deployment to a later Container Update release. New HCL Digital Experience 9.5 CFxxx container images are released on a regular cadence, through the HCL DX 9.5 Container Update deliveries . Consult the Digital Experience 9.5 Container Deployment topic for the latest list of DX 9.5 Container images that are available. HCL DX administrators should not apply maintenance to an HCL Digital Experience 9.5 container image. Instead, they should run the update process as described below. Note: HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only . DX administrators should not upgrade DX 9.5 container deployments to this release. HCL DX administrators should not extend the HCL Digital Experience 9.5 container images. They are not intended to be used in the FROM instruction as a parent image. It is recommended you create a backup of your DX 9.5 deployment before managing the update processes using the following steps: Create a backup of the wp_profile . To backup the wp_profile , it is recommended that the number of instances is 1 instance, and your Digital Experience 9.5 Container deployment is stopped using the following command: /opt/HCL/wp_profile/stopServer.sh Next, ensure the entire /opt/HCL/wp_profile directory is backed up. A method to generate this backup is shown using the following commands: cd /opt/HCL/wp_profile\u2028 tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /opt/HCL/wp_profile Note: Before starting the tar ensure that your file system has ~50% free capacity. \u2028Once complete, it is recommended that you copy the backup.tar.gz file that is generated to alternate long term storage. In addition, as outlined in the Backup and Recovery topic, the Digital Experience 9.5 database should be backed up at the same time as the wp_profile . Create a backup of the Persistence layer using the following example commands: pg_dump name_of_database > name_of_backup_file. We recommend backing up the system on a remote system: pg_dump -U user_name -h remote_host -p remote_port name_of_database > name_of_backup_file. If it is done locally, you need to execute into the POD. Once you have completed the command, it is recommended that a copy of the resulting file is created and placed to an alternate long-term storage. Update the HCL Digital Experience 9.5 core deployment files: Note: Beginning with HCL DX 9.5 Container Update CF192, the dxctl process is used to manage updates to later update releases. See the following deployment topics below for the update instructions details. Documentation resource: Deploy HCL Digital Experience 9.5 Container to Red Hat OpenShift Documentation resource: Deploy HCL Digital Experience 9.5 Container to Amazon EKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Microsoft Azure AKS Documentation resource: Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Video: Using dxctl to update HCL DX 9.5 on Red Hat OpenShift to Container Update CF192 Video: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Download the later version of the HCL DX 9.5 Container Update packages to update from the HCL Software License Portal. Consult the Digital Experience 9.5 Container Deployment topic for the latest list. Load, tag, and push the later version DX 9.5 Container images to your supported Kubernetes or OpenShift platform (similar to steps followed with your original HCL DX 9.5 image detailed in the HCL DX 9.5 Help Center Deployment topics). Use the following guidance and steps if managing an update to Container Update versions prior to CF192: To upgrade the deployment to a new version, update the IMAGETAG value that was used in the original deployDx.sh execution to the new IMAGETAG . Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Update the tag in the operator.yaml file to the later HCL DX 9.5 Container Update versions tag. Note: If updating your DX 9.5 container deployment to CF19, complete the following steps before proceeding to step 7. Delete the DxDeployment CRD. This terminates all deployments. OpenShift command: oc delete crd dxdeployments.git.cwp.pnp-hcl.com Kubernetes command: kubectl delete crd dxdeployments.git.cwp.pnp-hcl.com Release claim on the wp_profile persistent volume to make it available: OpenShift command: oc edit pv VOLUME Kubernetes command: kubectl edit pv VOLUME Delete the claimRef section: ``` Example: claimRef: kind: PersistentVolumeClaim namespace: dx-ns name: dx-deployment-pvc uid: 633c67f9-89fe-4ac8-8db1-929ccbb8a657 apiVersion: v1 resourceVersion: '658831' ``` Create the CF19 DxDeployment CRD: OpenShift command: oc create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml Kubernetes command: kubectl create -f deploy/crd/git.cwp.pnp-hcl.com_dxdeployments_crd.yaml To update the HCL Digital Experience 9.5 core deployment files to the later Container Update version, run the updateDx.sh script with updated values, as shown in the following examples: Kubernetes command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE INGRESSIMAGE INGRESSTAG OpenShift command: ./scripts/updateDx.sh NAMESPACE REPLICAS REPOSITORY IMAGENAME IMAGETAG VOLUMENAME STORAGECLASS DBTYPE NAMESPACE - the project or the namespace to create or use for deployment. REPLICAS - the number of initial instances for the deployment. REPOSITORY - your local repository, the repository used by OpenShift/ or supported Kubernetes platforms, Amazon EKS or MS Azure AKs IMAGENAME - the name of the dxCore image, as added to the repository above. IMAGETAG - the tag for the target image as added to the above repository. VOLUMENAME - the volume to be used by the deployment for persistence, this must use AccessMode ReadWriteMany . STORAGECLASS - the storage class name used to create the persistent volume. DBTYPE - the database type. By default, and initially, this is Derby. INGRESSIMAGE - The image name to use for ambassador. INGRESSTAG - The image tag to use for ambassador. After the updateDx.sh script has completed, DX administrators are encouraged to check the following log files to ensure the CF update completed successfully. Check the ConfigTrace.log located under the /opt/HCL/wp_profile/ConfigEngine/log/ directory to ensure the update task was successful, as shown in this example: [2020-08-26 19:53:28.658] Target finished: action-apply-cf BUILD SUCCESSFUL 2. Review for any exceptions in the SystemOut.log located under the /opt/HCL/wp\\_profile/logs/WebSphere\\_Portal directory. **Additional steps required for HCL DX 9.5 deployments to supported Kubernetes platforms: Amazon EKS or Microsoft Azure AKS.** Administrators must perform delete/deploy or redeploy the ambassador definitions. This can be done by performing these commands, operating on an MS Azure AKS environment in these examples: kubectl delete crd tracingservices.getambassador.io -n az-demo kubectl delete crd tlscontexts.getambassador.io -n az-demo kubectl delete crd tcpmappings.getambassador.io -n az-demo kubectl delete crd ratelimitservices.getambassador.io -n az-demo kubectl delete crd ratelimits.getambassador.io -n az-demo kubectl delete crd projectversions.getambassador.io -n az-demo kubectl delete crd projects.getambassador.io -n az-demo kubectl delete crd projectsrevisions.getambassador.io -n az-demo kubectl delete crd modules.getambassador.io -n az-demo kubectl delete crd mappings.getambassador.io -n az-demo kubectl delete crd logservices.getambassador.io -n az-demo kubectl delete crd authservices.getambassador.io -n az-demo kubectl delete crd consulresolvers.getambassador.io -n az-demo kubectl delete crd hosts.getambassador.io -n az-demo kubectl delete crd kubernetesserviceresolvers.getambassador.io -n az-demo kubectl delete crd kubernetesendpointresolvers.getambassador.io -n az-demo Upon completion, these are automatically redeployed at version 1 and version 2, provided you have an active deployment. If not, they are redeployed once the HCL DX 9.5 DX is deployed. The previous ambassador version, prior to CF183 at level 0.85.0, is deployed and uses the ambassador version 1 APIs. There are additional options to customize the deployment . For example, once the database is transferred to a non-Derby database, the DBTYPE must updated so you can scale the instances higher. Additionally, once the database is transferred, the number of replicas can be increased. During the Update process, the deployment automatically restarts a few times and make appropriate configuration changes during these restarts. Once complete the deployment is upgraded. For instructions to update the Content Composer, Digital Asset Management, and Experience API container deployment images, see the following topics. Documentation resource: Install the Experience API, Content Composer, and DAM Components Documentation resource: Update the Experience API, Content Composer, and DAM Components","title":"Update the Digital Experience 9.5 Core Kubernetes or Red Hat OpenShift container deployment"},{"location":"containerization/update_dx_core_kubernetes_container_deployment/#instructions-to-delete-a-dx-95-container-deployment","text":"Removing the entire deployment requires several steps, this is by design. To remove the deployment in a specific namespace, run the following: ./scripts/removeDx.sh NAMESPACE NAMESPACE - the project or the namespace created or used for deployment. To remove a namespace, use any of the following commands: OpenShift commands: 'oc delete project **<project\\_name\\>**' 'oc delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed Kubernetes command: 'kubectl delete -f dxNameSpace_**NAMESPACE**.yaml' where **NAMESPACE** is the namespace to be removed The persistent volume associated to the deployment needs to be cleaned up by your Administrator. To reuse a persistent volume, see the following steps: Open the persistent volume in a visual editor (vi) using the: OpenShift command: oc edit pv your_namespace Kubernetes command: kubectl edit pv your_namespace Remove the claimRef section: claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dx-deploy-pvc namespace: your_namespace resourceVersion: \"488931120\" uid: ebd58361-0e2a-11ea-b02e-02f8fe687954 Ensure you get the 'persistentvolume/your_namespace edited' message. You may need to manually remove any data remaining from the previous deployment. Parent topic: Container administration 9.5","title":"Instructions to Delete a DX 9.5 Container Deployment"},{"location":"containerization/update_helm_deployment/","text":"Update Helm deployment configuration This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. This section assumes that you prepared your cluster and your custom-values.yaml file, using guidance provided in the Planning your HCL DX 9.5 container deployment using Helm topic, and then installed your deployment using the instructions in the Install topic. Overview of Helm Configuration Updates Once an HCL Digital Experience Kubernetes 9.5 deployment is installed, it is possible to update its configuration directly using the standard Kubernetes or OpenShift commands (for example, by updating values in the various config maps). However, this is NOT the recommended approach. Some of the configuration parameters have interdependencies, as outlined in the Planning section . These require knowledgeable management to make changes that are compatible with interdependency requirements. For example, if you change the context root for DX Core you also need to change the readiness and liveness probes. The recommended approach for configuration changes is to update the custom-values.yaml file used to install the deployment, and then run a Helm upgrade. This has the added benefit that your custom-values.yaml file remains an up-to-date description of the configuration of your environment. Helm Upgrade configuration command After making the needed changes to your custom-values.yaml file, use the following command: ``` Helm upgrade command helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` The your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you have updated. The path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience Helm Chart that you extracted in the preparation steps. Parent topic: Operations using Helm","title":"Update Helm deployment configuration"},{"location":"containerization/update_helm_deployment/#update-helm-deployment-configuration","text":"This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. This section assumes that you prepared your cluster and your custom-values.yaml file, using guidance provided in the Planning your HCL DX 9.5 container deployment using Helm topic, and then installed your deployment using the instructions in the Install topic. Overview of Helm Configuration Updates Once an HCL Digital Experience Kubernetes 9.5 deployment is installed, it is possible to update its configuration directly using the standard Kubernetes or OpenShift commands (for example, by updating values in the various config maps). However, this is NOT the recommended approach. Some of the configuration parameters have interdependencies, as outlined in the Planning section . These require knowledgeable management to make changes that are compatible with interdependency requirements. For example, if you change the context root for DX Core you also need to change the readiness and liveness probes. The recommended approach for configuration changes is to update the custom-values.yaml file used to install the deployment, and then run a Helm upgrade. This has the added benefit that your custom-values.yaml file remains an up-to-date description of the configuration of your environment. Helm Upgrade configuration command After making the needed changes to your custom-values.yaml file, use the following command: ```","title":"Update Helm deployment configuration"},{"location":"containerization/update_helm_deployment/#helm-upgrade-command","text":"helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` The your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you have updated. The path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience Helm Chart that you extracted in the preparation steps. Parent topic: Operations using Helm","title":"Helm upgrade command"},{"location":"containerization/virtualportals/","text":"Managing virtual portals This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals. Virtual Portal commands Command description The manage-virtual-portal command is used to manage virtual portal tasks such as create, list, export, and import in the DX server. ``` dxclient manage-virtual-portal ``` Help command This command shows the help document on the manage-virtual-portal command: Help command for creating virtual portals: dxclient manage-virtual-portal create -h Help command for listing virtual portals: dxclient manage-virtual-portal list -h Help command for importing virtual portals: dxclient manage-virtual-portal import -h Help command for exporting virtual portals: dxclient manage-virtual-portal export -h Subcommands Create virtual portal task in the DX server: manage-virtual-portal create [OPTIONS] List virtual portal task in the DX server: manage-virtual-portal list [OPTIONS] Import virtual portal task in the DX server: manage-virtual-portal import [OPTIONS] Export virtual portal task in the DX server: manage-virtual-portal export [OPTIONS] Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered create virtual portal request> Required Commands manage-virtual-portal create command: Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this parameter to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this parameter to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this parameter to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this parameter to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this parameter to specify the username of the DX WAS server -dxWASUsername <value> Use this parameter to specify the password of the DX WAS server -dxWASPassword <value> Use this parameter to specify the virtual portal Title -vpTitle <value> Use this parameter to specify the virtual portal Realm -vpRealm <value> Use this parameter to specify the virtual portal AdminGroup -vpAdminGroup <value> Use this parameter to specify the virtual portal HostName -vpHostname <value> Use this parameter to specify the virtual portal Context -vpContext <value> Note: Create virtual portal task creates an empty virtual portal in the DX server. Example usage: dxclient manage-virtual-portal create -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX Server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername < Username of the DX WAS server> -dxWASPassword <Password of the DX WAS server> -vpTitle <virtual-portal-Title> -vpRealm <virtual-portal-realm> -vpAdminGroup <virtual-portal-adminGroup> -vpHostname <virtual-portal-hostname> -vpContext<virtual-portal-context> manage-virtual-portal list command Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this parameter to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this parameter to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this parameter to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this parameter to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this parameter to specify the username of the DX WAS server -dxWASUsername <value> Use this parameter to specify the password of the DX WAS server -dxWASPassword <value> Example usage: dxclient manage-virtual-portal list -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <profile-name-of-the-DX-server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername <username-of-the-DX-WAS-server> -dxWASPassword <password-of-the-DX-WAS-server> manage-virtual-portal import command: Use this parameter to specify the protocol with which to connect to the server -dxProtocol <value> Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the hostname of the target server -hostname <value> Use this parameter to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this parameter to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this parameter to specify the XML file name with absolute path of the input file -xmlFile <value> Use this parameter to specify the virtual portal Context -vpContext <value> Limitation: Currently, import virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with host name might be added in the future release. Example Usage: dxclient manage-virtual-portal import -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -vpContext <virtual-portal-context> manage-virtual-portal export command: Use this parameter to specify the protocol with which to connect to the server -dxProtocol <value> Use this parameter to specify the user name that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the host name of the target server -hostname <value> Use this parameter to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this parameter to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this parameter to specify the virtual portal Context -vpContext <value> Use this parameter to specify the virtual portal Title -vpTitle <value> Use this parameter to specify the XML file name with absolute path of the input file to export the virtual portal content. -xmlFile <value> Limitation: Currently, exporting virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example: dxclient manage-virtual-portal export -hostname <hostname> -dxProtocol <dxProtocol> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -vpTitle <vpTitle> -vpContext <vpContext> -xmlFile <xml-file-with-path> Log files from running the command can be found in the logs directory of the DXClient installation. Parent topic: DXClient Artifact Types","title":"Managing virtual portals"},{"location":"containerization/virtualportals/#managing-virtual-portals","text":"This topic describes the commands that are used in managing the virtual portal activities such as creating, listing, importing, or exporting virtual portals.","title":"Managing virtual portals"},{"location":"containerization/virtualportals/#virtual-portal-commands","text":"Command description The manage-virtual-portal command is used to manage virtual portal tasks such as create, list, export, and import in the DX server. ``` dxclient manage-virtual-portal ``` Help command This command shows the help document on the manage-virtual-portal command: Help command for creating virtual portals: dxclient manage-virtual-portal create -h Help command for listing virtual portals: dxclient manage-virtual-portal list -h Help command for importing virtual portals: dxclient manage-virtual-portal import -h Help command for exporting virtual portals: dxclient manage-virtual-portal export -h Subcommands Create virtual portal task in the DX server: manage-virtual-portal create [OPTIONS] List virtual portal task in the DX server: manage-virtual-portal list [OPTIONS] Import virtual portal task in the DX server: manage-virtual-portal import [OPTIONS] Export virtual portal task in the DX server: manage-virtual-portal export [OPTIONS] Use this attribute and retrigger the command to check the status of any previous request that was incomplete. -requestId <Unique ID of a previously triggered create virtual portal request> Required Commands manage-virtual-portal create command: Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this parameter to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this parameter to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this parameter to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this parameter to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this parameter to specify the username of the DX WAS server -dxWASUsername <value> Use this parameter to specify the password of the DX WAS server -dxWASPassword <value> Use this parameter to specify the virtual portal Title -vpTitle <value> Use this parameter to specify the virtual portal Realm -vpRealm <value> Use this parameter to specify the virtual portal AdminGroup -vpAdminGroup <value> Use this parameter to specify the virtual portal HostName -vpHostname <value> Use this parameter to specify the virtual portal Context -vpContext <value> Note: Create virtual portal task creates an empty virtual portal in the DX server. Example usage: dxclient manage-virtual-portal create -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <Profile name of the DX Server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername < Username of the DX WAS server> -dxWASPassword <Password of the DX WAS server> -vpTitle <virtual-portal-Title> -vpRealm <virtual-portal-realm> -vpAdminGroup <virtual-portal-adminGroup> -vpHostname <virtual-portal-hostname> -vpContext<virtual-portal-context> manage-virtual-portal list command Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the configuration wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to the cw_profile -dxConnectHostname <value> Use this parameter to specify the port number of the cw_profile(for Kubernetes Environment dxConnectPort is 443) -dxConnectPort <value> Use this parameter to specify the username that is required for authenticating to the cw_profile -dxConnectUsername <value> Use this parameter to specify the password that is required for authenticating to the cw_profile -dxConnectPassword <value> Use this parameter to specify the profile name of the DX core server -dxProfileName <Profile name of the DX core server> Use this parameter to specify the username of the DX WAS server -dxWASUsername <value> Use this parameter to specify the password of the DX WAS server -dxWASPassword <value> Example usage: dxclient manage-virtual-portal list -dxConnectHostname <dxConnectHostname> -dxConnectPort <dxConnectPort> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxProfileName <profile-name-of-the-DX-server> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxWASUsername <username-of-the-DX-WAS-server> -dxWASPassword <password-of-the-DX-WAS-server> manage-virtual-portal import command: Use this parameter to specify the protocol with which to connect to the server -dxProtocol <value> Use this parameter to specify the username that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the hostname of the target server -hostname <value> Use this parameter to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this parameter to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this parameter to specify the XML file name with absolute path of the input file -xmlFile <value> Use this parameter to specify the virtual portal Context -vpContext <value> Limitation: Currently, import virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with host name might be added in the future release. Example Usage: dxclient manage-virtual-portal import -dxProtocol <http/https> -hostname <host-name> -dxPort <dxPort> -xmlConfigPath <xmlConfigPath> -dxUsername <dxUsername> -dxPassword <dxPassword> -xmlFile <xml-file-with-path> -vpContext <virtual-portal-context> manage-virtual-portal export command: Use this parameter to specify the protocol with which to connect to the server -dxProtocol <value> Use this parameter to specify the user name that is required for authenticating with the server -dxUsername <value> Use this parameter to specify the password that is required for authenticating with the server -dxPassword <value> Use this parameter to specify the host name of the target server -hostname <value> Use this parameter to specify the port on which to connect to the server(for Kubernetes Environment dxPort is 443) -dxPort <value> Use this parameter to specify the path to DX configuration endpoint (for example: /wps/config) -xmlConfigPath <value> Use this parameter to specify the virtual portal Context -vpContext <value> Use this parameter to specify the virtual portal Title -vpTitle <value> Use this parameter to specify the XML file name with absolute path of the input file to export the virtual portal content. -xmlFile <value> Limitation: Currently, exporting virtual portal feature supports only vpContext and does not support vpHostname . Support for Virtual portal with hostname might be added in the future release. Example: dxclient manage-virtual-portal export -hostname <hostname> -dxProtocol <dxProtocol> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -vpTitle <vpTitle> -vpContext <vpContext> -xmlFile <xml-file-with-path> Log files from running the command can be found in the logs directory of the DXClient installation. Parent topic: DXClient Artifact Types","title":"Virtual Portal commands"},{"location":"containerization/wcmlibraries/","text":"Exporting and importing WCM libraries This section provides information about how to export and import WCM libraries using DXClient. Note: WCM export and import supports the WCM JCR nodes format. Export WCM libraries The wcm-library-export command is used to export the WCM libraries from the source server to an output location in the <working-directory>/store/ folder. Command description ``` dxclient wcm-library-export ``` Help command This command shows the help document about the wcm-library-export command: dxclient wcm-library-export -h Command options Use this attribute to specify the user name that is required to authenticate to the server: -dxUsername <value> Use this attribute to specify the password that is required to authenticate to the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as the hostname) that is required for authenticating to the cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile : -dxConnectPassword <value> Use this attribute to specify the user name of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the names of the WCM libraries of the DX core server (for example, \"hello_library,demo_library\"): -librariesName <value> Use this attribute to specify the export libraries. This value can be either true or false. If the value is true, then export the all libraries: -exportAllLibraries <value> Log files from running the command can be found in the logs directory of the DXClient installation. Example: dxclient wcm-library-export -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxWASUsername <dxWASUsername> -dxProfileName <dxProfileName> -librariesName <librariesName> Import WCM libraries The wcm-library-import command is used to import the WCM libraries from the source server to the target server. Limitation: Ensure that the WCM import.zip file size is not more than 100MB. This limitation will be addressed in the future release. Command description ``` dxclient wcm-library-import ``` Help command Use this attribute to specify the user name that is required to authenticate to the server: -dxUsername <value> Use this attribute to specify the password that is required to authenticate to the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to cw_profile : -dxConnectPassword <value> Use this attribute to specify the user name of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the path to a zip file or folder that contains the WCM libraries: -libFilePath <value> Log files from running the command can be found in the logs directory of the DXClient installation. Example: dxclient wcm-library-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxWASUsername <dxWASUsername> -dxProfileName <dxProfileName> -libFilePath <libFilePath> Parent topic: DXClient Artifact Types","title":"Exporting and importing WCM libraries"},{"location":"containerization/wcmlibraries/#exporting-and-importing-wcm-libraries","text":"This section provides information about how to export and import WCM libraries using DXClient. Note: WCM export and import supports the WCM JCR nodes format.","title":"Exporting and importing WCM libraries"},{"location":"containerization/wcmlibraries/#export-wcm-libraries","text":"The wcm-library-export command is used to export the WCM libraries from the source server to an output location in the <working-directory>/store/ folder. Command description ``` dxclient wcm-library-export ``` Help command This command shows the help document about the wcm-library-export command: dxclient wcm-library-export -h Command options Use this attribute to specify the user name that is required to authenticate to the server: -dxUsername <value> Use this attribute to specify the password that is required to authenticate to the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as the hostname) that is required for authenticating to the cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to the cw_profile : -dxConnectPassword <value> Use this attribute to specify the user name of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the names of the WCM libraries of the DX core server (for example, \"hello_library,demo_library\"): -librariesName <value> Use this attribute to specify the export libraries. This value can be either true or false. If the value is true, then export the all libraries: -exportAllLibraries <value> Log files from running the command can be found in the logs directory of the DXClient installation. Example: dxclient wcm-library-export -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxWASUsername <dxWASUsername> -dxProfileName <dxProfileName> -librariesName <librariesName>","title":"Export WCM libraries"},{"location":"containerization/wcmlibraries/#import-wcm-libraries","text":"The wcm-library-import command is used to import the WCM libraries from the source server to the target server. Limitation: Ensure that the WCM import.zip file size is not more than 100MB. This limitation will be addressed in the future release. Command description ``` dxclient wcm-library-import ``` Help command Use this attribute to specify the user name that is required to authenticate to the server: -dxUsername <value> Use this attribute to specify the password that is required to authenticate to the server: -dxPassword <value> Use this attribute to specify the config wizard home (route change only in case of Open Shift Kubernetes Environment, otherwise same as hostname) that is required for authenticating to cw_profile : -dxConnectHostname <value> Use this attribute to specify the port number of the cw_profile (for Kubernetes Environment, dxConnectPort is 443): -dxConnectPort <value> Use this attribute to specify the user name that is required for authenticating to the cw_profile : -dxConnectUsername <value> Use this attribute to specify the password that is required for authenticating to cw_profile : -dxConnectPassword <value> Use this attribute to specify the user name of the DX WAS server: -dxWASUsername <value> Use this attribute to specify the password of the DX WAS server: -dxWASPassword <value> Use this attribute to specify the profile name of the DX core server: -dxProfileName <Profile name of the DX core server> Use this attribute to specify the path to a zip file or folder that contains the WCM libraries: -libFilePath <value> Log files from running the command can be found in the logs directory of the DXClient installation. Example: dxclient wcm-library-import -dxProtocol <dxProtocol> -hostname <hostname> -dxPort <dxPort> -dxUsername <dxUsername> -dxPassword <dxPassword> -dxConnectHostname <dxConnectHostname> -dxConnectUsername <dxConnectUsername> -dxConnectPassword <dxConnectPassword> -dxWASUsername <dxWASUsername> -dxProfileName <dxProfileName> -libFilePath <libFilePath> Parent topic: DXClient Artifact Types","title":"Import WCM libraries"},{"location":"containerization/xmlaccess/","text":"XML Access This topic provides information about the xmlaccess command that is used to export or import portlet configurations. XML Access The xmlaccess command is used to export or import pages or portlet configurations from a target HCL DX 9.5 CF19 or later server using the input XMLAccess file. Required file XMLAccess file : This XML file must contain the configuration update or export operation for the web application. Command dxclient xmlaccess -xmlFile <path> Help command This command shows the help information for xmlaccess command usage: dxclient xmlaccess -h Command options Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to xmlaccess input file> Command options passed through the command line overrides values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient xmlaccess -xmlFile <xml-file-with-path> Parent topic: DXClient Artifact Types","title":"XML Access"},{"location":"containerization/xmlaccess/#xml-access","text":"This topic provides information about the xmlaccess command that is used to export or import portlet configurations.","title":"XML Access"},{"location":"containerization/xmlaccess/#xml-access_1","text":"The xmlaccess command is used to export or import pages or portlet configurations from a target HCL DX 9.5 CF19 or later server using the input XMLAccess file. Required file XMLAccess file : This XML file must contain the configuration update or export operation for the web application. Command dxclient xmlaccess -xmlFile <path> Help command This command shows the help information for xmlaccess command usage: dxclient xmlaccess -h Command options Use this attribute to specify the protocol with which to connect to the DX server (wp_profile): -dxProtocol <value> Use this attribute to specify the hostname of the target DX server: -hostname <value> Use this attribute to specify the port on which to connect to the DX server ( wp_profile ): -dxPort <value> Use this attribute to specify the path to DX configuration endpoint (e.g. /wps/config): -xmlConfigPath <value> Use this attribute to specify the username to authenticate with the DX server ( wp_profile ): -dxUsername <value> Use this attribute to specify the password for the user in the dxUsername attribute: -dxPassword <value> Use this attribute to specify the local path to the XMLAccess file: -xmlFile <Absolute or relative path to xmlaccess input file> Command options passed through the command line overrides values set in the config.json file. Log files from command execution can be found in the logs directory of the DXClient installation. Example: dxclient xmlaccess -xmlFile <xml-file-with-path> Parent topic: DXClient Artifact Types","title":"XML Access"},{"location":"containerization/docker/docker/","text":"Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. Docker container update file list The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF201 Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table. HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"Docker image list"},{"location":"containerization/docker/docker/#docker-image-list","text":"This section presents the latest HCL DX 9.5 Docker container update images available.","title":"Docker image list"},{"location":"containerization/docker/docker/#docker-container-update-file-list","text":"The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Docker container update file list"},{"location":"containerization/docker/docker/#hcl-dx-95-cf201","text":"Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF201"},{"location":"containerization/docker/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table.","title":"HCL DX 9.5 CF199"},{"location":"containerization/docker/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"containerization/docker/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"containerization/docker/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"containerization/docker/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"containerization/docker/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"containerization/docker/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"containerization/docker/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"HCL DX 9.5 CF181"},{"location":"containerization/docker/docker_compose/","text":"Docker image deployment using Docker Compose This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"containerization/docker/docker_compose/#docker-image-deployment-using-docker-compose","text":"This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"containerization/docker/docker_image_deployment/","text":"Docker image deployment This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"containerization/docker/docker_image_deployment/#docker-image-deployment","text":"This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"containerization/docker/docker_overview/","text":"Docker images for HCL Digital Experience 9.5 HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section. Overview Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process. Installation, Deployment and Migration Guidance Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance Parent topic: DX on Docker","title":"Digital Experience on Docker"},{"location":"containerization/docker/docker_overview/#docker-images-for-hcl-digital-experience-95","text":"HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section.","title":"Docker images for HCL Digital Experience 9.5"},{"location":"containerization/docker/docker_overview/#overview","text":"Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process.","title":"Overview"},{"location":"containerization/docker/docker_overview/#installation-deployment-and-migration-guidance","text":"Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance Parent topic: DX on Docker","title":"Installation, Deployment and Migration Guidance"},{"location":"containerization/docker/docker_remote_search/","text":"Configure Remote Search in Docker This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers. Introduction To support search services when deployed to Docker container platforms in Kubernetes, administrators should configure Remote Search services. This requires a different setup and configuration steps than those used to set up Remote Search on a non-Docker container platform . Some differences in the non-Docker container platform procedures are outlined below: The serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Since Docker dynamically allocates the host names for the containers, the /etc/hosts file doesn\u2019t have static entries for the HCL Digital Experience 9.5 container-based server nor the Remote Search server. The WebSphere Application Server ND Administration console will not have correct host names for the HCL Digital Experience container. Deploying Remote Search in HCL Digital Experience 9.5 Docker containers Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and higher packages will include a core software and Remote search container. Load both of these images into your Docker repository via the \u201cdocker load\u201d command. Note that if your organization has deployed these containers to a corporate Docker repository, you might use \u201cdocker pull\u201d instead to put it into your local repository. In this example, two Docker images and names are referred to. If a higher HCL Digital Experience 9.5 Container Update release is used, for example, HCL Digital Experience 9.5 CF181, the image names may vary. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz EJBs and host names HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force Docker to write the /etc/hosts file at container initialization time is needed. This is done via the Docker switch add-host . The situation is further complicated (at least on Linux) by the fact that containers on the default bridge network of Docker cannot DNS name resolve other containers on the same subnet. Therefore, one uses the Docker host as a proxy and starts both containers with the following: docker run \u2013add-host=dockerhost:172.19.0.1 \u2026 This has the effect of adding an entry in the /etc/hosts file on the HCL Digital Experience 9.5 Container core like this: 172.19.0.1 dockerhost Those familiar with Docker deployment practices will recognize 171.19.0.1 as the IP bridge address of the host machine that starts the Docker containers. Since all Docker containers have unique ports and the Docker host machine is not allowed to use these unique ports, one can refer to a port on any container as dockerhost:port_number . Launch the HCL Digital Experience 9.5 Core and Remote Search containers To deploy, following is the complete docker run command for both the HCL Digital Experience 9.5 Core and Remote Search containers. In these examples, wpsadmin / wpsadmin are used as the HCL Digital Experience and WebSphere Application Server admin user ID and password credentials. docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 9043:9043 -p 9403:9403 -p 2809:2809 -p 9060:9060 -v /home/dxengineer/Documents/prs_profile:/opt/HCL/AppServer/profiles/prs_profile hcl/dx/dxrs:v95_CF181_20200622-1550 docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -v /home/dxengineer/Documents/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_CF181_20200622-1550 The persisted profile for each container is located at /home/dxengineer/Documents/. The HCL Digital Experience admin username and password are passed as environment variables. Defining serverindex.xml on the Remote Search server On the HCL DX 9.5 container Remote search server, the serverindex.xml file is located at: /opt/HCL/AppServer/profiles/prs_profile/config/cells/{cell name}/nodes/{node name} Note that immediately after the Remote Search server is started (and since the profile is persisted on a persisted sub-directory), this file can be found at: {persisted volume for remote search profile}/prs_profile/config/cells/{cell name}/nodes/{node name} The serverindex.xml file contains 5 lines: host=\u201d{some hostname}\u201d where {some hostname} might likely be \"localhost\" or some host name like \u201c33b7e5004319\u201d . However, remote search will not work correctly until this host name field is replaced with a host name exactly like the host name in the \u201ciiop\u201d url in the search service is coded. So, for example, if your Portal search service has coded the \"iiop\" URL as \u201ciiop://some.host.com:2809\u201d , the host in serverindex.hml needs to be host=\u201dsome.host.com\u201d . It could be that your \"iiop\" url has the host name \"dockerhost\" if your iiop url lets your docker host route that URL. In that case, change these 5 lines to the following: host=\u201ddockerhost\u201d (apply the string \u201cdockerhost\u201d) Upon completion, when the HCL Digital Experience 9.5 Container core server communicates to the container Remote Search server over \"iiop\", the Remote Search server will return \u201cdockerhost\u201d as the host name of the Remote Search server. The HCL Digital Experience 9.5 Container has a configuration that will append the port to the host name that was just returned. Digital Experience instructions for Remote Search configuration The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic should also be completed in a Docker-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX Docker deployment. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use dockerhost (as the host) and the appropriate port for the SSL access. In the examples, the Remote Search server is on dockerhost:9443 and the DX host is on dockerhost:10042. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Docker container deployment. See the section on Creating a new search service for more information. Item Value IIOP_URL iiop://dockerhost:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Portal Search Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&amp;Action=GetDocuments&amp;Range=100&amp;locale=en-US Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Use the following parameters to create a JCR search collection . Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Action=GetDocuments&amp;Format=ATOM&amp;Locale=en_US&amp;Range=100&amp;Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&amp;Start=0&amp;SeedlistId=1@OOTB_CRAWLER1 Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Configure WCM Authoring Portlet search function. Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dockerhost:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Parent topic: Customizing your container deployment","title":"Configure Remote Search in Docker"},{"location":"containerization/docker/docker_remote_search/#configure-remote-search-in-docker","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers.","title":"Configure Remote Search in Docker"},{"location":"containerization/docker/docker_remote_search/#introduction","text":"To support search services when deployed to Docker container platforms in Kubernetes, administrators should configure Remote Search services. This requires a different setup and configuration steps than those used to set up Remote Search on a non-Docker container platform . Some differences in the non-Docker container platform procedures are outlined below: The serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Since Docker dynamically allocates the host names for the containers, the /etc/hosts file doesn\u2019t have static entries for the HCL Digital Experience 9.5 container-based server nor the Remote Search server. The WebSphere Application Server ND Administration console will not have correct host names for the HCL Digital Experience container.","title":"Introduction"},{"location":"containerization/docker/docker_remote_search/#deploying-remote-search-in-hcl-digital-experience-95-docker-containers","text":"Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and higher packages will include a core software and Remote search container. Load both of these images into your Docker repository via the \u201cdocker load\u201d command. Note that if your organization has deployed these containers to a corporate Docker repository, you might use \u201cdocker pull\u201d instead to put it into your local repository. In this example, two Docker images and names are referred to. If a higher HCL Digital Experience 9.5 Container Update release is used, for example, HCL Digital Experience 9.5 CF181, the image names may vary. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz","title":"Deploying Remote Search in HCL Digital Experience 9.5 Docker containers"},{"location":"containerization/docker/docker_remote_search/#ejbs-and-host-names","text":"HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force Docker to write the /etc/hosts file at container initialization time is needed. This is done via the Docker switch add-host . The situation is further complicated (at least on Linux) by the fact that containers on the default bridge network of Docker cannot DNS name resolve other containers on the same subnet. Therefore, one uses the Docker host as a proxy and starts both containers with the following: docker run \u2013add-host=dockerhost:172.19.0.1 \u2026 This has the effect of adding an entry in the /etc/hosts file on the HCL Digital Experience 9.5 Container core like this: 172.19.0.1 dockerhost Those familiar with Docker deployment practices will recognize 171.19.0.1 as the IP bridge address of the host machine that starts the Docker containers. Since all Docker containers have unique ports and the Docker host machine is not allowed to use these unique ports, one can refer to a port on any container as dockerhost:port_number .","title":"EJBs and host names"},{"location":"containerization/docker/docker_remote_search/#launch-the-hcl-digital-experience-95-core-and-remote-search-containers","text":"To deploy, following is the complete docker run command for both the HCL Digital Experience 9.5 Core and Remote Search containers. In these examples, wpsadmin / wpsadmin are used as the HCL Digital Experience and WebSphere Application Server admin user ID and password credentials. docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 9043:9043 -p 9403:9403 -p 2809:2809 -p 9060:9060 -v /home/dxengineer/Documents/prs_profile:/opt/HCL/AppServer/profiles/prs_profile hcl/dx/dxrs:v95_CF181_20200622-1550 docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -v /home/dxengineer/Documents/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_CF181_20200622-1550 The persisted profile for each container is located at /home/dxengineer/Documents/. The HCL Digital Experience admin username and password are passed as environment variables.","title":"Launch the HCL Digital Experience 9.5 Core and Remote Search containers"},{"location":"containerization/docker/docker_remote_search/#defining-serverindexxml-on-the-remote-search-server","text":"On the HCL DX 9.5 container Remote search server, the serverindex.xml file is located at: /opt/HCL/AppServer/profiles/prs_profile/config/cells/{cell name}/nodes/{node name} Note that immediately after the Remote Search server is started (and since the profile is persisted on a persisted sub-directory), this file can be found at: {persisted volume for remote search profile}/prs_profile/config/cells/{cell name}/nodes/{node name} The serverindex.xml file contains 5 lines: host=\u201d{some hostname}\u201d where {some hostname} might likely be \"localhost\" or some host name like \u201c33b7e5004319\u201d . However, remote search will not work correctly until this host name field is replaced with a host name exactly like the host name in the \u201ciiop\u201d url in the search service is coded. So, for example, if your Portal search service has coded the \"iiop\" URL as \u201ciiop://some.host.com:2809\u201d , the host in serverindex.hml needs to be host=\u201dsome.host.com\u201d . It could be that your \"iiop\" url has the host name \"dockerhost\" if your iiop url lets your docker host route that URL. In that case, change these 5 lines to the following: host=\u201ddockerhost\u201d (apply the string \u201cdockerhost\u201d) Upon completion, when the HCL Digital Experience 9.5 Container core server communicates to the container Remote Search server over \"iiop\", the Remote Search server will return \u201cdockerhost\u201d as the host name of the Remote Search server. The HCL Digital Experience 9.5 Container has a configuration that will append the port to the host name that was just returned.","title":"Defining serverindex.xml on the Remote Search server"},{"location":"containerization/docker/docker_remote_search/#digital-experience-instructions-for-remote-search-configuration","text":"The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic should also be completed in a Docker-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX Docker deployment. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use dockerhost (as the host) and the appropriate port for the SSL access. In the examples, the Remote Search server is on dockerhost:9443 and the DX host is on dockerhost:10042. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Docker container deployment. See the section on Creating a new search service for more information. Item Value IIOP_URL iiop://dockerhost:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Portal Search Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&amp;Action=GetDocuments&amp;Range=100&amp;locale=en-US Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Use the following parameters to create a JCR search collection . Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Action=GetDocuments&amp;Format=ATOM&amp;Locale=en_US&amp;Range=100&amp;Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&amp;Start=0&amp;SeedlistId=1@OOTB_CRAWLER1 Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Configure WCM Authoring Portlet search function. Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dockerhost:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Parent topic: Customizing your container deployment","title":"Digital Experience instructions for Remote Search configuration"},{"location":"containerization/helm/configure_deployment_helm/","text":"Configuring DX 9.5 deployments to container platforms using Helm This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Configuration overview In the DX 9.5 component definitions, each application has a dedicated ConfigMap. They contain application specific key/value pairs that are derived from the templates in the DX 9.5 Helm Chart and the values inside the values.yaml (and/or your custom value overrides). Rollout of configuration changes Configuration changes are automatically processed by the HCL Digital Experience 9.5 OpenShift or Kubernetes deployment with Helm. DX 9.5 Container Upgrades Via Helm Operations Details Beginning with HCL DX 9.5 Container Upgrade CF196, Helm deployment is supported on the Google Kubernetes Engine platform (GKE). As Helm deployment methods are supported on the additional DX 9.5 supported OpenShift or Kubernetes platform of choice, it is recommended that administrators use Helm deployment to manage upgrade processes. Note that use of dxctl to manage these operations is also supported, but will be deprecated over time in favor of Helm. When supported on the additional DX 9.5 container platforms, the recommended way to change the configuration of a running deployment is via a Helm upgrade. Once the upgrade command is executed, it calculates and apply all changes that derive from the changes that have been made to the values definitions. The DX Helm deployment uses annotations on each application to share the checksum of the last ConfigMap. This checksum is updated as soon as Helm upgrade is performed and that there has been a change to the configuration of an application. When the checksum is updated, OpenShift or Kubernetes proceeds to roll out the new configuration. Based on the count of Pods per application that you are running, this may cause a downtime in operations. To minimize the impact of operations to configuration changes processing, consider running at least a quorum of 3 Pods per application. This allows OpenShift and Kubernetes to properly roll out the new configurations and maintain availability of the applications operations throughout the entire upgrade processing cycle. Direct Configuration changes in Kubernetes or OpenShift For development and testing, sometimes it is useful to directly adjust configuration in ConfigMaps via Kubernetes (for example, kubectl edit cm ) or OpenShift commands. In that case, the runtime controller is to notice that there has been a change to a ConfigMap and calculates the checksum for the Pod specification of the affected application. Kubernetes or OpenShift thens proceed to roll out the new configuration. This is the same processing methodology applies as when using Helm upgrade definitions. Note: It is recommended that administrators use Helm upgrade for configuration changes, as the DX 9.5 Helm chart contains logic to calculate certain values. If you choose to edit ConfigMaps directly in the Kubernetes or OpenShift console, do note that running a Helm upgrade overwrites the settings you inserted directly/manually in your deployment. It is recommended to apply configuration changes directly on an as-needed basis and only for development and testing purposes. Please also ensure that the keys used inside the ConfigMap are ordered alphabetically . List of HCL DX 9.5 ConfigMaps Name Application <RELEASE-NAME>-content-composer Content Composer <RELEASE-NAME>-core Core <RELEASE-NAME>-design-studio Design Studio (beta) <RELEASE-NAME>-digital-asset-management Digital Asset Management <RELEASE-NAME>-image-processor Image Processor <RELEASE-NAME>-persistence Persistence <RELEASE-NAME>-ring-api Ring API <RELEASE-NAME>-runtime-controller Runtime Controller Parent topic: Overview of the Helm architecture","title":"Configuring DX 9.5 deployments to container platforms using Helm"},{"location":"containerization/helm/configure_deployment_helm/#configuring-dx-95-deployments-to-container-platforms-using-helm","text":"This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm.","title":"Configuring DX 9.5 deployments to container platforms using Helm"},{"location":"containerization/helm/configure_deployment_helm/#configuration-overview","text":"In the DX 9.5 component definitions, each application has a dedicated ConfigMap. They contain application specific key/value pairs that are derived from the templates in the DX 9.5 Helm Chart and the values inside the values.yaml (and/or your custom value overrides).","title":"Configuration overview"},{"location":"containerization/helm/configure_deployment_helm/#rollout-of-configuration-changes","text":"Configuration changes are automatically processed by the HCL Digital Experience 9.5 OpenShift or Kubernetes deployment with Helm. DX 9.5 Container Upgrades Via Helm Operations Details Beginning with HCL DX 9.5 Container Upgrade CF196, Helm deployment is supported on the Google Kubernetes Engine platform (GKE). As Helm deployment methods are supported on the additional DX 9.5 supported OpenShift or Kubernetes platform of choice, it is recommended that administrators use Helm deployment to manage upgrade processes. Note that use of dxctl to manage these operations is also supported, but will be deprecated over time in favor of Helm. When supported on the additional DX 9.5 container platforms, the recommended way to change the configuration of a running deployment is via a Helm upgrade. Once the upgrade command is executed, it calculates and apply all changes that derive from the changes that have been made to the values definitions. The DX Helm deployment uses annotations on each application to share the checksum of the last ConfigMap. This checksum is updated as soon as Helm upgrade is performed and that there has been a change to the configuration of an application. When the checksum is updated, OpenShift or Kubernetes proceeds to roll out the new configuration. Based on the count of Pods per application that you are running, this may cause a downtime in operations. To minimize the impact of operations to configuration changes processing, consider running at least a quorum of 3 Pods per application. This allows OpenShift and Kubernetes to properly roll out the new configurations and maintain availability of the applications operations throughout the entire upgrade processing cycle. Direct Configuration changes in Kubernetes or OpenShift For development and testing, sometimes it is useful to directly adjust configuration in ConfigMaps via Kubernetes (for example, kubectl edit cm ) or OpenShift commands. In that case, the runtime controller is to notice that there has been a change to a ConfigMap and calculates the checksum for the Pod specification of the affected application. Kubernetes or OpenShift thens proceed to roll out the new configuration. This is the same processing methodology applies as when using Helm upgrade definitions. Note: It is recommended that administrators use Helm upgrade for configuration changes, as the DX 9.5 Helm chart contains logic to calculate certain values. If you choose to edit ConfigMaps directly in the Kubernetes or OpenShift console, do note that running a Helm upgrade overwrites the settings you inserted directly/manually in your deployment. It is recommended to apply configuration changes directly on an as-needed basis and only for development and testing purposes. Please also ensure that the keys used inside the ConfigMap are ordered alphabetically .","title":"Rollout of configuration changes"},{"location":"containerization/helm/configure_deployment_helm/#list-of-hcl-dx-95-configmaps","text":"Name Application <RELEASE-NAME>-content-composer Content Composer <RELEASE-NAME>-core Core <RELEASE-NAME>-design-studio Design Studio (beta) <RELEASE-NAME>-digital-asset-management Digital Asset Management <RELEASE-NAME>-image-processor Image Processor <RELEASE-NAME>-persistence Persistence <RELEASE-NAME>-ring-api Ring API <RELEASE-NAME>-runtime-controller Runtime Controller Parent topic: Overview of the Helm architecture","title":"List of HCL DX 9.5 ConfigMaps"},{"location":"containerization/helm/container_scaling_helm/","text":"Scaling DX 9.5 container deployments using Helm This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Use of HorizontalPodAutoscalers in DX 9.5 Deployments using Helm The following DX 9.5 applications can be configured to leverage HorizontalPodAutoscalers for Kubernetes and OpenShift based automated scaling: Core Content Composer Design Studio (beta) Digital Asset Management Image Processor Ring API HorizontalPodAutoscalers monitor Pod resources such as CPU and Memory usage, and automatically scales up/down applications based on specific thresholds defined and scaling limits. For the above mentioned DX applications, the maximum and minimum count of Replicas can be configured via the values.yaml. The thresholds for CPU and Memory usage are also configurable allowing for load-based automated scaling of these applications. Per default, the automated scaling is not active and needs to be enabled before taking effect. Known limitations Core The HCL Digital Experience 9.5 Core can only be scaled to more than one Pod if you have performed a database transfer from the default packaged Derby database. Prior to that, any other additional Pod except for Pod-0 fails to start, since the default packaged Derby database does not allow for multiple Pods connecting to it. Persistence The Persistence used for Digital Asset Management currently supports scaling only with the updated persistence feature. For more information, see Digital Asset Management persistence architecture . Parent topic: Overview of the Helm architecture","title":"Scaling DX 9.5 container deployments using Helm"},{"location":"containerization/helm/container_scaling_helm/#scaling-dx-95-container-deployments-using-helm","text":"This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services.","title":"Scaling DX 9.5 container deployments using Helm"},{"location":"containerization/helm/container_scaling_helm/#use-of-horizontalpodautoscalers-in-dx-95-deployments-using-helm","text":"The following DX 9.5 applications can be configured to leverage HorizontalPodAutoscalers for Kubernetes and OpenShift based automated scaling: Core Content Composer Design Studio (beta) Digital Asset Management Image Processor Ring API HorizontalPodAutoscalers monitor Pod resources such as CPU and Memory usage, and automatically scales up/down applications based on specific thresholds defined and scaling limits. For the above mentioned DX applications, the maximum and minimum count of Replicas can be configured via the values.yaml. The thresholds for CPU and Memory usage are also configurable allowing for load-based automated scaling of these applications. Per default, the automated scaling is not active and needs to be enabled before taking effect.","title":"Use of HorizontalPodAutoscalers in DX 9.5 Deployments using Helm"},{"location":"containerization/helm/container_scaling_helm/#known-limitations","text":"Core The HCL Digital Experience 9.5 Core can only be scaled to more than one Pod if you have performed a database transfer from the default packaged Derby database. Prior to that, any other additional Pod except for Pod-0 fails to start, since the default packaged Derby database does not allow for multiple Pods connecting to it. Persistence The Persistence used for Digital Asset Management currently supports scaling only with the updated persistence feature. For more information, see Digital Asset Management persistence architecture . Parent topic: Overview of the Helm architecture","title":"Known limitations"},{"location":"containerization/helm/core_interactions_kubernetes/","text":"DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Volume mount points The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog. Additional Information about profile directories The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile. Core container Version-to-Version upgrade When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note: If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories). Parent topic: Overview of the Helm architecture","title":"DX 9.5 Core Interactions with Kubernetes"},{"location":"containerization/helm/core_interactions_kubernetes/#dx-95-core-interactions-with-kubernetes","text":"This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm.","title":"DX 9.5 Core Interactions with Kubernetes"},{"location":"containerization/helm/core_interactions_kubernetes/#volume-mount-points","text":"The persistent volumes used by the DX Core pod are mounted to the following directories in the Core container: profile (WebSphere Application Server profiles for the WebSphere_Portal application server, shared between pods): /opt/HCL/profiles log (WebSphere Application Server logs for the WebSphere_Portal application server, unique to a pod): /opt/HCL/logs tranlog (transaction log, unique to a pod): /opt/HCL/tranlog The logs directory /opt/HCL/wp_profile/logs is symbolically linked to /opt/HCL/logs. The tranlog directory /opt/HCL/wp_profile/tranlog is symbolically linked to /opt/HCL/tranlog.","title":"Volume mount points"},{"location":"containerization/helm/core_interactions_kubernetes/#additional-information-about-profile-directories","text":"The profile persistent volume (and thus, the /opt/HCL/profiles directory) contains a directory per container version, named: prof_ < product-version > _ < container-version > for example, prof_95_CF199 . During the Core container startup process the latest version directory is symbolically linked from /opt/HCL/wp_profile.","title":"Additional Information about profile directories"},{"location":"containerization/helm/core_interactions_kubernetes/#core-container-version-to-version-upgrade","text":"When a new version (tag) of the DX 9.5 Core container is specified in your custom values YAML file and you run helm upgrade , Kubernetes recycles all the pods in your Core stateful set one by one. It starts with the highest numbered pod and work downwards, only recycling the next pod when the current pod reports that it is \"ready\". Whenever a Core container is started, it compares its container version with the latest profile version. If they do not match, perform an Update using the process set out below: Kubernetes recycles the highest numbered pod, supplying the new DX 9.5 Container image. Highest numbered pod creates a new profile directory on the shared volume for the new version (named as described above) with contents copied from the previous version profile directory. Pod switches its symbolic link for /opt/HCL/wp_profile to the new directory. Pod performs the actual upgrade (\" applyCF \") and, when this is complete, is declared \"ready\" to Kubernetes. Kubernetes recycles the next highest numbered pod. Pod determines that a profile directory is already populated for the new HCL DX 9.5 container image version, and so, links to that as normal; and onwards Steps 5 and 6 are repeated until there are no further pods using the old image. Note: If you have more than one DX Core pod, those not yet recycled will still use the previous profile directory. Therefore, any configuration changes made during this time that are stored to the profile (for example, the installation of a portlet) are lost, as they are made to the previous profile after it has already been copied. We recommend that you avoid making any configuration changes while a Version-to-Version upgrade is in progress. As of HCL DX 9.5 Container Update CF199, DX profile directories are not automatically removed. If your DX 9.5 deployment has been around through a number of Container upgrades, you may wish to consider removing very old profile directories to save space (leaving, at least, two of the most recent profile directories). Parent topic: Overview of the Helm architecture","title":"Core container Version-to-Version upgrade"},{"location":"containerization/helm/dam_persistence_architecture/","text":"Digital Asset Management persistence architecture This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. , persistence-node persistence-node provides the database functionality for HCL Digital Asset Management . The persistence-node is a DX Red Hat Universal Base Image (UBI) container image installed with PostgreSQL and the Replication Manager Service. [repmgr](https://repmgr.org/) is an open-source tool suite for managing replication and failover in a cluster of PostgreSQL servers. repmgr enhances the built-in hot-standby capabilities of PostgreSQL with tools to set up standby servers, monitor replication, and perform administrative tasks, such as failover or manual switchover operations. In case of PostgreSQL master server failure, the repmgr service switches the server role from master to standby. The persistence-node configurations are available in the Helm Chart values.yaml file as persistenceNode . The administrator can configure number of persistence-node under scaling configuration. # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 Note: Scaling affects only the read requests and ensures fail-over capabilities. Write requests are always directed only to the primary pod. The persistence-node is a stateful application and it requires a volume. The configuration must have a dynamic volume class to start the container. The storageClassName and storage must be updated according to the cloud service provider and project requirement. # Persistent Volumes for Persistence Node persistenceNode: # Database PVC, one per Persistence Node database: storageClassName: \"manual\" requests: storage: \"2Gi\" # Optional volume name to specifically map to. volumeName: persistence-connection-pool The persistence-connection-pool container runs the Pg-pool service. Pg-pool is a middleware that works between persistence-node and HCL Digital Asset Management . The service provides: Connection pooling Load balancing For better performance, administrator can scale the persistence-connection-pool to more than one pod. The persistence-connection-pool configurations are available in Helm Chart values.yaml file as persistenceConnectionPool . # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 The following is an example of a persistence cluster in a successful deployment. . Parent topic: Overview of the Helm architecture","title":"Digital Asset Management persistence architecture"},{"location":"containerization/helm/dam_persistence_architecture/#digital-asset-management-persistence-architecture","text":"This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. ,","title":"Digital Asset Management persistence architecture"},{"location":"containerization/helm/dam_persistence_architecture/#persistence-node","text":"persistence-node provides the database functionality for HCL Digital Asset Management . The persistence-node is a DX Red Hat Universal Base Image (UBI) container image installed with PostgreSQL and the Replication Manager Service. [repmgr](https://repmgr.org/) is an open-source tool suite for managing replication and failover in a cluster of PostgreSQL servers. repmgr enhances the built-in hot-standby capabilities of PostgreSQL with tools to set up standby servers, monitor replication, and perform administrative tasks, such as failover or manual switchover operations. In case of PostgreSQL master server failure, the repmgr service switches the server role from master to standby. The persistence-node configurations are available in the Helm Chart values.yaml file as persistenceNode . The administrator can configure number of persistence-node under scaling configuration. # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 Note: Scaling affects only the read requests and ensures fail-over capabilities. Write requests are always directed only to the primary pod. The persistence-node is a stateful application and it requires a volume. The configuration must have a dynamic volume class to start the container. The storageClassName and storage must be updated according to the cloud service provider and project requirement. # Persistent Volumes for Persistence Node persistenceNode: # Database PVC, one per Persistence Node database: storageClassName: \"manual\" requests: storage: \"2Gi\" # Optional volume name to specifically map to. volumeName:","title":"persistence-node"},{"location":"containerization/helm/dam_persistence_architecture/#persistence-connection-pool","text":"The persistence-connection-pool container runs the Pg-pool service. Pg-pool is a middleware that works between persistence-node and HCL Digital Asset Management . The service provides: Connection pooling Load balancing For better performance, administrator can scale the persistence-connection-pool to more than one pod. The persistence-connection-pool configurations are available in Helm Chart values.yaml file as persistenceConnectionPool . # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 1 imageProcessor: 1 ringApi: 1 persistenceConnectionPool: 1 persistenceNode: 3 ambassadorIngress: 3 ambassadorRedis: 3 The following is an example of a persistence cluster in a successful deployment. . Parent topic: Overview of the Helm architecture","title":"persistence-connection-pool"},{"location":"containerization/helm/deploy_applications_using_helm/","text":"Deploying DX 9.5 applications to container platforms using Helm This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file listing topic. Do note that each application defined only lists its next direct dependencies. Sub-dependencies are not explicitly listed. DX 9.5 Core Depends on: No dependencies Type: Stateful HCL DX 9.5 CF196 and later Core contains the primary Portal and Web Content Manager HCL Digital Experience functionality. The standard deployment deploys at least one Pod of Core. If you have an existing on-premise installation of DX 9.5 Core, you can also use that one for your deployment using the Hybrid deployment pattern as described in the DX 9.5 Hybrid deployment topic. Note: Application of the hybrid deployment pattern is not yet supported with Helm in HCL DX 9.5 Container Update CF196, and will be added in a later update release. Ring API Depends on: Core (deployed to OpenShift, Kubernetes or Hybrid on-premise) Type: Stateless The Ring API, a component of the HCL DX Experience API , is a REST API wrapping Core functionality. It provides easy-to-use API endpoints and requires that a DX Core 9.5 instance is deployed and started. That instance can either be running inside the Kubernetes or OpenShift deployment or be an existing external on-premise DX-Core installation using the Hybrid pattern. Content Composer Depends on: Ring API Type: Stateless Content Composer requires the Ring API to be deployed to execute Content authoring capabilities. Digital Asset Management Depends on: Ring API, Image Processor, Persistence Type: Stateful Digital Asset Management requires the Ring API be deployed and operational to communicate with the DX Core, and Image Processor components to perform image manipulation, and Persistence to store its application data. Persistence Depends on: No dependencies Type: Stateful Persistence is used by the Digital Asset Management component to store application data. It consists of a read/write primary node and at least one standby read-only node. The switch between the read/write primary and the read-only nodes is automatically performed by the Runtime Controller. Image Processor Depends on: No dependencies Type: Stateless The Image Processor provides image manipulation capabilities that are leveraged by Digital Asset Management. Design Studio (Beta) Depends on: Core, Ring API Type: Stateless Refer to the Design Studio (Beta) topic section for more information about this application. Runtime Controller Depends on: No dependencies Type: Stateless The Runtime Controller incorporates runtime management functionality for the entire HCL DX 9.5 Container hcl-dx-deployment . It enables automated rollout of configuration changes during runtime and acts as a \u201cwatchdog\u201d to monitor for the automated read/write to read-only fallback of Persistence. Interdependency Matrix This matrix shows which HCL DX applications have dependencies on other applications. This also includes sub-dependencies. For example, if an application uses Ring API, it is also dependent on an operational DX 9.5 Core instance. Application names are defined as follows: Shortname Full Name Core HCL DX 9.5 Core Ring API Ring API CC Content Composer DAM Digital Asset Management DS Design Studio PER Persistence IMG Image Processor Parent topic: Overview of the Helm architecture","title":"Deploying DX 9.5 applications to container platforms using Helm"},{"location":"containerization/helm/deploy_applications_using_helm/#deploying-dx-95-applications-to-container-platforms-using-helm","text":"This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file listing topic. Do note that each application defined only lists its next direct dependencies. Sub-dependencies are not explicitly listed.","title":"Deploying DX 9.5 applications to container platforms using Helm"},{"location":"containerization/helm/deploy_applications_using_helm/#dx-95-core","text":"Depends on: No dependencies Type: Stateful HCL DX 9.5 CF196 and later Core contains the primary Portal and Web Content Manager HCL Digital Experience functionality. The standard deployment deploys at least one Pod of Core. If you have an existing on-premise installation of DX 9.5 Core, you can also use that one for your deployment using the Hybrid deployment pattern as described in the DX 9.5 Hybrid deployment topic. Note: Application of the hybrid deployment pattern is not yet supported with Helm in HCL DX 9.5 Container Update CF196, and will be added in a later update release.","title":"DX 9.5 Core"},{"location":"containerization/helm/deploy_applications_using_helm/#ring-api","text":"Depends on: Core (deployed to OpenShift, Kubernetes or Hybrid on-premise) Type: Stateless The Ring API, a component of the HCL DX Experience API , is a REST API wrapping Core functionality. It provides easy-to-use API endpoints and requires that a DX Core 9.5 instance is deployed and started. That instance can either be running inside the Kubernetes or OpenShift deployment or be an existing external on-premise DX-Core installation using the Hybrid pattern.","title":"Ring API"},{"location":"containerization/helm/deploy_applications_using_helm/#content-composer","text":"Depends on: Ring API Type: Stateless Content Composer requires the Ring API to be deployed to execute Content authoring capabilities.","title":"Content Composer"},{"location":"containerization/helm/deploy_applications_using_helm/#digital-asset-management","text":"Depends on: Ring API, Image Processor, Persistence Type: Stateful Digital Asset Management requires the Ring API be deployed and operational to communicate with the DX Core, and Image Processor components to perform image manipulation, and Persistence to store its application data.","title":"Digital Asset Management"},{"location":"containerization/helm/deploy_applications_using_helm/#persistence","text":"Depends on: No dependencies Type: Stateful Persistence is used by the Digital Asset Management component to store application data. It consists of a read/write primary node and at least one standby read-only node. The switch between the read/write primary and the read-only nodes is automatically performed by the Runtime Controller.","title":"Persistence"},{"location":"containerization/helm/deploy_applications_using_helm/#image-processor","text":"Depends on: No dependencies Type: Stateless The Image Processor provides image manipulation capabilities that are leveraged by Digital Asset Management.","title":"Image Processor"},{"location":"containerization/helm/deploy_applications_using_helm/#design-studio-beta","text":"Depends on: Core, Ring API Type: Stateless Refer to the Design Studio (Beta) topic section for more information about this application.","title":"Design Studio (Beta)"},{"location":"containerization/helm/deploy_applications_using_helm/#runtime-controller","text":"Depends on: No dependencies Type: Stateless The Runtime Controller incorporates runtime management functionality for the entire HCL DX 9.5 Container hcl-dx-deployment . It enables automated rollout of configuration changes during runtime and acts as a \u201cwatchdog\u201d to monitor for the automated read/write to read-only fallback of Persistence.","title":"Runtime Controller"},{"location":"containerization/helm/deploy_applications_using_helm/#interdependency-matrix","text":"This matrix shows which HCL DX applications have dependencies on other applications. This also includes sub-dependencies. For example, if an application uses Ring API, it is also dependent on an operational DX 9.5 Core instance. Application names are defined as follows: Shortname Full Name Core HCL DX 9.5 Core Ring API Ring API CC Content Composer DAM Digital Asset Management DS Design Studio PER Persistence IMG Image Processor Parent topic: Overview of the Helm architecture","title":"Interdependency Matrix"},{"location":"containerization/helm/docker/","text":"Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. Docker container update file list The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF201 Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table. HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"Docker image list"},{"location":"containerization/helm/docker/#docker-image-list","text":"This section presents the latest HCL DX 9.5 Docker container update images available.","title":"Docker image list"},{"location":"containerization/helm/docker/#docker-container-update-file-list","text":"The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Docker container update file list"},{"location":"containerization/helm/docker/#hcl-dx-95-cf201","text":"Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF201"},{"location":"containerization/helm/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table.","title":"HCL DX 9.5 CF199"},{"location":"containerization/helm/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"containerization/helm/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"containerization/helm/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"containerization/helm/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"containerization/helm/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"containerization/helm/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"containerization/helm/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"HCL DX 9.5 CF181"},{"location":"containerization/helm/helm/","text":"Helm-based deployment Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Overview of the Helm architecture This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Deploying container platforms using Helm Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Operations using Helm This topic provides operations guidance for DX administrators to manage DX 9.5 deployments using Helm, such a configuration updates, monitoring, and troubleshooting strategies. Migrating from Operator to Helm deployments This topic provides administrators the guidance to migrate HCL Digital Experience Container Update CF199 and later releases from Operator to Helm deployment. Backup and recovery procedures for Helm Containerization This section provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 Helm environments. Parent topic: Digital Experience on containerized platforms","title":"Helm-based deployment"},{"location":"containerization/helm/helm/#helm-based-deployment","text":"Learn how to deploy HCL Digital Experience 9.5 containers along with Ambassador to Kubernetes, as verified in Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. Overview of the Helm architecture This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Deploying container platforms using Helm Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Operations using Helm This topic provides operations guidance for DX administrators to manage DX 9.5 deployments using Helm, such a configuration updates, monitoring, and troubleshooting strategies. Migrating from Operator to Helm deployments This topic provides administrators the guidance to migrate HCL Digital Experience Container Update CF199 and later releases from Operator to Helm deployment. Backup and recovery procedures for Helm Containerization This section provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 Helm environments. Parent topic: Digital Experience on containerized platforms","title":"Helm-based deployment"},{"location":"containerization/helm/helm_access_configwizard/","text":"Accessing the ConfigWizard admin console in a container environment This topic describes how you can access the ConfigWizard admin console in a container environment from your local system. The ConfigWizard admin console opens to the TCP port number 10203, but this port cannot be accessed directly via the Kubernetes ingress controller. Hence, use the following instructions to access the ConfigWizard console. Procedure Connect to your Kubernetes cluster from your workstation via CLI. For instructions, refer to the documentation of your cloud platform provider. Use the following command to find your deployment name space: kubectl get <namespace> Example: ``` sh-3.2$ kubectl get ns NAME STATUS AGE default Active 336d eks-n0-clstrhyb-rel Active 11h eks-rel-helm-fresh Active 15h kube-node-lease Active 336d kube-public Active 336d kube-system Active 336d ``` Use the following command to find your DX core pod name: ``` kubectl get pods -n ``` Example: ``` sh-3.2$ kubectl get pods -n eks-rel-helm-fresh NAME READY STATUS RESTARTS AGE dx-deployment-ambassador-7f954c7d74-46km2 1/1 Running 0 15h dx-deployment-ambassador-7f954c7d74-vfzxw 1/1 Running 0 15h dx-deployment-ambassador-7f954c7d74-wtfqq 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-7bw8r 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-j9s7v 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-nf7p5 1/1 Running 0 15h dx-deployment-content-composer-5cb56f94d8-wlnqw 1/1 Running 0 15h dx-deployment-core-0 3/3 Running 1 15h dx-deployment-design-studio-65d4bdbd89-hh62t 1/1 Running 0 15h dx-deployment-digital-asset-management-0 1/1 Running 4 15h dx-deployment-image-processor-96dc59fcf-956wz 1/1 Running 0 15h dx-deployment-open-ldap-0 1/1 Running 0 15h dx-deployment-persistence-connection-pool-75b77b8b86-pmfrz 1/1 Running 0 15h dx-deployment-persistence-node-0 2/2 Running 0 15h dx-deployment-persistence-node-1 2/2 Running 0 15h dx-deployment-persistence-node-2 2/2 Running 0 15h dx-deployment-remote-search-0 3/3 Running 0 15h dx-deployment-ring-api-845658b658-tt588 1/1 Running 0 15h dx-deployment-runtime-controller-7d9df9db98-h4mrf 1/1 Running 0 15h ``` Enable port-forwarding to your workstation. kubectl port-forward <dx-core pod-name> 10203:10203 -n <namespace> Note: Ensure that the port-forwarding service keeps running in your terminal to access the ConfigWizard admin console. Example: sh-3.2$ kubectl port-forward dx-deployment-core-0 10203:10203 -n eks-rel-helm-fresh Forwarding from 127.0.0.1:10203 -> 10203 Forwarding from [::1]:10203 -> 10203 Navigate to the following URL to open the ConfigWizard admin console: https://localhost:10203/ibm/console Parent topic: Operations using Helm","title":"Accessing the ConfigWizard admin console in a container environment"},{"location":"containerization/helm/helm_access_configwizard/#accessing-the-configwizard-admin-console-in-a-container-environment","text":"This topic describes how you can access the ConfigWizard admin console in a container environment from your local system. The ConfigWizard admin console opens to the TCP port number 10203, but this port cannot be accessed directly via the Kubernetes ingress controller. Hence, use the following instructions to access the ConfigWizard console.","title":"Accessing the ConfigWizard admin console in a container environment"},{"location":"containerization/helm/helm_access_configwizard/#procedure","text":"Connect to your Kubernetes cluster from your workstation via CLI. For instructions, refer to the documentation of your cloud platform provider. Use the following command to find your deployment name space: kubectl get <namespace> Example: ``` sh-3.2$ kubectl get ns NAME STATUS AGE default Active 336d eks-n0-clstrhyb-rel Active 11h eks-rel-helm-fresh Active 15h kube-node-lease Active 336d kube-public Active 336d kube-system Active 336d ``` Use the following command to find your DX core pod name: ``` kubectl get pods -n ``` Example: ``` sh-3.2$ kubectl get pods -n eks-rel-helm-fresh NAME READY STATUS RESTARTS AGE dx-deployment-ambassador-7f954c7d74-46km2 1/1 Running 0 15h dx-deployment-ambassador-7f954c7d74-vfzxw 1/1 Running 0 15h dx-deployment-ambassador-7f954c7d74-wtfqq 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-7bw8r 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-j9s7v 1/1 Running 0 15h dx-deployment-ambassador-redis-6c5b8f84c6-nf7p5 1/1 Running 0 15h dx-deployment-content-composer-5cb56f94d8-wlnqw 1/1 Running 0 15h dx-deployment-core-0 3/3 Running 1 15h dx-deployment-design-studio-65d4bdbd89-hh62t 1/1 Running 0 15h dx-deployment-digital-asset-management-0 1/1 Running 4 15h dx-deployment-image-processor-96dc59fcf-956wz 1/1 Running 0 15h dx-deployment-open-ldap-0 1/1 Running 0 15h dx-deployment-persistence-connection-pool-75b77b8b86-pmfrz 1/1 Running 0 15h dx-deployment-persistence-node-0 2/2 Running 0 15h dx-deployment-persistence-node-1 2/2 Running 0 15h dx-deployment-persistence-node-2 2/2 Running 0 15h dx-deployment-remote-search-0 3/3 Running 0 15h dx-deployment-ring-api-845658b658-tt588 1/1 Running 0 15h dx-deployment-runtime-controller-7d9df9db98-h4mrf 1/1 Running 0 15h ``` Enable port-forwarding to your workstation. kubectl port-forward <dx-core pod-name> 10203:10203 -n <namespace> Note: Ensure that the port-forwarding service keeps running in your terminal to access the ConfigWizard admin console. Example: sh-3.2$ kubectl port-forward dx-deployment-core-0 10203:10203 -n eks-rel-helm-fresh Forwarding from 127.0.0.1:10203 -> 10203 Forwarding from [::1]:10203 -> 10203 Navigate to the following URL to open the ConfigWizard admin console: https://localhost:10203/ibm/console Parent topic: Operations using Helm","title":"Procedure"},{"location":"containerization/helm/helm_additional_tasks/","text":"Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Prepare cluster nodes You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: ``` Edit Node kubectl edit node k8s-node-4 ``` OpenShift Client: ``` Edit Node kubectl edit node k8s-node-4 ``` The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: ``` Execute lookup via kubectl kubectl get node k8s-node-4 --show-labels Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress Execute lookup via kubectl oc get node k8s-node-4 --show-labels Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress ``` Configure nodes You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> Select DX applications to deploy HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed. Refer to the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic for related information. Disabling or enabling specific applications You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one. Supported LDAP configuration You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Parameter Values Description type - none - other - dx |- Determines which type of LDAP to use. - Accepts none , dx or other - none : No LDAP configuration - dx : use and configure DX Open LDAP - other : use other LDAP | |bindUser| |- User used to connect to LDAP - Only used if ldap type is other | |bindPassword| |- Password used to connect to LDAP - Only used if ldap type is other | |suffix| |- Suffix in LDAP - Only used if ldap type is other | |host| |- Host of LDAP - Only used if ldap type is other | |port| |- Port of LDAP - Only used if ldap type is other | |attributeMappingLdap|- mail - title - userPassword |- Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) | |attributeMappingPortal|- ibm-primaryEmail - ibm-jobTitle - password |- Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) | |attributeNonSupported|- certificate - members |- Non-supported LDAP attributes (comma-separated list) | |serverType|- CUSTOM |- Supported LDAP Server types | |id|- dx_ldap |- LDAP configuration id | Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option Authoring/Rendering configuration You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true Configuration Wizard configuration You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true OpenLDAP configuration If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\" Remote Search configuration You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true Configure scaling The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note: You are not able to use more than one (1) Core Pod until you have performed a database transfer. Configuring pod count Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3 Configuring HorizontalPodAutoscalers The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications. Configure credentials HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment. Adjusting default credentials You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\" Configure Core sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" Configure Remote Search sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\" Configure Labels and Annotations This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: ``` release ``` ``` helm.sh/chart ``` ``` app.kubernetes.io/version ``` ``` app.kubernetes.io/managed-by ``` ``` app.kubernetes.io/name ``` app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Configure environment variables for DX resources This section explains the configuration of environment variables for different DX resources. Environment variables To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note: Additional environment values are not mandatory for a deployment. Sample environment variables for core pods To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2 Incubator section in the values.yaml file The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Note: All features within the incubator section are not recommended to be used in the production environments. Refer to Install and uninstall commands for the next steps. Use ImagePullSecrets To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note: Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to. Configure deployment to use ImagePullSecrets In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images. Parent topic: Deploying container platforms using Helm","title":"Additional Helm tasks"},{"location":"containerization/helm/helm_additional_tasks/#additional-helm-tasks","text":"This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node.","title":"Additional Helm tasks"},{"location":"containerization/helm/helm_additional_tasks/#prepare-cluster-nodes","text":"You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: ```","title":"Prepare cluster nodes"},{"location":"containerization/helm/helm_additional_tasks/#edit-node","text":"kubectl edit node k8s-node-4 ``` OpenShift Client: ```","title":"Edit Node"},{"location":"containerization/helm/helm_additional_tasks/#edit-node_1","text":"kubectl edit node k8s-node-4 ``` The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: ```","title":"Edit Node"},{"location":"containerization/helm/helm_additional_tasks/#execute-lookup-via-kubectl","text":"kubectl get node k8s-node-4 --show-labels","title":"Execute lookup via kubectl"},{"location":"containerization/helm/helm_additional_tasks/#command-output","text":"NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress","title":"Command output"},{"location":"containerization/helm/helm_additional_tasks/#execute-lookup-via-kubectl_1","text":"oc get node k8s-node-4 --show-labels","title":"Execute lookup via kubectl"},{"location":"containerization/helm/helm_additional_tasks/#command-output_1","text":"NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress ```","title":"Command output"},{"location":"containerization/helm/helm_additional_tasks/#configure-nodes","text":"You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none>","title":"Configure nodes"},{"location":"containerization/helm/helm_additional_tasks/#select-dx-applications-to-deploy","text":"HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed. Refer to the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic for related information. Disabling or enabling specific applications You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one.","title":"Select DX applications to deploy"},{"location":"containerization/helm/helm_additional_tasks/#supported-ldap-configuration","text":"You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Parameter Values Description type - none - other - dx |- Determines which type of LDAP to use. - Accepts none , dx or other - none : No LDAP configuration - dx : use and configure DX Open LDAP - other : use other LDAP | |bindUser| |- User used to connect to LDAP - Only used if ldap type is other | |bindPassword| |- Password used to connect to LDAP - Only used if ldap type is other | |suffix| |- Suffix in LDAP - Only used if ldap type is other | |host| |- Host of LDAP - Only used if ldap type is other | |port| |- Port of LDAP - Only used if ldap type is other | |attributeMappingLdap|- mail - title - userPassword |- Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) | |attributeMappingPortal|- ibm-primaryEmail - ibm-jobTitle - password |- Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) | |attributeNonSupported|- certificate - members |- Non-supported LDAP attributes (comma-separated list) | |serverType|- CUSTOM |- Supported LDAP Server types | |id|- dx_ldap |- LDAP configuration id | Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option","title":"Supported LDAP configuration"},{"location":"containerization/helm/helm_additional_tasks/#authoringrendering-configuration","text":"You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true","title":"Authoring/Rendering configuration"},{"location":"containerization/helm/helm_additional_tasks/#configuration-wizard-configuration","text":"You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true","title":"Configuration Wizard configuration"},{"location":"containerization/helm/helm_additional_tasks/#openldap-configuration","text":"If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\"","title":"OpenLDAP configuration"},{"location":"containerization/helm/helm_additional_tasks/#remote-search-configuration","text":"You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true","title":"Remote Search configuration"},{"location":"containerization/helm/helm_additional_tasks/#configure-scaling","text":"The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note: You are not able to use more than one (1) Core Pod until you have performed a database transfer. Configuring pod count Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3 Configuring HorizontalPodAutoscalers The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications.","title":"Configure scaling"},{"location":"containerization/helm/helm_additional_tasks/#configure-credentials","text":"HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment. Adjusting default credentials You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\"","title":"Configure credentials"},{"location":"containerization/helm/helm_additional_tasks/#configure-core-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\"","title":"Configure Core sidecar logging"},{"location":"containerization/helm/helm_additional_tasks/#configure-remote-search-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\"","title":"Configure Remote Search sidecar logging"},{"location":"containerization/helm/helm_additional_tasks/#configure-labels-and-annotations","text":"This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: ``` release ``` ``` helm.sh/chart ``` ``` app.kubernetes.io/version ``` ``` app.kubernetes.io/managed-by ``` ``` app.kubernetes.io/name ``` app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2","title":"Configure Labels and Annotations"},{"location":"containerization/helm/helm_additional_tasks/#configure-environment-variables-for-dx-resources","text":"This section explains the configuration of environment variables for different DX resources. Environment variables To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note: Additional environment values are not mandatory for a deployment. Sample environment variables for core pods To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2","title":"Configure environment variables for DX resources"},{"location":"containerization/helm/helm_additional_tasks/#incubator-section-in-the-valuesyaml-file","text":"The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Note: All features within the incubator section are not recommended to be used in the production environments. Refer to Install and uninstall commands for the next steps.","title":"Incubator section in the values.yaml file"},{"location":"containerization/helm/helm_additional_tasks/#use-imagepullsecrets","text":"To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note: Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to.","title":"Use ImagePullSecrets"},{"location":"containerization/helm/helm_additional_tasks/#configure-deployment-to-use-imagepullsecrets","text":"In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images. Parent topic: Deploying container platforms using Helm","title":"Configure deployment to use ImagePullSecrets"},{"location":"containerization/helm/helm_backup_and_recovery_procedures/","text":"Backup and recovery procedures for Helm Containerization This section provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 Helm environments. Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. Parent topic: Helm-based deployment","title":"Backup and recovery procedures for Helm Containerization"},{"location":"containerization/helm/helm_backup_and_recovery_procedures/#backup-and-recovery-procedures-for-helm-containerization","text":"This section provides the instructions to create and manage backup and recovery of HCL Digital Experience components in containerized DX 9.5 Helm environments. Back up and restore a DAM image This topic shows you how to backup and restore for Digital Asset Management persistence and binaries in a Helm-based deployment. Parent topic: Helm-based deployment","title":"Backup and recovery procedures for Helm Containerization"},{"location":"containerization/helm/helm_cf192andlater/","text":"Deploying HCL DX CF196 to container platforms using Helm This topic provides administrators with a high-level overview of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process . Overview Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms. Using Helm Charts for DX 9.5 deployments The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates (for example, Kubernetes resources) value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm. From Helm Chart to deployment on container platforms As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace. Deployment structure Basic per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used.","title":"Deploying HCL DX CF196 to container platforms using Helm"},{"location":"containerization/helm/helm_cf192andlater/#deploying-hcl-dx-cf196-to-container-platforms-using-helm","text":"This topic provides administrators with a high-level overview of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process .","title":"Deploying HCL DX CF196 to container platforms using Helm"},{"location":"containerization/helm/helm_cf192andlater/#overview","text":"Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms.","title":"Overview"},{"location":"containerization/helm/helm_cf192andlater/#using-helm-charts-for-dx-95-deployments","text":"The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates (for example, Kubernetes resources) value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm.","title":"Using Helm Charts for DX 9.5 deployments"},{"location":"containerization/helm/helm_cf192andlater/#from-helm-chart-to-deployment-on-container-platforms","text":"As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace.","title":"From Helm Chart to deployment on container platforms"},{"location":"containerization/helm/helm_cf192andlater/#deployment-structure","text":"Basic per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used.","title":"Deployment structure"},{"location":"containerization/helm/helm_configure_networking/","text":"Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Full Kubernetes or OpenShift deployment If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured. Core host In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly. Configure Cross Origin Resource Sharing (CORS) The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured. Hybrid host Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details. Configure Ingress certificate To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading. Generate self-signed certificate It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment. Use certificate Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note: The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience Configure secret in deployment You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note: Verify you have entered the correct name. Configure minimum TLS version for Ingress From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure networking"},{"location":"containerization/helm/helm_configure_networking/#configure-networking","text":"This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster.","title":"Configure networking"},{"location":"containerization/helm/helm_configure_networking/#full-kubernetes-or-openshift-deployment","text":"If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured.","title":"Full Kubernetes or OpenShift deployment"},{"location":"containerization/helm/helm_configure_networking/#core-host","text":"In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly.","title":"Core host"},{"location":"containerization/helm/helm_configure_networking/#configure-cross-origin-resource-sharing-cors","text":"The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured.","title":"Configure Cross Origin Resource Sharing (CORS)"},{"location":"containerization/helm/helm_configure_networking/#hybrid-host","text":"Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details.","title":"Hybrid host"},{"location":"containerization/helm/helm_configure_networking/#configure-ingress-certificate","text":"To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading.","title":"Configure Ingress certificate"},{"location":"containerization/helm/helm_configure_networking/#generate-self-signed-certificate","text":"It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment.","title":"Generate self-signed certificate"},{"location":"containerization/helm/helm_configure_networking/#use-certificate","text":"Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note: The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience","title":"Use certificate"},{"location":"containerization/helm/helm_configure_networking/#configure-secret-in-deployment","text":"You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note: Verify you have entered the correct name.","title":"Configure secret in deployment"},{"location":"containerization/helm/helm_configure_networking/#configure-minimum-tls-version-for-ingress","text":"From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure minimum TLS version for Ingress"},{"location":"containerization/helm/helm_dam_migration/","text":"Migrate Digital Asset Management persistence and binaries This section shows the guidance to back up and restore your DAM persistence and binaries. Follow this guidance to create a backup of the DAM persistence and binaries from your Operator deployment, and restore them to a Helm-based deployment. Back up from an Operator-based deployment You must start Digital Asset Management in maintenance mode, to ensure that no actions are performed during the migration. You can set DAM to maintenance mode by changing the ConfigMap of the Operator deployment: kubectl -n <namespace> edit cm <configmap> Example : kubectl edit cm -n dxns dx-deployment In the data section, add the following entry and save the changes: data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod restarts automatically. Please wait until the pod restarts before proceeding. Verify that persistence (read-write) and DAM pods are up and running, and are in maintenance mode. Use the following command to see the current status of the pods: kubectl -n <namespace> get pods If more than one DAM or persistence pod is running, scale down the pods to only one of each type. Adjust the dxctl property file: ``` dam.minreplicas:1 dam.maxreplicas:1 persist.minreplicas:1 persist.maxreplicas:1 ``` And apply it using the dxctl tool. ./dxctl -\u2013update -p deployment.properties The changes are applied and any replicas are terminated. Use kubectl to check the progress. Verify that DAM is in maintenance mode by running the following command: kubectl -n <namespace> logs <pod-name> Example : kubectl -n dxns logs dx-deployment-dam-0 If your output looks similar to the following, maintenance mode is enabled and you can continue: Maintenance mode is: true Listening for SIGTERM Maintenance mode is enabled. This mode solely starts the pod without any processes within it. Connect to the persistence pod. The following command opens a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database using pg_dump : pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the shell in the persistence pod exit Download the dumped database from the persistence pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Connect to the DAM pod. The following command opens a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located in /opt/app/upload: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Exit the shell in the DAM pod: exit Download the compressed DAM binaries from the DAM pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your back up to the Helm-based deployment Important: Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file is done. You must enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except digitalAssetManagement and the persistence : applications: core: false runtimeController: false contentComposer: false designStudio: false digitalAssetManagement: true imageProcessor: false openLdap: false persistence: true remoteSearch: false ringApi: false ambassador: false Scale down persistence to a single node: scaling: replicas: persistenceNode: 1 You can now start the Helm deployment. If you're running the migration of DAM first: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example : helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If core migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> Example : helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment The following outcomes are expected: The DAM and persistence pods are running and kept alive. The DAM application is not running. Upload the backup database. Use the following command to transfer the backup database to the remote persistence pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example : kubectl cp /tmp/dxmediadb.dmp dxns-helm/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to the persistence (read-write) pod. Use the following command to open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns-helm -- /bin/bash You must drop the DAM database, if it exists. Disconnect all connections that use the database and drop any existing databases of the Helm deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the shell in the persistence pod: exit Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Exit the shell in the DAM pod: exit Disable the migration mode and the deployment. Before you can start the final upgrade of the Helm deployment, some adjustments are needed: Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. You can now upgrade the Helm deployment. helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example : helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment Parent topic: Migrating from Operator to Helm deployments","title":"Migrate Digital Asset Management persistence and binaries"},{"location":"containerization/helm/helm_dam_migration/#migrate-digital-asset-management-persistence-and-binaries","text":"This section shows the guidance to back up and restore your DAM persistence and binaries. Follow this guidance to create a backup of the DAM persistence and binaries from your Operator deployment, and restore them to a Helm-based deployment. Back up from an Operator-based deployment You must start Digital Asset Management in maintenance mode, to ensure that no actions are performed during the migration. You can set DAM to maintenance mode by changing the ConfigMap of the Operator deployment: kubectl -n <namespace> edit cm <configmap> Example : kubectl edit cm -n dxns dx-deployment In the data section, add the following entry and save the changes: data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod restarts automatically. Please wait until the pod restarts before proceeding. Verify that persistence (read-write) and DAM pods are up and running, and are in maintenance mode. Use the following command to see the current status of the pods: kubectl -n <namespace> get pods If more than one DAM or persistence pod is running, scale down the pods to only one of each type. Adjust the dxctl property file: ``` dam.minreplicas:1 dam.maxreplicas:1 persist.minreplicas:1 persist.maxreplicas:1 ``` And apply it using the dxctl tool. ./dxctl -\u2013update -p deployment.properties The changes are applied and any replicas are terminated. Use kubectl to check the progress. Verify that DAM is in maintenance mode by running the following command: kubectl -n <namespace> logs <pod-name> Example : kubectl -n dxns logs dx-deployment-dam-0 If your output looks similar to the following, maintenance mode is enabled and you can continue: Maintenance mode is: true Listening for SIGTERM Maintenance mode is enabled. This mode solely starts the pod without any processes within it. Connect to the persistence pod. The following command opens a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-persistence-0 -n dxns -- /bin/bash Dump the current database using pg_dump : pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the shell in the persistence pod exit Download the dumped database from the persistence pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp Connect to the DAM pod. The following command opens a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-dam-0 -n dxns -- /bin/bash Compress the DAM binaries located in /opt/app/upload: tar -cvpzf backupml.tar.gz --exclude=/backupml.tar.gz --one-file-system --directory /opt/app/upload . Exit the shell in the DAM pod: exit Download the compressed DAM binaries from the DAM pod to your local system: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-dam-0:/opt/app/server-v1/backupml.tar.gz /tmp/backupml.tar.gz Restore your back up to the Helm-based deployment Important: Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file is done. You must enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except digitalAssetManagement and the persistence : applications: core: false runtimeController: false contentComposer: false designStudio: false digitalAssetManagement: true imageProcessor: false openLdap: false persistence: true remoteSearch: false ringApi: false ambassador: false Scale down persistence to a single node: scaling: replicas: persistenceNode: 1 You can now start the Helm deployment. If you're running the migration of DAM first: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example : helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment If core migration is done first: helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> Example : helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment The following outcomes are expected: The DAM and persistence pods are running and kept alive. The DAM application is not running. Upload the backup database. Use the following command to transfer the backup database to the remote persistence pod: kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example : kubectl cp /tmp/dxmediadb.dmp dxns-helm/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to the persistence (read-write) pod. Use the following command to open a shell in the running persistence pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns-helm -- /bin/bash You must drop the DAM database, if it exists. Disconnect all connections that use the database and drop any existing databases of the Helm deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the shell in the persistence pod: exit Connect to the DAM pod. Use the following command to open a shell in the running DAM pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-digital-asset-management-0 -n dxns-helm -- /bin/bash Restore the DAM binaries: tar -mpxf /tmp/backupml.tar.gz --directory /opt/app/upload rm /backupml.tar.gz Exit the shell in the DAM pod: exit Disable the migration mode and the deployment. Before you can start the final upgrade of the Helm deployment, some adjustments are needed: Disable the migration mode by updating the following value in custom-values.yaml migration: operatorToHelm: enabled: false Enable all relevant applications. You can now upgrade the Helm deployment. helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> Example : helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment Parent topic: Migrating from Operator to Helm deployments","title":"Migrate Digital Asset Management persistence and binaries"},{"location":"containerization/helm/helm_dam_migration_newDB/","text":"Migrate to new DAM DB in Helm-based deployments This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB. 1. Upgrade your existing Helm-based deployments to CF200 To perform the DAM DB migration you must first upgrade your existing DX deployment to CF200. Important: During upgrading to CF200, the Helm-based upgrade procedure detects the old deprecated DAM DB and notifies you with the following warning. The message indicates that the deployment is using an old DAM DB system that is deprecated, hence you must migrate to the new DAM DB. If you do not migrate to a new DAM DB, you might lose data during future DX updates. Warning message: Installation of HCL DX 95 CF200 done. This deployment is using an old DAM Database system and is deprecated. You must migrate this to the new DAM Database. If you receive this message, you must upgrade your DAM Database using the following steps; otherwise you can continue with the upgrade procedure. 2. Back up the existing DAM DB Back up of your existing DAM Database. Ensure that the DX pods are in running state before proceeding with the backup procedure. Connect to the Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns -- /bin/bash Dump the current database using the pg_dump command: pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit Download the dumped database to local system by using the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp 3. Migrate to new DB Enable the DAM DB migration mode to migrate your existing DAM DB to the new DB. Change the following configuration in your custom values.yaml file. Enable DAM Database Migration mode: ``` Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: true ``` Scale down persistence nodes to 1: scaling: # The default amount of replicas per application replicas: persistenceNode: 1 Perform an upgrade with the new configuration: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n <namespace> The upgrade will turn off the deprecated old Database system and deploy the new DAM Database system. 4. Restore DB from Old DB Backup Restore the data from the old database to the new database. Upload Old DB backup to persistence pod: You can now transfer the backup database to the remote Persistence container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Drop the DAM database if it exists: Disconnect all connections that use the database and drop any existing databases of the deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit 5. Disable DAM Database Migration mode and Scale the persistence nodes to 3 Once the restore procedure is completed, you can disable the DAM DB migration mode. You must also scale the persistence node to 3 for scalability and availability. Disable DAM Database Migration mode: ``` Flags to enable various migration modes migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: false ``` Scale up persistence nodes to 3: scaling: # The default amount of replicas per application replicas: persistenceNode: 3 Perform a helm upgrade with the updated values.yaml file: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n < namespace > On successful migration, you will receive the following message. Success message: Installation of HCL DX 95 CF200 done. See HCL Digital Experience product documentation website , for further information. You might have to wait for a few minutes until all the persistence pods and DAM pods are back to the running state. Parent topic: Update deployment to a later version","title":"Migrate to new DAM DB in Helm-based deployments"},{"location":"containerization/helm/helm_dam_migration_newDB/#migrate-to-new-dam-db-in-helm-based-deployments","text":"This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB.","title":"Migrate to new DAM DB in Helm-based deployments"},{"location":"containerization/helm/helm_dam_migration_newDB/#1-upgrade-your-existing-helm-based-deployments-to-cf200","text":"To perform the DAM DB migration you must first upgrade your existing DX deployment to CF200. Important: During upgrading to CF200, the Helm-based upgrade procedure detects the old deprecated DAM DB and notifies you with the following warning. The message indicates that the deployment is using an old DAM DB system that is deprecated, hence you must migrate to the new DAM DB. If you do not migrate to a new DAM DB, you might lose data during future DX updates. Warning message: Installation of HCL DX 95 CF200 done. This deployment is using an old DAM Database system and is deprecated. You must migrate this to the new DAM Database. If you receive this message, you must upgrade your DAM Database using the following steps; otherwise you can continue with the upgrade procedure.","title":"1. Upgrade your existing Helm-based deployments to CF200"},{"location":"containerization/helm/helm_dam_migration_newDB/#2-back-up-the-existing-dam-db","text":"Back up of your existing DAM Database. Ensure that the DX pods are in running state before proceeding with the backup procedure. Connect to the Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-rw-0 -n dxns -- /bin/bash Dump the current database using the pg_dump command: pg_dump dxmediadb > /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit Download the dumped database to local system by using the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> Example: kubectl cp dxns/dx-deployment-persistence-0:/tmp/dxmediadb.dmp /tmp/dxmediadb.dmp","title":"2. Back up the existing DAM DB"},{"location":"containerization/helm/helm_dam_migration_newDB/#3-migrate-to-new-db","text":"Enable the DAM DB migration mode to migrate your existing DAM DB to the new DB. Change the following configuration in your custom values.yaml file. Enable DAM Database Migration mode: ```","title":"3. Migrate to new DB"},{"location":"containerization/helm/helm_dam_migration_newDB/#flags-to-enable-various-migration-modes","text":"migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: true ``` Scale down persistence nodes to 1: scaling: # The default amount of replicas per application replicas: persistenceNode: 1 Perform an upgrade with the new configuration: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n <namespace> The upgrade will turn off the deprecated old Database system and deploy the new DAM Database system.","title":"Flags to enable various migration modes"},{"location":"containerization/helm/helm_dam_migration_newDB/#4-restore-db-from-old-db-backup","text":"Restore the data from the old database to the new database. Upload Old DB backup to persistence pod: You can now transfer the backup database to the remote Persistence container. kubectl cp <source-file> <namespace>/<pod-name>:<target-file> Example: kubectl cp /tmp/dxmediadb.dmp dxns/dx-deployment-persistence-node-0:/tmp/dxmediadb.dmp Connect to Persistence pod: The following command opens a shell in the running Persistence container. kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example: kubectl exec --stdin --tty pod/dx-deployment-persistence-node-0 -n dxns -- /bin/bash Drop the DAM database if it exists: Disconnect all connections that use the database and drop any existing databases of the deployment. echo \"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();\" | psql dropdb dxmediadb Restore the database from the previous backup: createdb -O dxuser dxmediadb psql dxmediadb < /tmp/dxmediadb.dmp Exit the Persistence container: Close the shell in the Persistence container. exit","title":"4. Restore DB from Old DB Backup"},{"location":"containerization/helm/helm_dam_migration_newDB/#5-disable-dam-database-migration-mode-and-scale-the-persistence-nodes-to-3","text":"Once the restore procedure is completed, you can disable the DAM DB migration mode. You must also scale the persistence node to 3 for scalability and availability. Disable DAM Database Migration mode: ```","title":"5. Disable DAM Database Migration mode and Scale the persistence nodes to 3"},{"location":"containerization/helm/helm_dam_migration_newDB/#flags-to-enable-various-migration-modes_1","text":"migration: damDB: # Enable for DAM Database migration from old DB to new DB enabled: false ``` Scale up persistence nodes to 3: scaling: # The default amount of replicas per application replicas: persistenceNode: 3 Perform a helm upgrade with the updated values.yaml file: helm upgrade dx-deployment . -f < your_custom_value_file.yaml > -n < namespace > On successful migration, you will receive the following message. Success message: Installation of HCL DX 95 CF200 done. See HCL Digital Experience product documentation website , for further information. You might have to wait for a few minutes until all the persistence pods and DAM pods are back to the running state. Parent topic: Update deployment to a later version","title":"Flags to enable various migration modes"},{"location":"containerization/helm/helm_deployment/","text":"Deploying container platforms using Helm Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Video: Deploy HCL DX 9.5 Container Update using Helm . About this task This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Note: Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic. Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Parent topic: Helm-based deployment","title":"Deploying on containers"},{"location":"containerization/helm/helm_deployment/#deploying-container-platforms-using-helm","text":"Learn to deploy HCL Digital Experience 9.5 CF196 and later release containers to Kubernetes using Helm as verified in Google Kubernetes Engine (GKE) . Beginning with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Video: Deploy HCL DX 9.5 Container Update using Helm .","title":"Deploying container platforms using Helm"},{"location":"containerization/helm/helm_deployment/#about-this-task","text":"This section provides administrators with all Helm-based deployment tasks to deploy HCL Digital Experience CF196 and later releases to supported Kubernetes platforms. This includes preparation, installation, and uninstallation of the deployments using Helm. Note: Beginning with HCL Digital Experience 9.5 Container Update CF199, migration from an Operator (dxctl) based deployment to a Helm deployment of Container Update CF199 or higher is supported. Reference the Help Center topic Migration from Operator (dxctl) to Helm deployment. for more information. Migration from earlier HCL Digital Experience 9.5 Container Update CF196 - CF198 Operator based deployments to Helm deployments is not supported. Follow these steps to prepare for and deploy HCL Digital Experience 9.5 CF196 and later release to Kubernetes using Helm, as verified in Google Kubernetes Engine (GKE) , and with HCL Digital Experience CF197 and later releases, the Helm deployment pattern is supported for new deployments to Red Hat Open Shift , Amazon Elastic Kubernetes Service (EKS) , and Microsoft Azure Kubernetes Service (AKS) . Before you begin: Refer to the latest HCL DX 9.5 Container Update image files list given in the Docker image list topic. Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Parent topic: Helm-based deployment","title":"About this task"},{"location":"containerization/helm/helm_extract_operator_properties/","text":"Prepare the Operator properties for migration This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. The Operator-based to Helm-based migration is a strict side-by-side migration. This section outlines the needed steps to configure your new Helm-based deployment with your old Operator-based deployment configuration settings. Once you have extracted the needed data, and have shut down your Operator-based deployment, you can apply your exported data in your new Helm-based deployment. Note that you are not migrating your DX Core database as you will reuse the same database instance in your Helm-based deployment. Important: Ensure that the current HCL DX 9.5 limitations and requirements , as well as limitations specific to a Helm deployment , are met. Ensure that you have followed the preparation process in Planning your container deployment using Helm , and that you have already created your custom-values.yaml. As with any migration activity, we recommend that you make backups of the data of your current environment before proceeding. See Backup and recovery procedures for more information. In case of any errors after migration, you can fall back to your previous Operator-based environment. See Migration to restore Core and DAM Operator deployment for more information. You must have the properties file you used with dxctl in your old Operator deployment. If you do not have the properties file, refer to the dxctl topic to extract the properties file from your existing deployment using the getProperties function. Ensure to prepare any other needed infrastructure-related items (like persistent volumes, Kubernetes load balancer configuration, etc.) before proceeding with migration to Helm. Optionally, you can perform a test deployment to make sure that all prerequisites and requirements for the Helm deployment are met. Follow the installation steps and check if all the functionality of the default deployment is accessible. If you do not prefer to do an initial test, you can skip to start with the Core and DAM migration immediately. Before migrating to Helm, you must migrate the configuration of your Operator-based deployment first. Follow this guidance to prepare the property mappings for your HCL DX 9.5 Operator-based deployment. You can reuse the values from your deployment.properties file in your new custom-values.yaml. Property mappings This section lists the mapping of the dxctl deployment.properties file with the custom-values.yaml. Note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you, or those that have been adjusted by you prior deploying the Operator with dxctl . dxctl deployment.properties custom-values.yaml Description dx.namespace Not applicable The namespace used for the deployment. This is handed directly to Helm through the command line interface. dx.name Not applicable The deployment name . This is handed directly to Helm through the command line interface. default.repository images.repository Defines the image repository for all container images. dx.pullpolicy images.pullPolicy Defines the image pull policy for all container images. <application>.image images.name.<application> Name of the container image. <application>.tag images.tag.<application> Name of the container tag. <application>.enabled applications.<application> Enables or disables specific applications. dx.pod.nodeselector nodeSelector.<application> NodeSelector used for pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance is tuned for authoring or not. composer.enabled applications.contentComposer Selects if Content Composer is deployed or not. dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management is deployed or not. persist.force-read Not applicable Read-only fallback enablement. This is always enabled in Helm. dxctl deployment.properties custom-values.yaml Description dx.volume volumes.core.profile.volumeName The name of the volume used for the DX core profile. dx.volume.size volumes.core.profile.requests.storage Size of the volume used for the DX core profile. dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for the DX core profile. dx.splitlogging: false Not applicable. Defines if the log directory uses a separate volume. This is always enabled in Helm. dx.logging.stgclass volumes.core.log.storageClassName StorageClass for the DX core logging volume. dx.logging.size volumes.core.log.requests.storage StorageClass for the DX core logging volume. dx.tranlogging Not applicable. Defines if the transaction log directory uses a separate volume. This is always enabled in Helm. dx.tranlogging.reclaim Not applicable. Reclaim policy for DX core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX core transaction log volume. dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX core transaction log volume. remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile. remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile. dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM. dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM. dxctl deployment.properties custom-values.yaml Description dx.path.contextroot networking.core.contextRoot Context root used for DX. dx.path.personalized networking.core.personalizedHome Personalized URL path for DX. dx.path.home networking.core.home Non-personalized URL path for DX. dx.deploy.host.override networking.core.host Host name to be used instead of the load balancer host name. dx.deploy.host.override.force Not applicable. Force the use of the override host. Obsolete in Helm. dx.config.cors / dam.config.cors networking.addon.<application>.corsOrigin CORS configuration for applications, can be configured per application in Helm. hybrid.enabled Not applicable. Defines if hybrid is enabled or not. Helm derives this from other networking and application settings. hybrid.url networking.core.host URL of the DX core instance in a hybrid deployment. hybrid.port networking.core.port Port of the DX core instance in a hybrid deployment. dxctl deployment.properties custom-values.yaml Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum number of pods when scaling is enabled. dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum number of pods when scaling is enabled. dx.replicas scaling.replicas.core Default number of pods when scaling is enabled. dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU target for autoscaling. dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum number of pods when scaling is enabled. api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum number of pods when scaling is enabled. api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU target for autoscaling. api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum number of pods when scaling is enabled. composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum number of pods when scaling is enabled. composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU target for autoscaling. composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum number of pods when scaling is enabled. dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum number of pods when scaling is enabled. dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU target for autoscaling. dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum number of pods when scaling is enabled. imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum number of pods when scaling is enabled. imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU target for autoscaling. imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dx.request.cpu resources.core.requests.cpu CPU request. dx.request.memory resources.core.requests.memory Memory request. dx.limit.cpu resources.core.limits.cpu CPU limit. dx.limit.memory resources.core.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description api.request.cpu resources.ringApi.requests.cpu CPU request. api.request.memory resources.ringApi.requests.memory Memory request. api.limit.cpu resources.ringApi.limits.cpu CPU limit. api.limit.memory resources.ringApi.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description composer.request.cpu resources.contentComposer.requests.cpu CPU request. composer.request.memory resources.contentComposer.requests.memory Memory request. composer.limit.cpu resources.contentComposer.limits.cpu CPU limit. composer.limit.memory resources.contentComposer.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU request. dam.request.memory resources.digitalAssetManagement.requests.memory Memory request. dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU limit. dam.limit.memory resources.digitalAssetManagement.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description persist.request.cpu resources.persistence.requests.cpu CPU request. persist.request.memory resources.persistence.requests.memory Memory request. persist.limit.cpu resources.persistence.limits.cpu CPU limit. persist.limit.memory resources.persistence.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU request. imgproc.request.memory resources.imageProcessor.requests.memory Memory request. imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU limit. imgproc.limit.memory resources.imageProcessor.limits.memory Memory limit. Parent topic: Migrating from Operator to Helm deployments","title":"Prepare the Operator properties for migration"},{"location":"containerization/helm/helm_extract_operator_properties/#prepare-the-operator-properties-for-migration","text":"This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. The Operator-based to Helm-based migration is a strict side-by-side migration. This section outlines the needed steps to configure your new Helm-based deployment with your old Operator-based deployment configuration settings. Once you have extracted the needed data, and have shut down your Operator-based deployment, you can apply your exported data in your new Helm-based deployment. Note that you are not migrating your DX Core database as you will reuse the same database instance in your Helm-based deployment. Important: Ensure that the current HCL DX 9.5 limitations and requirements , as well as limitations specific to a Helm deployment , are met. Ensure that you have followed the preparation process in Planning your container deployment using Helm , and that you have already created your custom-values.yaml. As with any migration activity, we recommend that you make backups of the data of your current environment before proceeding. See Backup and recovery procedures for more information. In case of any errors after migration, you can fall back to your previous Operator-based environment. See Migration to restore Core and DAM Operator deployment for more information. You must have the properties file you used with dxctl in your old Operator deployment. If you do not have the properties file, refer to the dxctl topic to extract the properties file from your existing deployment using the getProperties function. Ensure to prepare any other needed infrastructure-related items (like persistent volumes, Kubernetes load balancer configuration, etc.) before proceeding with migration to Helm. Optionally, you can perform a test deployment to make sure that all prerequisites and requirements for the Helm deployment are met. Follow the installation steps and check if all the functionality of the default deployment is accessible. If you do not prefer to do an initial test, you can skip to start with the Core and DAM migration immediately. Before migrating to Helm, you must migrate the configuration of your Operator-based deployment first. Follow this guidance to prepare the property mappings for your HCL DX 9.5 Operator-based deployment. You can reuse the values from your deployment.properties file in your new custom-values.yaml. Property mappings This section lists the mapping of the dxctl deployment.properties file with the custom-values.yaml. Note: You should only transfer settings that you have adjusted for your Operator deployment. It is not recommended to overwrite all Helm defaults with the defaults of the old Operator deployment. Only migrate settings that are relevant for you, or those that have been adjusted by you prior deploying the Operator with dxctl . dxctl deployment.properties custom-values.yaml Description dx.namespace Not applicable The namespace used for the deployment. This is handed directly to Helm through the command line interface. dx.name Not applicable The deployment name . This is handed directly to Helm through the command line interface. default.repository images.repository Defines the image repository for all container images. dx.pullpolicy images.pullPolicy Defines the image pull policy for all container images. <application>.image images.name.<application> Name of the container image. <application>.tag images.tag.<application> Name of the container tag. <application>.enabled applications.<application> Enables or disables specific applications. dx.pod.nodeselector nodeSelector.<application> NodeSelector used for pods, can now be done per application in Helm dx.config.authoring configuration.core.tuning.authoring Selects if the instance is tuned for authoring or not. composer.enabled applications.contentComposer Selects if Content Composer is deployed or not. dam.enabled applications.digitalAssetManagement Selects if Digital Asset Management is deployed or not. persist.force-read Not applicable Read-only fallback enablement. This is always enabled in Helm. dxctl deployment.properties custom-values.yaml Description dx.volume volumes.core.profile.volumeName The name of the volume used for the DX core profile. dx.volume.size volumes.core.profile.requests.storage Size of the volume used for the DX core profile. dx.storageclass volumes.core.profile.storageClassName StorageClass of the volume used for the DX core profile. dx.splitlogging: false Not applicable. Defines if the log directory uses a separate volume. This is always enabled in Helm. dx.logging.stgclass volumes.core.log.storageClassName StorageClass for the DX core logging volume. dx.logging.size volumes.core.log.requests.storage StorageClass for the DX core logging volume. dx.tranlogging Not applicable. Defines if the transaction log directory uses a separate volume. This is always enabled in Helm. dx.tranlogging.reclaim Not applicable. Reclaim policy for DX core transaction log volume. Determined by PV instead of Helm dx.tranlogging.stgclass volumes.core.tranlog.storageClassName StorageClass for the DX core transaction log volume. dx.tranlogging.size volumes.core.tranlog.requests.storage Size used for the DX core transaction log volume. remote.search.volume volumes.remoteSearch.prsprofile.volumeName Name of the volume used for the DX Remote Search profile. remote.search.stgclass volumes.remoteSearch.prsprofile.storageClassName StorageClass of the volume for the DX Remote Search profile. dam.volume volumes.digitalAssetManagement.binaries.volumeName Name of the volume used for DAM. dam.stgclass volumes.digitalAssetManagement.binaries.storageClassName StorageClass of the volume used for DAM. dxctl deployment.properties custom-values.yaml Description dx.path.contextroot networking.core.contextRoot Context root used for DX. dx.path.personalized networking.core.personalizedHome Personalized URL path for DX. dx.path.home networking.core.home Non-personalized URL path for DX. dx.deploy.host.override networking.core.host Host name to be used instead of the load balancer host name. dx.deploy.host.override.force Not applicable. Force the use of the override host. Obsolete in Helm. dx.config.cors / dam.config.cors networking.addon.<application>.corsOrigin CORS configuration for applications, can be configured per application in Helm. hybrid.enabled Not applicable. Defines if hybrid is enabled or not. Helm derives this from other networking and application settings. hybrid.url networking.core.host URL of the DX core instance in a hybrid deployment. hybrid.port networking.core.port Port of the DX core instance in a hybrid deployment. dxctl deployment.properties custom-values.yaml Description dx.minreplicas scaling.horizontalPodAutoScaler.core.minReplicas Minimum number of pods when scaling is enabled. dx.maxreplicas scaling.horizontalPodAutoScaler.core.maxReplicas Maximum number of pods when scaling is enabled. dx.replicas scaling.replicas.core Default number of pods when scaling is enabled. dx.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.core.targetCPUUtilizationPercentage CPU target for autoscaling. dx.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.core.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description api.minreplicas scaling.horizontalPodAutoScaler.ringApi.minReplicas Minimum number of pods when scaling is enabled. api.maxreplicas scaling.horizontalPodAutoScaler.ringApi.maxReplicas Maximum number of pods when scaling is enabled. api.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetCPUUtilizationPercentage CPU target for autoscaling. api.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.ringApi.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description composer.minreplicas scaling.horizontalPodAutoScaler.contentComposer.minReplicas Minimum number of pods when scaling is enabled. composer.maxreplicas scaling.horizontalPodAutoScaler.contentComposer.maxReplicas Maximum number of pods when scaling is enabled. composer.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetCPUUtilizationPercentage CPU target for autoscaling. composer.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.contentComposer.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dam.minreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.minReplicas Minimum number of pods when scaling is enabled. dam.maxreplicas scaling.horizontalPodAutoScaler.digitalAssetManagement.maxReplicas Maximum number of pods when scaling is enabled. dam.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetCPUUtilizationPercentage CPU target for autoscaling. dam.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.digitalAssetManagement.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description imgproc.minreplicas scaling.horizontalPodAutoScaler.imageProcessor.minReplicas Minimum number of pods when scaling is enabled. imgproc.maxreplicas scaling.horizontalPodAutoScaler.imageProcessor.maxReplicas Maximum number of pods when scaling is enabled. imgproc.targetcpuutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetCPUUtilizationPercentage CPU target for autoscaling. imgproc.targetmemoryutilizationpercent scaling.horizontalPodAutoScaler.imageProcessor.targetMemoryUtilizationPercentage Memory target for autoscaling. dxctl deployment.properties custom-values.yaml Description dx.request.cpu resources.core.requests.cpu CPU request. dx.request.memory resources.core.requests.memory Memory request. dx.limit.cpu resources.core.limits.cpu CPU limit. dx.limit.memory resources.core.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description api.request.cpu resources.ringApi.requests.cpu CPU request. api.request.memory resources.ringApi.requests.memory Memory request. api.limit.cpu resources.ringApi.limits.cpu CPU limit. api.limit.memory resources.ringApi.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description composer.request.cpu resources.contentComposer.requests.cpu CPU request. composer.request.memory resources.contentComposer.requests.memory Memory request. composer.limit.cpu resources.contentComposer.limits.cpu CPU limit. composer.limit.memory resources.contentComposer.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description dam.request.cpu resources.digitalAssetManagement.requests.cpu CPU request. dam.request.memory resources.digitalAssetManagement.requests.memory Memory request. dam.limit.cpu resources.digitalAssetManagement.limits.cpu CPU limit. dam.limit.memory resources.digitalAssetManagement.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description persist.request.cpu resources.persistence.requests.cpu CPU request. persist.request.memory resources.persistence.requests.memory Memory request. persist.limit.cpu resources.persistence.limits.cpu CPU limit. persist.limit.memory resources.persistence.limits.memory Memory limit. dxctl deployment.properties custom-values.yaml Description imgproc.request.cpu resources.imageProcessor.requests.cpu CPU request. imgproc.request.memory resources.imageProcessor.requests.memory Memory request. imgproc.limit.cpu resources.imageProcessor.limits.cpu CPU limit. imgproc.limit.memory resources.imageProcessor.limits.memory Memory limit. Parent topic: Migrating from Operator to Helm deployments","title":"Prepare the Operator properties for migration"},{"location":"containerization/helm/helm_fallback_migration_Operator_deployment/","text":"Migrate to restore Core and DAM Operator deployment This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Follow this guidance to create a backup to support the capability to restore the DX 9.5 Core and Digital Asset Management Operator deployment. Restore Core Operator deployment Connect to the Core pod. The following command opens a shell in the running core container: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Start the server. Navigate to the profile bin folder and run the startServer command. cd /opt/HCL/wp_profile/bin/./startServer.sh WebSphere_Portal Exit the core container. Close the shell in the core container. exit Reset the scaling of the Core pods. Reset the replication settings for Core to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dx.minreplicas:<min_number_of_replicas> dx.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and the core pods are started. Use the kubectl get pods command to check the progress. If the pods are not started after a few minutes, force the change to be applied using the following command: kubectl delete statefulset -n <namespace> dx-deployment Restore DAM Operator deployment Disable maintenance mode for DAM. Digital Asset Management must be started without maintenance mode to restore the running status. We achieve this by changing the ConfigMap of the Operator deployment. kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, remove maintenance_mode:true from the dx.deploy.dam.features entry and save the changes. If maintenance_mode:true is the only entry for this key, dx.deploy.dam.features can be removed completely. data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod will restart automatically after some seconds. Please wait until the pod is restarted before proceeding. Reset the scaling of the DAM and Persistence pods. Reset the replication settings for DAM and Persistence to the previous values if necessary. On the operator deployment adjust the DXCTL property file: ``` dam.minreplicas: dam.maxreplicas: persist.minreplicas: persist.maxreplicas: ``` and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties Parent topic: Migrating from Operator to Helm deployments","title":"Migrate to restore Core and DAM Operator deployment"},{"location":"containerization/helm/helm_fallback_migration_Operator_deployment/#migrate-to-restore-core-and-dam-operator-deployment","text":"This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Follow this guidance to create a backup to support the capability to restore the DX 9.5 Core and Digital Asset Management Operator deployment. Restore Core Operator deployment Connect to the Core pod. The following command opens a shell in the running core container: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash Example : kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Start the server. Navigate to the profile bin folder and run the startServer command. cd /opt/HCL/wp_profile/bin/./startServer.sh WebSphere_Portal Exit the core container. Close the shell in the core container. exit Reset the scaling of the Core pods. Reset the replication settings for Core to the previous values if necessary. On the operator deployment adjust the DXCTL property file: dx.minreplicas:<min_number_of_replicas> dx.maxreplicas:<max_number_of_replicas> and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and the core pods are started. Use the kubectl get pods command to check the progress. If the pods are not started after a few minutes, force the change to be applied using the following command: kubectl delete statefulset -n <namespace> dx-deployment Restore DAM Operator deployment Disable maintenance mode for DAM. Digital Asset Management must be started without maintenance mode to restore the running status. We achieve this by changing the ConfigMap of the Operator deployment. kubectl -n <namespace> edit cm <configmap> Example: kubectl edit cm -n dxns dx-deployment In the data section, remove maintenance_mode:true from the dx.deploy.dam.features entry and save the changes. If maintenance_mode:true is the only entry for this key, dx.deploy.dam.features can be removed completely. data: dx.deploy.dam.features: \"maintenance_mode:true\" After saving the changes, the DAM pod will restart automatically after some seconds. Please wait until the pod is restarted before proceeding. Reset the scaling of the DAM and Persistence pods. Reset the replication settings for DAM and Persistence to the previous values if necessary. On the operator deployment adjust the DXCTL property file: ``` dam.minreplicas: dam.maxreplicas: persist.minreplicas: persist.maxreplicas: ``` and apply it via DXCTL tool: ./dxctl -\u2013update -p deployment.properties Parent topic: Migrating from Operator to Helm deployments","title":"Migrate to restore Core and DAM Operator deployment"},{"location":"containerization/helm/helm_install_commands/","text":"Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Note: Deployment using Helm is supported for new deployments to Google Kubernetes Engine (GKE) only with DX 9.5 Container Update CF196. Deployment using Helm is supported for new deployments to Microsoft Azure Kubernetes Service, Amazon Elastic Kubernetes Service, and Red Hat OpenShift beginning with HCL DX Container Update CF197. Install command Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. To run the installation of your prepared configurations using Helm, use the following command: # Helm install command helm install -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. The -f path/to/your/custom-values.yaml must point to the custom-values.yaml you have created, which contains all deployment configuration. your-release-name is the Helm release name and prefixes all resources created in that installation, such as Pods, Services, and others. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Helm Chart that you have extracted as described earlier in the planning and preparation steps. After a successful deployment, Helm responds with the following message: NAME: dx LAST DEPLOYED: Thu Jun 17 14:27:58 2021 NAMESPACE: my-namespace STATUS: deployed REVISION: 1 TEST SUITE: None Hybrid deployment configuration If you are installing in a Hybrid deployment pattern, after successfully running the Helm deployment, you need to configure services in DX Core to be aware of the external host name of your hybrid DX environment. To complete these steps, log on to the DX Core Server and run the following Config Engine tasks to enable other DX applications. In the following examples, replace <host-url> with your corresponding external host name. Content Composer: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-headless-content -Dstatic.ui.url=https://<host-url>/dx/ui/content-ui/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Digital Asset Manager: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-media-library -Dstatic.ui.url=https://<host-url>/dx/ui/dam/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Design Studio (Beta): <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-content-sites -Dcontentsites.static.ui.url=https://<host-url>/dx/ui/site-manager/static -DWasPassword=<was-password> -DPortalAdminPwd=<admin-password> Default URLs post installation During the configuration process, you might need the following URLs to access different administration user interfaces. Use the following default URLs to access HCL Digital Experience (Portal and WCM), the WebSphere\u00ae Integrated Solutions Console, and the Configuration Wizard: HCL Digital Experience (Portal and WCM) https://yourserver/wps/portal IBM WebSphere\u00ae Integrated Solutions Console https://yourserver/ibm/console IBM WebSphere\u00ae Integrated Solutions Console for Remote Search https://yourserver:9043/ibm/console HCL Digital Experience Configuration Wizard https://yourserver/hcl/wizard Uninstall command To remove your HCL Digital Experience 9.5 deployment from your Cluster deployed using Helm, it is recommended that you use Helm uninstall. To run the uninstall, use the following command as shown in this example: # Helm uninstall command helm uninstall your-release-name -n my-namespace The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. your-release-name is the Helm release name you selected during installation. After a successful deployment, Helm responds with the following message: release \"your-release-name\" uninstalled (Optional) External DNS configuration After a successful Helm deployment in a hybrid deployment, you can add a load balancer or an external IP to the DNS registry. Run the following command to get a load balancer external IP: kubectl get services -n <your-namespace> For Amazon EKS, you must add the external IP to route53. (Optional) Automated host extraction As described in the Configure networking topic, there are instances wherein you do not know the resulting external IP or FQDN for your deployment and you left the host value empty. In that case, run a Helm upgrade command, and it automatically polls the Ambassador Ingress and extract the found IP or FQDN from there. The Helm Chart logic goes ahead and populates all application configuration with the correct settings. An example is provided below. You may use the following Helm upgrade command to trigger the automated host extraction: # Helm upgrade command helm upgrade -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz Additional reference Step by Step Guide: How to deploy HCL DX Container Update CF197 and higher to Microsoft Azure Kubernetes Service (AKS) using Helm Parent topic: Deploying container platforms using Helm","title":"Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm"},{"location":"containerization/helm/helm_install_commands/#install-and-uninstall-commands-for-hcl-dx-95-cf196-and-later-container-deployments-to-kubernetes-and-red-hat-openshift-platforms-using-helm","text":"The following are install and uninstall commands that are used to deploy or uninstall HCL Digital Experience 9.5 CF196 and later releases to Kubernetes and Red Hat OpenShift platforms using Helm. Note: Deployment using Helm is supported for new deployments to Google Kubernetes Engine (GKE) only with DX 9.5 Container Update CF196. Deployment using Helm is supported for new deployments to Microsoft Azure Kubernetes Service, Amazon Elastic Kubernetes Service, and Red Hat OpenShift beginning with HCL DX Container Update CF197.","title":"Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm"},{"location":"containerization/helm/helm_install_commands/#install-command","text":"Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. To run the installation of your prepared configurations using Helm, use the following command: # Helm install command helm install -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. The -f path/to/your/custom-values.yaml must point to the custom-values.yaml you have created, which contains all deployment configuration. your-release-name is the Helm release name and prefixes all resources created in that installation, such as Pods, Services, and others. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Helm Chart that you have extracted as described earlier in the planning and preparation steps. After a successful deployment, Helm responds with the following message: NAME: dx LAST DEPLOYED: Thu Jun 17 14:27:58 2021 NAMESPACE: my-namespace STATUS: deployed REVISION: 1 TEST SUITE: None","title":"Install command"},{"location":"containerization/helm/helm_install_commands/#hybrid-deployment-configuration","text":"If you are installing in a Hybrid deployment pattern, after successfully running the Helm deployment, you need to configure services in DX Core to be aware of the external host name of your hybrid DX environment. To complete these steps, log on to the DX Core Server and run the following Config Engine tasks to enable other DX applications. In the following examples, replace <host-url> with your corresponding external host name. Content Composer: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-headless-content -Dstatic.ui.url=https://<host-url>/dx/ui/content-ui/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Digital Asset Manager: <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-media-library -Dstatic.ui.url=https://<host-url>/dx/ui/dam/static -DWasPassword=<was- password> -DPortalAdminPwd=<admin-password> Design Studio (Beta): <Path to wp_profile>/ConfigEngine/ConfigEngine.sh enable-content-sites -Dcontentsites.static.ui.url=https://<host-url>/dx/ui/site-manager/static -DWasPassword=<was-password> -DPortalAdminPwd=<admin-password>","title":"Hybrid deployment configuration"},{"location":"containerization/helm/helm_install_commands/#default-urls-post-installation","text":"During the configuration process, you might need the following URLs to access different administration user interfaces. Use the following default URLs to access HCL Digital Experience (Portal and WCM), the WebSphere\u00ae Integrated Solutions Console, and the Configuration Wizard: HCL Digital Experience (Portal and WCM) https://yourserver/wps/portal IBM WebSphere\u00ae Integrated Solutions Console https://yourserver/ibm/console IBM WebSphere\u00ae Integrated Solutions Console for Remote Search https://yourserver:9043/ibm/console HCL Digital Experience Configuration Wizard https://yourserver/hcl/wizard","title":"Default URLs post installation"},{"location":"containerization/helm/helm_install_commands/#uninstall-command","text":"To remove your HCL Digital Experience 9.5 deployment from your Cluster deployed using Helm, it is recommended that you use Helm uninstall. To run the uninstall, use the following command as shown in this example: # Helm uninstall command helm uninstall your-release-name -n my-namespace The my-namespace is the namespace where your HCL Digital Experience 9.5 deployment is installed to. your-release-name is the Helm release name you selected during installation. After a successful deployment, Helm responds with the following message: release \"your-release-name\" uninstalled","title":"Uninstall command"},{"location":"containerization/helm/helm_install_commands/#optional-external-dns-configuration","text":"After a successful Helm deployment in a hybrid deployment, you can add a load balancer or an external IP to the DNS registry. Run the following command to get a load balancer external IP: kubectl get services -n <your-namespace> For Amazon EKS, you must add the external IP to route53.","title":"(Optional) External DNS configuration"},{"location":"containerization/helm/helm_install_commands/#optional-automated-host-extraction","text":"As described in the Configure networking topic, there are instances wherein you do not know the resulting external IP or FQDN for your deployment and you left the host value empty. In that case, run a Helm upgrade command, and it automatically polls the Ambassador Ingress and extract the found IP or FQDN from there. The Helm Chart logic goes ahead and populates all application configuration with the correct settings. An example is provided below. You may use the following Helm upgrade command to trigger the automated host extraction: # Helm upgrade command helm upgrade -n my-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"(Optional) Automated host extraction"},{"location":"containerization/helm/helm_install_commands/#additional-reference","text":"Step by Step Guide: How to deploy HCL DX Container Update CF197 and higher to Microsoft Azure Kubernetes Service (AKS) using Helm Parent topic: Deploying container platforms using Helm","title":"Additional reference"},{"location":"containerization/helm/helm_operations/","text":"Operations using Helm This topic provides operations guidance for DX administrators to manage DX 9.5 deployments using Helm, such a configuration updates, monitoring, and troubleshooting strategies. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file list topic. Update Helm deployment configuration This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. Accessing the ConfigWizard admin console in a container environment This topic describes how you can access the ConfigWizard admin console in a container environment from your local system. The ConfigWizard admin console opens to the TCP port number 10203, but this port cannot be accessed directly via the Kubernetes ingress controller. Hence, use the following instructions to access the ConfigWizard console. Troubleshooting your Helm deployment This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm. Parent topic: Helm-based deployment","title":"Operations using Helm"},{"location":"containerization/helm/helm_operations/#operations-using-helm","text":"This topic provides operations guidance for DX administrators to manage DX 9.5 deployments using Helm, such a configuration updates, monitoring, and troubleshooting strategies. For more information about Helm applications, consult the Helm documentation . Refer to the DX 9.5 Container component image listing in the DX 9.5 Docker Image file list topic. Update Helm deployment configuration This section describes how to update the configuration of an HCL Digital Experience 9.5 CF196 or later deployment to Kubernetes or OpenShift installed using Helm. Accessing the ConfigWizard admin console in a container environment This topic describes how you can access the ConfigWizard admin console in a container environment from your local system. The ConfigWizard admin console opens to the TCP port number 10203, but this port cannot be accessed directly via the Kubernetes ingress controller. Hence, use the following instructions to access the ConfigWizard console. Troubleshooting your Helm deployment This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm. Parent topic: Helm-based deployment","title":"Operations using Helm"},{"location":"containerization/helm/helm_operator_core_migration/","text":"Migrate the Core profile This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment. Follow this guidance to create a backup of the Core profile from your Operator deployment, and restore the profile to a Helm-based deployment. Back up from an Operator-based deployment Ensure that only one Core pod is running. Check how many pods are running with the following command: kubectl -n <namespace> get pods If more than one pod is running, scale down the Core pods so only one (1) pod is running. On the Operator deployment, adjust the dxctl property file: dx.maxreplicas:1 dx.minreplicas:1 then apply the changes in the updated property file using the dxctl tool: ./dxctl -\u2013update -p deployment.properties Once the changes are applied, any replicas will be terminated. You may check the progress by running the kubectl command. Connect to the Core pod . The following command opens a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Stop the Core application before a backup from the wp_profile is created. Navigate to the profile bin folder and run the stopServer command: cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> Note: While the server is stopped, the liveness probe returns a failure result to Kubernetes. Once the maximum allowed number of failures is reached, Kubernetes restarts the pod, closing any kubectl exec session and brings the Core pod back online. However, with the default liveness probe settings in full-deployment.properties , this process takes approximately two (2) hours to occur. If you need to adjust your liveness probe settings to allow time to perform the profile backup (for instance, because you have reduced them considerably from the default in your deployment), make the changes in your full-deployment.properties file, and then apply the changes using dxctl as described in Step 1. Compress the whole Core profile folder /opt/HCL/wp_profile by running the following command: cd /opt/HCL tar -cvpzf core_prof_95_CF199.tar.gz --exclude=/core_prof_95_CF199.tar.gz --one-file-system wp_profile Close the shell in the Core pod using the following command: exit Download the backup Core profile from the Core pod to your local system by running the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> For example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF199.tar.gz /tmp/core_prof_95_CF199.tar.gz (Optional) Make the old environment unavailable. Kubernetes eventually brings the Core pod back online by restarting it (see the note on liveness probes in Step 2.a). If you want to keep the Operator-based deployment unavailable to users after backing up the profile, you can do the following steps: Adjust the dxctl property file : dx.maxreplicas: 0 dx.minreplicas: 0 dx.replicas: 0 Apply it using the dxctl tool : ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and all the additional Core pods are terminated. You can use kubectl get pods to check the progress. If the additional Core pods are not terminated after a few minutes, use the following command to force the changes to be applied: kubectl delete statefulset -n <namespace> dx-deployment If you want to enable the Operator-based deployment Core pod again, set the values of the dxctl property file to values greater than zero and apply the changes using the dxctl tool. Restore your back up to the Helm-based deployment Important: Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file. Enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except runtimeController and the core : applications: core: true runtimeController: true contentComposer: false designStudio: false digitalAssetManagement: false imageProcessor: false openLdap: false persistence: false remoteSearch: false ringApi: false ambassador: false Start the Helm deployment. If you are running the migration of the Core first, run the following command: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: ``` helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment ``` If DAM migration is done first: ``` helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> ``` For example: ``` helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment ``` The following outcomes are expected: - The `core` pod is running and kept alive. - The `core` application is not running. - No default profile is created automatically. The folder `/opt/HCL/wp_profile` is empty. 2. **Upload the backup profile.** Use the following command to transfer the backup profile to the remote Core pod: ``` kubectl cp <source-file> <namespace>/<pod-name>:<target-file> ``` For example: ``` kubectl cp /tmp/core_prof_95_CF199.tar.gz dxns-helm/dx-deployment-core-0:/tmp/core_prof_95_CF199.tar.gz ``` 3. **Connect to the Core pod.** Use the following command to pine a shell in the running Core pod: ``` kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash ``` For example: ``` kubectl exec --stdin --tty pod/dx-deployment-core-0 -n dxns-helm -- /bin/bash ``` 1. **Extract the profile.** Move the core\\_prof\\_95\\_CF199.tar.gz from the /temp folder to the profile folder /opt/HCL/profiles before extracting it: ``` tar -xf /tmp/core_prof_95_CF199.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF199 rm /tmp/core_prof_95_CF199.tar.gz ``` 2. **Create a symbolink \\(symlink\\)** by running the following command: ``` rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF199 /opt/HCL/wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ``` 4. **Disable the migration mode and the deployment.** **Note:** If Digital Asset Management was used in the Operator-based deployment, you must follow the [DAM migration instructions](helm_dam_migration.md) before disabling migration mode. 1. **Disable the `migration` mode** by updating the following value in custom-values.yaml ``` migration: operatorToHelm: enabled: false ``` 2. **Enable all relevant applications.** 3. **Upgrade the Helm deployment** using the following command: ``` helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> ``` For example: ``` helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment ``` 5. **Reconfigure applications to use relative paths.** Coming from old Operator deployments, it can appear that the applications \\(Digital Asset Management, Content Composer, and Design Studio\\) are still configured to use absolute URLs for their rendering. If you use any of these applications, it is highly recommended that you reconfigure them to use relative paths. CAUTION: Reconfigure relative paths for applications that are active in your deployment. Otherwise, the command will fail. Replace the placeholders for `NAMESPACE`, `YOUR_WAS_ADMIN_USER`, and `YOUR_WAS_ADMIN_PASSWORD` with the corresponding values of your deployment. - For Content Composer: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-headless-content-pages -Dcc.static.ui.url=/dx/ui/content/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` - For Digital Asset Management: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-media-library-pages -Ddam.static.ui.url=/dx/ui/dam/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` - For Design Studio: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-content-sites-pages -Dcontentsites.ui.url=/dx/ui/site-manager/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` 6. **Create the secret with your TLS certificate for the Ambassador Ingress in your Helm-based deployment.** To make your migrated Helm deployment accessible, you need to configure the TLS certificate that is used by the Ambassador Ingress. See [Use certificate](helm_configure_networking.md) for more information. 7. \\(**Optional**\\) If you have changed the WAS/Portal Administrator user, update the corresponding secrets \\(`dx-deployment-was` and `dx-deployment-wps`\\) in the Helm-based deployment. 8. **\\(Optional\\)** Configure Remote Search. Skip this step if you have not used Remote Search in your Operator deployment, or if you have no plans to use Remote Search within your Helm-based deployment. Configure Remote Search and re-index your data again. See instructions on [how to configure Remote Search within a Kubernetes environment](kubernetes_remote_search.md). Parent topic: Migrating from Operator to Helm deployments","title":"Migrate the Core profile"},{"location":"containerization/helm/helm_operator_core_migration/#migrate-the-core-profile","text":"This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment. Follow this guidance to create a backup of the Core profile from your Operator deployment, and restore the profile to a Helm-based deployment. Back up from an Operator-based deployment Ensure that only one Core pod is running. Check how many pods are running with the following command: kubectl -n <namespace> get pods If more than one pod is running, scale down the Core pods so only one (1) pod is running. On the Operator deployment, adjust the dxctl property file: dx.maxreplicas:1 dx.minreplicas:1 then apply the changes in the updated property file using the dxctl tool: ./dxctl -\u2013update -p deployment.properties Once the changes are applied, any replicas will be terminated. You may check the progress by running the kubectl command. Connect to the Core pod . The following command opens a shell in the running Core pod: kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash For example: kubectl exec --stdin --tty pod/dx-deployment-0 -n dxns -- /bin/bash Stop the Core application before a backup from the wp_profile is created. Navigate to the profile bin folder and run the stopServer command: cd /opt/HCL/wp_profile/bin/ ./stopServer.sh WebSphere_Portal -username <username> -password <password> Note: While the server is stopped, the liveness probe returns a failure result to Kubernetes. Once the maximum allowed number of failures is reached, Kubernetes restarts the pod, closing any kubectl exec session and brings the Core pod back online. However, with the default liveness probe settings in full-deployment.properties , this process takes approximately two (2) hours to occur. If you need to adjust your liveness probe settings to allow time to perform the profile backup (for instance, because you have reduced them considerably from the default in your deployment), make the changes in your full-deployment.properties file, and then apply the changes using dxctl as described in Step 1. Compress the whole Core profile folder /opt/HCL/wp_profile by running the following command: cd /opt/HCL tar -cvpzf core_prof_95_CF199.tar.gz --exclude=/core_prof_95_CF199.tar.gz --one-file-system wp_profile Close the shell in the Core pod using the following command: exit Download the backup Core profile from the Core pod to your local system by running the following command: kubectl cp <namespace>/<pod-name>:<source-file> <target-file> For example: kubectl cp dxns/dx-deployment-0:opt/HCL/core_prof_95_CF199.tar.gz /tmp/core_prof_95_CF199.tar.gz (Optional) Make the old environment unavailable. Kubernetes eventually brings the Core pod back online by restarting it (see the note on liveness probes in Step 2.a). If you want to keep the Operator-based deployment unavailable to users after backing up the profile, you can do the following steps: Adjust the dxctl property file : dx.maxreplicas: 0 dx.minreplicas: 0 dx.replicas: 0 Apply it using the dxctl tool : ./dxctl -\u2013update -p deployment.properties The change is applied after a few seconds and all the additional Core pods are terminated. You can use kubectl get pods to check the progress. If the additional Core pods are not terminated after a few minutes, use the following command to force the changes to be applied: kubectl delete statefulset -n <namespace> dx-deployment If you want to enable the Operator-based deployment Core pod again, set the values of the dxctl property file to values greater than zero and apply the changes using the dxctl tool. Restore your back up to the Helm-based deployment Important: Ensure to note the requirements and limitations here . For the new Helm deployment, you must use a different Kubernetes namespace from the one used in the Operator-based deployment. Ensure that the Helm-based deployment is in the correct state before restoring a backup. Ensure that you have extracted the Kubernetes DX configuration from the Operator-based deployment to a valid custom-values.yaml file. Enable migration mode for operatorToHelm by adding or updating the following value in custom-values.yaml: migration: operatorToHelm: enabled: true Disable all the applications, except runtimeController and the core : applications: core: true runtimeController: true contentComposer: false designStudio: false digitalAssetManagement: false imageProcessor: false openLdap: false persistence: false remoteSearch: false ringApi: false ambassador: false Start the Helm deployment. If you are running the migration of the Core first, run the following command: helm install -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> For example: ``` helm install -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment ``` If DAM migration is done first: ``` helm upgrade -n <namespace> -f <custom-values.yaml> <prefix> <chart> ``` For example: ``` helm upgrade -n dxns-helm -f custom-values.yaml dx-deployment hcl-dx-deployment ``` The following outcomes are expected: - The `core` pod is running and kept alive. - The `core` application is not running. - No default profile is created automatically. The folder `/opt/HCL/wp_profile` is empty. 2. **Upload the backup profile.** Use the following command to transfer the backup profile to the remote Core pod: ``` kubectl cp <source-file> <namespace>/<pod-name>:<target-file> ``` For example: ``` kubectl cp /tmp/core_prof_95_CF199.tar.gz dxns-helm/dx-deployment-core-0:/tmp/core_prof_95_CF199.tar.gz ``` 3. **Connect to the Core pod.** Use the following command to pine a shell in the running Core pod: ``` kubectl exec --stdin --tty pod/<pod-name> -n <namespace> -- /bin/bash ``` For example: ``` kubectl exec --stdin --tty pod/dx-deployment-core-0 -n dxns-helm -- /bin/bash ``` 1. **Extract the profile.** Move the core\\_prof\\_95\\_CF199.tar.gz from the /temp folder to the profile folder /opt/HCL/profiles before extracting it: ``` tar -xf /tmp/core_prof_95_CF199.tar.gz --directory /opt/HCL/profiles mv /opt/HCL/profiles/wp_profile /opt/HCL/profiles/prof_95_CF199 rm /tmp/core_prof_95_CF199.tar.gz ``` 2. **Create a symbolink \\(symlink\\)** by running the following command: ``` rm -r /opt/HCL/wp_profile ln -s /opt/HCL/profiles/prof_95_CF199 /opt/HCL/wp_profile ``` 3. **Close the shell in the Core pod** using the following command: ``` exit ``` 4. **Disable the migration mode and the deployment.** **Note:** If Digital Asset Management was used in the Operator-based deployment, you must follow the [DAM migration instructions](helm_dam_migration.md) before disabling migration mode. 1. **Disable the `migration` mode** by updating the following value in custom-values.yaml ``` migration: operatorToHelm: enabled: false ``` 2. **Enable all relevant applications.** 3. **Upgrade the Helm deployment** using the following command: ``` helm upgrade -n <namespace> --create-namespace -f <custom-values.yaml> <prefix> <chart> ``` For example: ``` helm upgrade -n dxns-helm --create-namespace -f custom-values.yaml dx-deployment hcl-dx-deployment ``` 5. **Reconfigure applications to use relative paths.** Coming from old Operator deployments, it can appear that the applications \\(Digital Asset Management, Content Composer, and Design Studio\\) are still configured to use absolute URLs for their rendering. If you use any of these applications, it is highly recommended that you reconfigure them to use relative paths. CAUTION: Reconfigure relative paths for applications that are active in your deployment. Otherwise, the command will fail. Replace the placeholders for `NAMESPACE`, `YOUR_WAS_ADMIN_USER`, and `YOUR_WAS_ADMIN_PASSWORD` with the corresponding values of your deployment. - For Content Composer: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-headless-content-pages -Dcc.static.ui.url=/dx/ui/content/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` - For Digital Asset Management: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-media-library-pages -Ddam.static.ui.url=/dx/ui/dam/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` - For Design Studio: ``` kubectl exec -n <NAMESPACE> dx-deployment-core-0 -- bash -c 'cd /opt/HCL/ConfigEngine/ && ./ConfigEngine.sh action-deploy-content-sites-pages -Dcontentsites.ui.url=/dx/ui/site-manager/static -DWasPassword=<YOUR_WAS_ADMIN_USER> -DPortalAdminPwd=<YOUR_WAS_ADMIN_PASSWORD>' ``` 6. **Create the secret with your TLS certificate for the Ambassador Ingress in your Helm-based deployment.** To make your migrated Helm deployment accessible, you need to configure the TLS certificate that is used by the Ambassador Ingress. See [Use certificate](helm_configure_networking.md) for more information. 7. \\(**Optional**\\) If you have changed the WAS/Portal Administrator user, update the corresponding secrets \\(`dx-deployment-was` and `dx-deployment-wps`\\) in the Helm-based deployment. 8. **\\(Optional\\)** Configure Remote Search. Skip this step if you have not used Remote Search in your Operator deployment, or if you have no plans to use Remote Search within your Helm-based deployment. Configure Remote Search and re-index your data again. See instructions on [how to configure Remote Search within a Kubernetes environment](kubernetes_remote_search.md). Parent topic: Migrating from Operator to Helm deployments","title":"Migrate the Core profile"},{"location":"containerization/helm/helm_operator_migration/","text":"Migrating from Operator to Helm deployments This topic provides administrators the guidance to migrate HCL Digital Experience Container Update CF199 and later releases from Operator to Helm deployment. Prepare the Operator properties for migration This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. Migrate the Core profile This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment. Migrate Digital Asset Management persistence and binaries This section shows the guidance to back up and restore your DAM persistence and binaries. Migrate to restore Core and DAM Operator deployment This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Parent topic: Helm-based deployment","title":"Migrating from Operator to Helm deployments"},{"location":"containerization/helm/helm_operator_migration/#migrating-from-operator-to-helm-deployments","text":"This topic provides administrators the guidance to migrate HCL Digital Experience Container Update CF199 and later releases from Operator to Helm deployment. Prepare the Operator properties for migration This section shows the guidance to prepare the mapping of your Operator deployment properties, so you can reuse them in your Helm deployment. Migrate the Core profile This section shows the steps to migrate your Core profile. You can create a backup of the profile and restore it later in the Helm deployment. Migrate Digital Asset Management persistence and binaries This section shows the guidance to back up and restore your DAM persistence and binaries. Migrate to restore Core and DAM Operator deployment This section shows the steps necessary to revert a DX 9.5 Container Deployment to the previous Operator-based deployment in case of any error during the migration to Helm. Parent topic: Helm-based deployment","title":"Migrating from Operator to Helm deployments"},{"location":"containerization/helm/helm_overview/","text":"Overview of the Helm architecture This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process . Overview Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms. Helm Chart contents The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates for e.g. Kubernetes resources value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm. Helm deployment flow As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace. Deployment structure Basics per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used. DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Deploying DX 9.5 applications to container platforms using Helm This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. PersistentVolumes and related operations considerations This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Configuring DX 9.5 deployments to container platforms using Helm This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Scaling DX 9.5 container deployments using Helm This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Digital Asset Management persistence architecture This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. Parent topic: Helm-based deployment","title":"Overview of the Helm architecture"},{"location":"containerization/helm/helm_overview/#overview-of-the-helm-architecture","text":"This topic provides administrators with a high-level overview and important pre-requisite guidance to prepare your container environments for later deployments of the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment capabilities using Helm. Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm Charts. Using a Helm Chart deployment provides administrators a larger degree of transparency in the deployment operations than the operator-based deployment also available using the HCL DX dxctl process .","title":"Overview of the Helm architecture"},{"location":"containerization/helm/helm_overview/#overview","text":"Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Helm is a tool for managing Kubernetes applications and deployments. It allows for packaging all required resource definitions into a single package, called a Helm Chart. The Helm Charts provide a convenient way to define application deployments with a predefined set of configurable items. Furthermore, Helm Charts are written using declarative definitions, applying yaml structures and go templates. This approach provides administrators with transparency about the operations the Helm Chart is performing during the DX 9.5 container deployment. In addition to foundation packaging and installation capabilities, Helm can also be used to modify and upgrade existing deployments, if the Helm Charts are built to support this. Configuration changes and application upgrades can both be managed using Helm. For more information about Helm, please reference documentation available on Helm topics for Red Hat Red Hat OpenShift and Kubernetes container platforms.","title":"Overview"},{"location":"containerization/helm/helm_overview/#helm-chart-contents","text":"The HCL Digital Experience 9.5 Deployment Helm Chart (Helm Chart name: hcl-dx-deployment ) follows the standard Helm structures and guidelines. hcl-dx-deployment/ crds/ # Custom Resource definitions used for the Ambassador Ingress templates/ # The directory containing all Helm templates for e.g. Kubernetes resources value-samples/ # Contains sample value files for different types of deployments README.md # README with information on Helm Chart usage and references to further documentation values.yaml # Default chart configuration values values.schema.json # Defines the validation schema for values.yaml Chart.yaml # The Chart yaml file containing chart specific information templates : The templates directory contains all resource definitions, for example, Services and Pods. values.yaml : The values.yaml contains all default values for a deployment. It is possible to customize the deployment overwriting the default values of the values.yaml. values.schema.json : To validate the values entered for a deployment, the values.schema.json provides configuration whenever an install or upgrade is performed with Helm.","title":"Helm Chart contents"},{"location":"containerization/helm/helm_overview/#helm-deployment-flow","text":"As outlined in the flow chart, when performing an install (or upgrade), the Helm Chart reads the values.yaml (and any overridden values, either provided through Helm CLI parameters or additional values files) and perform a schema validation check. After the schema check is successfully performed, Helm runs the templating engine to create the Kubernetes resource definitions out of the templates inside the Helm Charts. As a last step, Helm accesses the Kubernetes or OpenShift Cluster and create the resulting Kubernetes resources in the desired namespace.","title":"Helm deployment flow"},{"location":"containerization/helm/helm_overview/#deployment-structure","text":"Basics per application structure Each deployed application follows a similar deployment structure, using a common set of OpenShift or Kubernetes resources that follow naming conventions. Some of the DX 9.5 applications may have a different setup based on their special requirements, for example, the Digital Asset Management component, and its persistence definitions. Stateful applications - Definition DX 9.5 container applications are managed by a StatefulSet, which controls the creation and life cycle of all pods it is responsible for. These Pods use Persistent Volumes for storing their application data, ConfigMaps to adjust application configuration, and Secrets to obtain access credentials. In front of all Pods is a Service which manages routing the traffic to the Pods. This Service is also called by the Ingress to fulfill incoming requests from outside the Kubernetes or OpenShift cluster. Stateless applications - Services Management Ingress and routing For accessing applications from the outside, we deploy an Ingress in form of an Ambassador. This Ingress routes the incoming requests to all application Services, which then distributes the requests to the corresponding Pods hosting the applications. Ambassador uses Mappings that are created by the DX 9.5 Helm deployment to decide which requests needs to be mapped to which application in the DX 9.5 deployment (back-end). When requests are initiated from outside the Kubernetes or OpenShift cluster, the Ambassador tries to fulfill that request by using the configured Mappings. If it finds a matching endpoint, it forwards the request to the corresponding Service, which then forwards the same requests to a Pod that is ready to fulfill the request. The Ambassador performs SSL termination and must be provided with a TLS secret inside Kubernetes that contains the SSL certificate used. DX 9.5 Core Interactions with Kubernetes This section provides more detailed information about how the HCL Digital Experience 9.5 Core container interacts with Kubernetes. Understanding this information may assist in interpreting observed behavior or in troubleshooting your HCL DX 9.5 Container deployments in Helm. Deploying DX 9.5 applications to container platforms using Helm This topic provides a list of all DX applications and resource definitions that configure the application runtime and are deployed to containers using Helm on OpenShift or Kubernetes platforms. As outlined in the overview, applications can also include ConfigMaps, Secrets, and Ingress. PersistentVolumes and related operations considerations This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Configuring DX 9.5 deployments to container platforms using Helm This topic covers details the configuration structure in the HCL Digital Experience 9.5 OpenShift and Kubernetes deployment with Helm. Scaling DX 9.5 container deployments using Helm This topic provides information to apply container scaling capabilities, and how scaling resources are handled within the HCL DX 9.5 deployment using Helm. Refer to HorizontalPodAutoscaler details in Kubernetes and Red Hat OpenShift documentation for more information on these services. Digital Asset Management persistence architecture This topic describes the components of the Digital Asset Management persistence. The updated DAM persistence feature is available from HCL Digital Experience 9.5 Container Update CF198 and later. Parent topic: Helm-based deployment","title":"Deployment structure"},{"location":"containerization/helm/helm_persistent_volume_claims/","text":"Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note: The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information. Persistent Volume Types Important note: Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count. Configuration parameters To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important note: Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to. Sample PVC configurations The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" Sample Persistent Volume definitions Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure PersistentVolumeClaims \\(PVCs\\)"},{"location":"containerization/helm/helm_persistent_volume_claims/#configure-persistentvolumeclaims-pvcs","text":"To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note: The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information.","title":"Configure PersistentVolumeClaims (PVCs)"},{"location":"containerization/helm/helm_persistent_volume_claims/#persistent-volume-types","text":"Important note: Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count.","title":"Persistent Volume Types"},{"location":"containerization/helm/helm_persistent_volume_claims/#configuration-parameters","text":"To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important note: Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to.","title":"Configuration parameters"},{"location":"containerization/helm/helm_persistent_volume_claims/#sample-pvc-configurations","text":"The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\"","title":"Sample PVC configurations"},{"location":"containerization/helm/helm_persistent_volume_claims/#sample-persistent-volume-definitions","text":"Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps. Parent topic: Deploying container platforms using Helm","title":"Sample Persistent Volume definitions"},{"location":"containerization/helm/helm_planning_deployment/","text":"Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Before proceeding with your HCL DX 9.5 deployment using Helm, review the following Help Center topics: Deploying HCL DX CF196 to container platforms using Helm for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 Container Update CF196 and later deployment using Helm . Prepare your HCL DX 9.5 target environment. This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 to Google Kubernetes Engine using Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Mandatory tasks: The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm. Prepare a namespace. Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster. You need to create a namespace in your Kubernetes cluster that contains all the resources related to your HCL DX 9.5 Container deployment. It is recommended that this is created before deployment as you may need to add an ImagePullSecret or configure the TLS certificate for the Ambassador Ingress before deployment. Identify a name for your namespace and create it using the following syntax: On Kubernetes platforms Kubectl ``` Command to create a namespace using kubectl This example creates a namespace called \"my-namespace\" kubectl create ns my-namespace ``` OpenShift For OpenShift, you must create a namespace with specific settings. Use the following namespace definition and save it as namespace.yaml. You must replace my-namespace in the template with the name of the namespace you are using. apiVersion: v1 kind: Namespace metadata: name: my-namespace annotations: openshift.io/sa.scc.mcs: \"s0:c24,c4\" openshift.io/sa.scc.supplemental-groups: \"1001/10000\" openshift.io/sa.scc.uid-range: \"1000/10000\" OpenShift client ``` Command to create namespace from template file oc apply -f namespace.yaml ``` Prepare the Helm deployment configuration file. Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. The configuration flow Helm provides multiple ways to define values that can be processed to run an installation. Processing involves a three-step approach, that is ordered sequentially within a hierarchy. Helm Chart values.yaml Every Helm Chart contains a values.yaml file. It defines all configurable parameters that a Helm Chart accepts and the default values that are used during an installation. If you do not provide any other configuration during an installation, Helm extracts all deployment information from the values.yaml file inside the Helm Chart. All parameters that were not overwritten using any other configuration methods return to their default values from the values.yaml file inside the Helm Chart. Custom value files Helm provides you with a way to maintain your own custom values files. You can specify a custom values file you want to use when running an installation. This custom values file only needs to contain the parameters that you want to overwrite with your preferred settings. Note: There is no need to have the same complete set of parameters inside your custom values file, as there are available by default in the Helm Chart values.yaml . As outlined previously in this section, everything that is not defined in your custom values file are applied using the defaults from values.yaml inside the Helm Charts. Please be aware that the parameters you can configure using your custom values file need to exactly align with those provided by the Helm Charts own values.yaml. You cannot configure anything that is not exposed in the values.yaml definition. Override parameters It is possible to define values using a --set parameter in the Helm CLI during the installation of a Helm Chart. Since there are many values that can be configured in the HCL Digital Experience deployment, we do not recommend this technique, since it makes installation commands very large and confusing. The default HCL DX 9.5 Container values.yaml file HCL DX 9.5 Helm Chart provides a default values.yaml, which contains all possible configuration parameters. To access this file, you may use the following command when you have the HCL DX 9.5 CF196 or later Helm Chart tar.gz file on hand: ``` Command to extract values.ymal from Helm Chart helm show values hcl-dx-deployment.tar.gz > values.yaml ``` The file contains all configurable parameters and their default values. You may use this file as a blueprint to create your own custom-values.yaml . You may also just rename the extracted values.yaml to custom-values.yaml . Note: Having a complete copy of the default values.yaml is not necessary and may bloat your configuration file with values that are already present in the DX Helm Chart. A custom configuration file Helm allows you to provide a custom configuration file during the installation or upgrade process. That file only overwrites settings that are defined within it. For parts of the configuration that are not defined in your custom configuration file, Helm returns to the default values in the values.yaml file inside the DX Helm Chart. This allows you to create a file that only overwrites settings that are required, keeping the overall size of your configuration file small and the maintainability high. This Help Center documentation refers to the custom configuration file as custom-values.yaml . You may name your custom configuration file as preferred. Load container images. This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same. Extract HCL Digital Experience 9.5 package. The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note: The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. ``` Unzip of HCL Digital Experience 9.5 CFxxx package unzip hcl-dx-kubernetes-v95-CF196.zip ``` The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: ``` hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt Notices file dx-dx-ambassador-image-154.tar.gz Image for the Ambassador Ingress hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for the Core Operator (not needed for Helm deployments) hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip Cloud deployment scripts incl. dxctl (not needed for Helm deployments) hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Content Composer hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for Core hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for the Digital Asset Management Operator (not needed for Helm deployments) hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Digital Asset Management hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip Sample UI for Experience API hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Image Processor hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz Image for OpenLDAP hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Digital Asset Management Persistence hcl-dx-redis-image-X.X.X.tar.gz Image for Ambassador Ingress Redis hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for Remote Search hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Ring API hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz Image for Runtime Controller hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz Helm Charts ``` Load images locally. To load the individual image files, you may use the following command: ``` Command to load container image into local repository docker load < image-file-name.tar.gz docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz ``` If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: ``` Command to load all images at once Since HCL Digital Experience images are all containing the word \"images\", we can filter for fitting tar.gz files ls -f | grep image | xargs -L 1 docker load -i ``` This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: ``` List all images docker images Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Re-tag images. If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Important Note: Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: ``` Re-tag an existing loaded image docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION Example command for DX Core: docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: ``` Command to prefix all HCL Digital Experience container images export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' ``` The output may be verified by using the following command: ``` List all images docker images Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Push to repository. You may use the following command to push the container images to your repository: ``` Push the new tagged images docker push NEW_IMAGE_TAG:VERSION Example command for core: docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to push all your locally processed images, you may use the following command: ``` Command to push all HCL Digital Experience images to a remote repository export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" Push the images, first we filter for the ones necessary Second we execute a docker push for each image docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' ``` After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster. Adjust deployment configuration. After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note: If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. ``` Fill in the values fitting to your configuration Ensure to use the correct image version tags images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\" ``` Additional tasks: If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Configure Networking topic for instructions on how to configure this. Refer to PersistentVolumeClaims (PVCs) for the next steps. Parent topic: Deploying container platforms using Helm","title":"Planning your container deployment using Helm"},{"location":"containerization/helm/helm_planning_deployment/#planning-your-container-deployment-using-helm","text":"Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Before proceeding with your HCL DX 9.5 deployment using Helm, review the following Help Center topics: Deploying HCL DX CF196 to container platforms using Helm for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 Container Update CF196 and later deployment using Helm . Prepare your HCL DX 9.5 target environment. This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 to Google Kubernetes Engine using Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Mandatory tasks: The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm. Prepare a namespace. Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster. You need to create a namespace in your Kubernetes cluster that contains all the resources related to your HCL DX 9.5 Container deployment. It is recommended that this is created before deployment as you may need to add an ImagePullSecret or configure the TLS certificate for the Ambassador Ingress before deployment. Identify a name for your namespace and create it using the following syntax: On Kubernetes platforms Kubectl ```","title":"Planning your container deployment using Helm"},{"location":"containerization/helm/helm_planning_deployment/#command-to-create-a-namespace-using-kubectl","text":"","title":"Command to create a namespace using kubectl"},{"location":"containerization/helm/helm_planning_deployment/#this-example-creates-a-namespace-called-my-namespace","text":"kubectl create ns my-namespace ``` OpenShift For OpenShift, you must create a namespace with specific settings. Use the following namespace definition and save it as namespace.yaml. You must replace my-namespace in the template with the name of the namespace you are using. apiVersion: v1 kind: Namespace metadata: name: my-namespace annotations: openshift.io/sa.scc.mcs: \"s0:c24,c4\" openshift.io/sa.scc.supplemental-groups: \"1001/10000\" openshift.io/sa.scc.uid-range: \"1000/10000\" OpenShift client ```","title":"This example creates a namespace called \"my-namespace\""},{"location":"containerization/helm/helm_planning_deployment/#command-to-create-namespace-from-template-file","text":"oc apply -f namespace.yaml ``` Prepare the Helm deployment configuration file. Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. The configuration flow Helm provides multiple ways to define values that can be processed to run an installation. Processing involves a three-step approach, that is ordered sequentially within a hierarchy. Helm Chart values.yaml Every Helm Chart contains a values.yaml file. It defines all configurable parameters that a Helm Chart accepts and the default values that are used during an installation. If you do not provide any other configuration during an installation, Helm extracts all deployment information from the values.yaml file inside the Helm Chart. All parameters that were not overwritten using any other configuration methods return to their default values from the values.yaml file inside the Helm Chart. Custom value files Helm provides you with a way to maintain your own custom values files. You can specify a custom values file you want to use when running an installation. This custom values file only needs to contain the parameters that you want to overwrite with your preferred settings. Note: There is no need to have the same complete set of parameters inside your custom values file, as there are available by default in the Helm Chart values.yaml . As outlined previously in this section, everything that is not defined in your custom values file are applied using the defaults from values.yaml inside the Helm Charts. Please be aware that the parameters you can configure using your custom values file need to exactly align with those provided by the Helm Charts own values.yaml. You cannot configure anything that is not exposed in the values.yaml definition. Override parameters It is possible to define values using a --set parameter in the Helm CLI during the installation of a Helm Chart. Since there are many values that can be configured in the HCL Digital Experience deployment, we do not recommend this technique, since it makes installation commands very large and confusing. The default HCL DX 9.5 Container values.yaml file HCL DX 9.5 Helm Chart provides a default values.yaml, which contains all possible configuration parameters. To access this file, you may use the following command when you have the HCL DX 9.5 CF196 or later Helm Chart tar.gz file on hand: ```","title":"Command to create namespace from template file"},{"location":"containerization/helm/helm_planning_deployment/#command-to-extract-valuesymal-from-helm-chart","text":"helm show values hcl-dx-deployment.tar.gz > values.yaml ``` The file contains all configurable parameters and their default values. You may use this file as a blueprint to create your own custom-values.yaml . You may also just rename the extracted values.yaml to custom-values.yaml . Note: Having a complete copy of the default values.yaml is not necessary and may bloat your configuration file with values that are already present in the DX Helm Chart. A custom configuration file Helm allows you to provide a custom configuration file during the installation or upgrade process. That file only overwrites settings that are defined within it. For parts of the configuration that are not defined in your custom configuration file, Helm returns to the default values in the values.yaml file inside the DX Helm Chart. This allows you to create a file that only overwrites settings that are required, keeping the overall size of your configuration file small and the maintainability high. This Help Center documentation refers to the custom configuration file as custom-values.yaml . You may name your custom configuration file as preferred. Load container images. This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same. Extract HCL Digital Experience 9.5 package. The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note: The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. ```","title":"Command to extract values.ymal from Helm Chart"},{"location":"containerization/helm/helm_planning_deployment/#unzip-of-hcl-digital-experience-95-cfxxx-package","text":"unzip hcl-dx-kubernetes-v95-CF196.zip ``` The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: ``` hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt","title":"Unzip of HCL Digital Experience 9.5 CFxxx package"},{"location":"containerization/helm/helm_planning_deployment/#notices-file","text":"dx-dx-ambassador-image-154.tar.gz","title":"Notices file"},{"location":"containerization/helm/helm_planning_deployment/#image-for-the-ambassador-ingress","text":"hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for the Ambassador Ingress"},{"location":"containerization/helm/helm_planning_deployment/#image-for-the-core-operator-not-needed-for-helm-deployments","text":"hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip","title":"Image for the Core Operator (not needed for Helm deployments)"},{"location":"containerization/helm/helm_planning_deployment/#cloud-deployment-scripts-incl-dxctl-not-needed-for-helm-deployments","text":"hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Cloud deployment scripts incl. dxctl (not needed for Helm deployments)"},{"location":"containerization/helm/helm_planning_deployment/#image-for-content-composer","text":"hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Content Composer"},{"location":"containerization/helm/helm_planning_deployment/#image-for-core","text":"hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Core"},{"location":"containerization/helm/helm_planning_deployment/#image-for-the-digital-asset-management-operator-not-needed-for-helm-deployments","text":"hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for the Digital Asset Management Operator (not needed for Helm deployments)"},{"location":"containerization/helm/helm_planning_deployment/#image-for-digital-asset-management","text":"hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip","title":"Image for Digital Asset Management"},{"location":"containerization/helm/helm_planning_deployment/#sample-ui-for-experience-api","text":"hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Sample UI for Experience API"},{"location":"containerization/helm/helm_planning_deployment/#image-for-image-processor","text":"hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz","title":"Image for Image Processor"},{"location":"containerization/helm/helm_planning_deployment/#image-for-openldap","text":"hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for OpenLDAP"},{"location":"containerization/helm/helm_planning_deployment/#image-for-digital-asset-management-persistence","text":"hcl-dx-redis-image-X.X.X.tar.gz","title":"Image for Digital Asset Management Persistence"},{"location":"containerization/helm/helm_planning_deployment/#image-for-ambassador-ingress-redis","text":"hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Ambassador Ingress Redis"},{"location":"containerization/helm/helm_planning_deployment/#image-for-remote-search","text":"hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for Remote Search"},{"location":"containerization/helm/helm_planning_deployment/#image-for-ring-api","text":"hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz","title":"Image for Ring API"},{"location":"containerization/helm/helm_planning_deployment/#image-for-runtime-controller","text":"hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz","title":"Image for Runtime Controller"},{"location":"containerization/helm/helm_planning_deployment/#helm-charts","text":"``` Load images locally. To load the individual image files, you may use the following command: ```","title":"Helm Charts"},{"location":"containerization/helm/helm_planning_deployment/#command-to-load-container-image-into-local-repository","text":"","title":"Command to load container image into local repository"},{"location":"containerization/helm/helm_planning_deployment/#docker-load-image-file-nametargz","text":"docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz ``` If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: ```","title":"docker load &lt; image-file-name.tar.gz"},{"location":"containerization/helm/helm_planning_deployment/#command-to-load-all-images-at-once","text":"","title":"Command to load all images at once"},{"location":"containerization/helm/helm_planning_deployment/#since-hcl-digital-experience-images-are-all-containing-the-word-images","text":"","title":"Since HCL Digital Experience images are all containing the word \"images\","},{"location":"containerization/helm/helm_planning_deployment/#we-can-filter-for-fitting-targz-files","text":"ls -f | grep image | xargs -L 1 docker load -i ``` This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: ```","title":"we can filter for fitting tar.gz files"},{"location":"containerization/helm/helm_planning_deployment/#list-all-images","text":"docker images","title":"List all images"},{"location":"containerization/helm/helm_planning_deployment/#command-output-minified-example","text":"REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Re-tag images. If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Important Note: Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: ```","title":"Command output (minified, example)"},{"location":"containerization/helm/helm_planning_deployment/#re-tag-an-existing-loaded-image","text":"","title":"Re-tag an existing loaded image"},{"location":"containerization/helm/helm_planning_deployment/#docker-tag-old_image_pathversion-new_image_tagversion","text":"","title":"docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION"},{"location":"containerization/helm/helm_planning_deployment/#example-command-for-dx-core","text":"docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: ```","title":"Example command for DX Core:"},{"location":"containerization/helm/helm_planning_deployment/#command-to-prefix-all-hcl-digital-experience-container-images","text":"","title":"Command to prefix all HCL Digital Experience container images"},{"location":"containerization/helm/helm_planning_deployment/#export-the-prefix-for-the-repository-structure-without-tailing-slash","text":"export REMOTE_REPO_PREFIX=\"my/test/repository\"","title":"export the prefix for the repository structure, without tailing slash"},{"location":"containerization/helm/helm_planning_deployment/#first-we-list-all-hcl-digital-experience-95-images-then-we-remove-the-first-line-containing-the-header","text":"","title":"First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header"},{"location":"containerization/helm/helm_planning_deployment/#then-we-execute-the-docker-tag-command-prefixing-each-image-with-the-remote_repo_prefix","text":"docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' ``` The output may be verified by using the following command: ```","title":"Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX"},{"location":"containerization/helm/helm_planning_deployment/#list-all-images_1","text":"docker images","title":"List all images"},{"location":"containerization/helm/helm_planning_deployment/#command-output-minified-example_1","text":"REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Push to repository. You may use the following command to push the container images to your repository: ```","title":"Command output (minified, example)"},{"location":"containerization/helm/helm_planning_deployment/#push-the-new-tagged-images","text":"","title":"Push the new tagged images"},{"location":"containerization/helm/helm_planning_deployment/#docker-push-new_image_tagversion","text":"","title":"docker push NEW_IMAGE_TAG:VERSION"},{"location":"containerization/helm/helm_planning_deployment/#example-command-for-core","text":"docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to push all your locally processed images, you may use the following command: ```","title":"Example command for core:"},{"location":"containerization/helm/helm_planning_deployment/#command-to-push-all-hcl-digital-experience-images-to-a-remote-repository","text":"","title":"Command to push all HCL Digital Experience images to a remote repository"},{"location":"containerization/helm/helm_planning_deployment/#export-the-prefix-for-the-repository-structure-without-tailing-slash_1","text":"export REMOTE_REPO_PREFIX=\"my/test/repository\"","title":"export the prefix for the repository structure, without tailing slash"},{"location":"containerization/helm/helm_planning_deployment/#push-the-images-first-we-filter-for-the-ones-necessary","text":"","title":"Push the images, first we filter for the ones necessary"},{"location":"containerization/helm/helm_planning_deployment/#second-we-execute-a-docker-push-for-each-image","text":"docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' ``` After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster. Adjust deployment configuration. After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note: If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. ```","title":"Second we execute a docker push for each image"},{"location":"containerization/helm/helm_planning_deployment/#fill-in-the-values-fitting-to-your-configuration","text":"","title":"Fill in the values fitting to your configuration"},{"location":"containerization/helm/helm_planning_deployment/#ensure-to-use-the-correct-image-version-tags","text":"images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\" ``` Additional tasks: If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Configure Networking topic for instructions on how to configure this. Refer to PersistentVolumeClaims (PVCs) for the next steps. Parent topic: Deploying container platforms using Helm","title":"Ensure to use the correct image version tags"},{"location":"containerization/helm/helm_troubleshooting/","text":"Troubleshooting your Helm deployment This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm. Logs Access to the HCL Digital Experience 9.5 component logs is important for maintaining and troubleshooting both your container platform environments and your custom applications. It is also essential for supplying information that may be requested by HCL Software Support . Most component logs can be accessed directly on the Kubernetes or OpenShift platforms via the kubectl logs or OpenShift log access commands. The DX 9.5 Core component has additional important log files that are stored on persistent volumes, and need to be retrieved in a different manner. Accessing DX 9.5 container logs on Kubernetes or OpenShift All container logs from DX 9.5 pods in a deployment with Helm can be combined into a single output using the commands: kubectl logs -n your-namespace -l release=your-release-name --tail=-1 In the example above, the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command. Retrieving additional DX Core logs Useful additional DX Core logs are stored on persistent volumes. To retrieve these, repeat the command below for each DX Core pod: kubectl cp -n your-namespace pod-name:opt/HCL/wp_profile/logs/WebSphere_Portal/ . In the example above the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and pod-name is the particular DX Core pod from which you wish to retrieve logs. Events Pod events can give useful information for troubleshooting, such as why certain pods are not running. To get the events for a pod, you can use the following command: kubectl describe pod -n your-namespace pod-name In the above example, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and pod-name is the particular pod that you wish to examine. PersistentVolumeClaims (PVCs) One of the more common reasons for a pod not starting that can be identified via the pod events (see above) is that it has unbound persistent volume claims. To understand why the claims have not been fulfilled, it is useful to list both the current persistent volume claims and the current persistent volumes and to compare the two. When comparing, it is useful to check mismatches in storage class, access modes and capacity between available volumes and unfulfilled claims. The commands to list these resources are: kubectl get pvc -n your-namespace kubectl get pv In the above example, your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed. Retrieving the deployment configuration In addition to logs, HCL Support may also request configuration information about your deployment. This is can be obtained using kubectl describe commands for different classes of objects and a selector to get all for your deployment. The most likely object types are given in the examples below: kubectl describe pods -n your-namespace -l release=your-release-name kubectl describe deployments -n your-namespace -l release=your-release-name kubectl describe statefulsets -n your-namespace -l release=your-release-name kubectl describe secrets -n your-namespace -l release=your-release-name kubectl describe services -n your-namespace -l release=your-release-name kubectl describe mappings -n your-namespace -l release=your-release-name kubectl describe configmaps -n your-namespace -l release=your-release-name In the above examples, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file by appending > some-file-name to any command. Configure and access logs in Helm This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitor the Digital Experience deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments. Parent topic: Operations using Helm","title":"Troubleshooting your Helm deployment"},{"location":"containerization/helm/helm_troubleshooting/#troubleshooting-your-helm-deployment","text":"This section shows how to find and resolve issues when deploying HCL DX 9.5 CF196 and later releases using Helm.","title":"Troubleshooting your Helm deployment"},{"location":"containerization/helm/helm_troubleshooting/#logs","text":"Access to the HCL Digital Experience 9.5 component logs is important for maintaining and troubleshooting both your container platform environments and your custom applications. It is also essential for supplying information that may be requested by HCL Software Support . Most component logs can be accessed directly on the Kubernetes or OpenShift platforms via the kubectl logs or OpenShift log access commands. The DX 9.5 Core component has additional important log files that are stored on persistent volumes, and need to be retrieved in a different manner.","title":"Logs"},{"location":"containerization/helm/helm_troubleshooting/#accessing-dx-95-container-logs-on-kubernetes-or-openshift","text":"All container logs from DX 9.5 pods in a deployment with Helm can be combined into a single output using the commands: kubectl logs -n your-namespace -l release=your-release-name --tail=-1 In the example above, the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file for convenience by appending > some-file-name to the command.","title":"Accessing DX 9.5 container logs on Kubernetes or OpenShift"},{"location":"containerization/helm/helm_troubleshooting/#retrieving-additional-dx-core-logs","text":"Useful additional DX Core logs are stored on persistent volumes. To retrieve these, repeat the command below for each DX Core pod: kubectl cp -n your-namespace pod-name:opt/HCL/wp_profile/logs/WebSphere_Portal/ . In the example above the your-namespace reference is the namespace in which your HCL Digital Experience 9.5 deployment is installed and pod-name is the particular DX Core pod from which you wish to retrieve logs.","title":"Retrieving additional DX Core logs"},{"location":"containerization/helm/helm_troubleshooting/#events","text":"Pod events can give useful information for troubleshooting, such as why certain pods are not running. To get the events for a pod, you can use the following command: kubectl describe pod -n your-namespace pod-name In the above example, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and pod-name is the particular pod that you wish to examine.","title":"Events"},{"location":"containerization/helm/helm_troubleshooting/#persistentvolumeclaims-pvcs","text":"One of the more common reasons for a pod not starting that can be identified via the pod events (see above) is that it has unbound persistent volume claims. To understand why the claims have not been fulfilled, it is useful to list both the current persistent volume claims and the current persistent volumes and to compare the two. When comparing, it is useful to check mismatches in storage class, access modes and capacity between available volumes and unfulfilled claims. The commands to list these resources are: kubectl get pvc -n your-namespace kubectl get pv In the above example, your-namespace is the namespace in which your HCL Digital Experience 9.5 deployment is installed.","title":"PersistentVolumeClaims (PVCs)"},{"location":"containerization/helm/helm_troubleshooting/#retrieving-the-deployment-configuration","text":"In addition to logs, HCL Support may also request configuration information about your deployment. This is can be obtained using kubectl describe commands for different classes of objects and a selector to get all for your deployment. The most likely object types are given in the examples below: kubectl describe pods -n your-namespace -l release=your-release-name kubectl describe deployments -n your-namespace -l release=your-release-name kubectl describe statefulsets -n your-namespace -l release=your-release-name kubectl describe secrets -n your-namespace -l release=your-release-name kubectl describe services -n your-namespace -l release=your-release-name kubectl describe mappings -n your-namespace -l release=your-release-name kubectl describe configmaps -n your-namespace -l release=your-release-name In the above examples, your-namespace is the namespace in which your HCL Digital Experience deployment is installed and your-release-name is the Helm release name you used when installing. On UNIX-based operating systems, the output can be directed to a file by appending > some-file-name to any command. Configure and access logs in Helm This topic shows you how to configure logging in Helm, as well as how to access Kubernetes container logs. Basic monitoring This topic describes options for monitoring an HCL Digital Experience 9.5 Kubernetes deployments installed using Helm. Monitor the Digital Experience deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments. Parent topic: Operations using Helm","title":"Retrieving the deployment configuration"},{"location":"containerization/helm/helm_update_deployment/","text":"Update deployment to a later version This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. To proceed, administrators should have prepared the container platform cluster, together with the HCL DX 9.5 container deployment custom-values.yaml using the following guidance, Planning your container deployment using Helm , and then install your deployment using the instructions in Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm . Important: As of HCL DX 9.5 Container Update CF197, you can use this process to update a DX 9.5 deployment from Container Update CF196 on the Google Kubernetes Engine (GKE) platform. Support to update DX 9.5 197 container deployments using Helm to CF198 and later DX 9.5 container versions is provided for Red Hat OpenShift, Amazon EKS, Azure AKS, as well as Google GKE platforms beginning with Container Update CF198. Follow the guidance in this section to update the HCL DX 9.5 container release version CF197 and later deployment, to Kubernetes or Red Hat OpenShift that was installed using Helm. These instructions assume that you have made all configuration changes using the recommended Helm upgrade route described in Updating the DX 9.5 Deployment Configuration . This ensures that your custom-values.yaml file is an updated description of the configuration of your environment. If that is not the case, you must update your custom-values.yaml file first with all configuration changes. Update Ambassador CRDs when updating from CF199 or earlier When updating CF199 or earlier, you need to update the ambassador CRDs to a newer version. As a known pattern, Helm will not update CRDs by design. Important: The CRDs are currently backward compatible. It is still possible to run or perform an install of an older version of DX on that cluster, even when the CRDs are at the latest version. Note: Do not remove/downgrade the CF200 CRDs to an older version while having CF200 or later deployments running on your cluster. Prerequisites: You need to have tar and HCL DX Helm Chart downloaded. Your kubectl configuration must point to the target cluster and the user requires Cluster Admin Access to access it. Note: If you changed the Portal or WAS Administrator user, please ensure you have the correct values for security.core.wasUser/wasPassword and wpsUser/wpsPassword in custom-values.yaml. Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in /opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Procedure to update CRDs : Locate the hcl-dx-deployment-*.tgz in your downloaded package. Execute the following commands to update the CRDs: ``` Extract CRD directory from downloaded Helm Chart tar vxf hcl-dx-deployment-*.tgz hcl-dx-deployment/crds Apply extracted CRDs on cluster level kubectl apply -f hcl-dx-deployment/crds ``` After the action is completed, you can proceed with the DX update procedure. This step is required to be executed only once. Populate your repository with the new images Download the new HCL DX 9.5 container update images you need to upgrade and ensure that they are available in the image repository specified in your custom-values.yaml file. See the Docker image list for the latest HCL DX 9.5 container update images available. Download the Helm charts for the version to be installed Download the Helm charts corresponding to the HCL DX 9.5 container versions you want to install. You must always use the Helm charts that correspond to the container versions you are installing or to which you are upgrading. (Optional) Remove the operatorToHelm property from the existing custom-values.yaml file before upgrading to CF200 and later If you have CF199 deployed, ensure that you remove the operatorToHelm configuration from the custom-values.yaml file before upgrading to CF200, irrespective of whether the property is enabled or not. migration: operatorToHelm: enabled: true Note: The operatorToHelm configuration property is not supported in CF200, hence it must be removed. This property was introduced in CF199 to facilitate migration from the previous Operator-based deployments to Helm-based deployments. Update the image tags Update the image tags in your custom-values.yaml file to match those for the new images in your repository. See Planning your container deployment using Helm for more information. Run the upgrade command After making the changes to the custom-values.yaml file, use the following command to upgrade your HCL DX 9.5 deployment to CF197 and later release version: ``` Helm upgrade command helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` In this example: your-namespace is the namespace in which your HCL Digital Experience 9.5 Container Update deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you updated. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Container Update Helm Chart that you extracted in the preparation steps. Running DX Core configuration tasks This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments. Migrate to new DAM DB in Helm-based deployments This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB. Restore Digital Asset Management image to previous version This section shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. Parent topic: Deploying container platforms using Helm","title":"Update deployment to a later version"},{"location":"containerization/helm/helm_update_deployment/#update-deployment-to-a-later-version","text":"This section shows how to update your HCL DX 9.5 Container Update CF197 and later deployment to a newer DX 9.5 Container Update release version. To proceed, administrators should have prepared the container platform cluster, together with the HCL DX 9.5 container deployment custom-values.yaml using the following guidance, Planning your container deployment using Helm , and then install your deployment using the instructions in Install and uninstall commands for HCL DX 9.5 CF196 and later container deployments to Kubernetes and Red Hat OpenShift platforms using Helm . Important: As of HCL DX 9.5 Container Update CF197, you can use this process to update a DX 9.5 deployment from Container Update CF196 on the Google Kubernetes Engine (GKE) platform. Support to update DX 9.5 197 container deployments using Helm to CF198 and later DX 9.5 container versions is provided for Red Hat OpenShift, Amazon EKS, Azure AKS, as well as Google GKE platforms beginning with Container Update CF198. Follow the guidance in this section to update the HCL DX 9.5 container release version CF197 and later deployment, to Kubernetes or Red Hat OpenShift that was installed using Helm. These instructions assume that you have made all configuration changes using the recommended Helm upgrade route described in Updating the DX 9.5 Deployment Configuration . This ensures that your custom-values.yaml file is an updated description of the configuration of your environment. If that is not the case, you must update your custom-values.yaml file first with all configuration changes. Update Ambassador CRDs when updating from CF199 or earlier When updating CF199 or earlier, you need to update the ambassador CRDs to a newer version. As a known pattern, Helm will not update CRDs by design. Important: The CRDs are currently backward compatible. It is still possible to run or perform an install of an older version of DX on that cluster, even when the CRDs are at the latest version. Note: Do not remove/downgrade the CF200 CRDs to an older version while having CF200 or later deployments running on your cluster. Prerequisites: You need to have tar and HCL DX Helm Chart downloaded. Your kubectl configuration must point to the target cluster and the user requires Cluster Admin Access to access it. Note: If you changed the Portal or WAS Administrator user, please ensure you have the correct values for security.core.wasUser/wasPassword and wpsUser/wpsPassword in custom-values.yaml. Note: If you performed a database transfer, please ensure the <database>.DbUser and <database>.DbPassword for all Portal databases reflect the current user and password in /opt/HCL/wp_profile/ConfigEngine/properties/wkplc_dbdomain.properties prior to updating the Portal Core image. Procedure to update CRDs : Locate the hcl-dx-deployment-*.tgz in your downloaded package. Execute the following commands to update the CRDs: ```","title":"Update deployment to a later version"},{"location":"containerization/helm/helm_update_deployment/#extract-crd-directory-from-downloaded-helm-chart","text":"tar vxf hcl-dx-deployment-*.tgz hcl-dx-deployment/crds","title":"Extract CRD directory from downloaded Helm Chart"},{"location":"containerization/helm/helm_update_deployment/#apply-extracted-crds-on-cluster-level","text":"kubectl apply -f hcl-dx-deployment/crds ``` After the action is completed, you can proceed with the DX update procedure. This step is required to be executed only once. Populate your repository with the new images Download the new HCL DX 9.5 container update images you need to upgrade and ensure that they are available in the image repository specified in your custom-values.yaml file. See the Docker image list for the latest HCL DX 9.5 container update images available. Download the Helm charts for the version to be installed Download the Helm charts corresponding to the HCL DX 9.5 container versions you want to install. You must always use the Helm charts that correspond to the container versions you are installing or to which you are upgrading. (Optional) Remove the operatorToHelm property from the existing custom-values.yaml file before upgrading to CF200 and later If you have CF199 deployed, ensure that you remove the operatorToHelm configuration from the custom-values.yaml file before upgrading to CF200, irrespective of whether the property is enabled or not. migration: operatorToHelm: enabled: true Note: The operatorToHelm configuration property is not supported in CF200, hence it must be removed. This property was introduced in CF199 to facilitate migration from the previous Operator-based deployments to Helm-based deployments. Update the image tags Update the image tags in your custom-values.yaml file to match those for the new images in your repository. See Planning your container deployment using Helm for more information. Run the upgrade command After making the changes to the custom-values.yaml file, use the following command to upgrade your HCL DX 9.5 deployment to CF197 and later release version: ```","title":"Apply extracted CRDs on cluster level"},{"location":"containerization/helm/helm_update_deployment/#helm-upgrade-command","text":"helm upgrade -n your-namespace -f path/to/your/custom-values.yaml your-release-name path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz ``` In this example: your-namespace is the namespace in which your HCL Digital Experience 9.5 Container Update deployment is installed and your-release-name is the Helm release name you used when installing. The -f path/to/your/custom-values.yaml parameter must point to the custom-values.yaml you updated. path/to/hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz is the HCL Digital Experience 9.5 Container Update Helm Chart that you extracted in the preparation steps. Running DX Core configuration tasks This topic shows how to run manual Core configuration tasks on your HCL DX 9.5 CF197 and later container deployments. Migrate to new DAM DB in Helm-based deployments This manual migration process to the new DAM DB is mandatory if you have DX CF196 or CF197 deployed using the Helm-based deployment option and are now upgrading to CF200. It is mandatory because you cannot upgrade to a future release, such as CF201, without manually migrating to the new DB. If you already have CF 198 or CF199 installed using the Helm-based deployment option, then you need not manually migrate the DAM DB. Restore Digital Asset Management image to previous version This section shows you how to restore the HCL Digital Experience 9.5 Digital Asset Management image to a previous version. Parent topic: Deploying container platforms using Helm","title":"Helm upgrade command"},{"location":"containerization/helm/hybrid_deployment_helm/","text":"Hybrid Deployment - Helm This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method. Overview The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services using Helm, in a production environment. Prerequisites HCL Digital Experience V9.5 CF198 or a higher release is deployed to supported on-premises platforms in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio is enabled in the Digital Experience 9.5 CF198 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain that uses an SSL connection is established for both the on-premise HCL DX 9.5 CF198 and higher on-premise environments and the target Red Hat Open Shift or Amazon EKS, Azure AKS, or Google GKE platform deployment to contain the cloud native components (HCL DX Experience API, Digital Asset Management and Content Composer). For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF198 or a higher on-premises environment. On DMGR or WAS Admin console, under Security > Global Security > Web & SIP Security > Single Sign-On , the Enabled box is checked and the Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment and the target DX Red Hat Open Shift or Kubernetes platform deployment. Volume Requirement : Requires an AccessMode of ReadWriteMany . Refer to the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information. Step 1: Configure Networking between the on-premises DX 9.5 CF198 or later deployment so that the HCL DX 9.5 Container Update CF198 or later components are accessible externally from the Kubernetes or OpenShift platform cluster. Reference the Configure Networking topic in the Helm Planning sections of the Help Center. Step 2: Set the Core application deployment parameter in your custom-values.yaml file to false . Refer to the Planning your container deployment using Helm Help Center topic for more information. Step 3: Proceed to configuration instructions listed in this Help Center topic: Helm Deployment Configuration . Parent topic: Deploying container platforms using Helm Parent topic: Digital Experience on containerized platforms","title":"Hybrid Deployment - Helm"},{"location":"containerization/helm/hybrid_deployment_helm/#hybrid-deployment-helm","text":"This section describes how to install HCL Digital Experience 9.5 Container Update CF198 and later Portal Server and Web Content Manager services to on-premises platforms, operating with Digital Asset Management, Content Composer, Experience API deployed to cloud-based Kubernetes and OpenShift platforms using the Helm deployment method.","title":"Hybrid Deployment - Helm"},{"location":"containerization/helm/hybrid_deployment_helm/#overview","text":"The HCL Digital Experience 9.5 Hybrid deployment reference architecture and topics describe an approach to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services using Helm, in a production environment.","title":"Overview"},{"location":"containerization/helm/hybrid_deployment_helm/#prerequisites","text":"HCL Digital Experience V9.5 CF198 or a higher release is deployed to supported on-premises platforms in a standalone, cluster, or farm topology. See the Roadmaps to deploy your Digital Experience 9.5 system topic for more information. Practitioner Studio is enabled in the Digital Experience 9.5 CF198 or higher installation. See the How to enable Practitioner Studio topic for instructions. A common domain that uses an SSL connection is established for both the on-premise HCL DX 9.5 CF198 and higher on-premise environments and the target Red Hat Open Shift or Amazon EKS, Azure AKS, or Google GKE platform deployment to contain the cloud native components (HCL DX Experience API, Digital Asset Management and Content Composer). For example, mytargetcloud.dx.com and myonprem.dx.com would have the same domain: dx.com. Single sign-on must be enabled on HCL DX 9.5 CF198 or a higher on-premises environment. On DMGR or WAS Admin console, under Security > Global Security > Web & SIP Security > Single Sign-On , the Enabled box is checked and the Domain name is set to common domain. For example, dx.com. A high-performance network connection is established between the HCL DX 9.5 CF19 and higher on-premises environment and the target DX Red Hat Open Shift or Kubernetes platform deployment. Volume Requirement : Requires an AccessMode of ReadWriteMany . Refer to the Storage Class and Volume topic for more information. Ensure you have obtained a backup of the HCL DX 9.5 on-premises deployment. See the Backup and Restore topic for additional information. Step 1: Configure Networking between the on-premises DX 9.5 CF198 or later deployment so that the HCL DX 9.5 Container Update CF198 or later components are accessible externally from the Kubernetes or OpenShift platform cluster. Reference the Configure Networking topic in the Helm Planning sections of the Help Center. Step 2: Set the Core application deployment parameter in your custom-values.yaml file to false . Refer to the Planning your container deployment using Helm Help Center topic for more information. Step 3: Proceed to configuration instructions listed in this Help Center topic: Helm Deployment Configuration . Parent topic: Deploying container platforms using Helm Parent topic: Digital Experience on containerized platforms","title":"Prerequisites"},{"location":"containerization/helm/limitations_requirements/","text":"Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels. Requirements and Limitations for Helm-based deployments This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0 Requirements and Limitations for Operator-based deployments Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Containerization requirements and limitations"},{"location":"containerization/helm/limitations_requirements/#containerization-requirements-and-limitations","text":"This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels.","title":"Containerization requirements and limitations"},{"location":"containerization/helm/limitations_requirements/#requirements-and-limitations-for-helm-based-deployments","text":"This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0","title":"Requirements and Limitations for Helm-based deployments"},{"location":"containerization/helm/limitations_requirements/#requirements-and-limitations-for-operator-based-deployments","text":"Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Requirements and Limitations for Operator-based deployments"},{"location":"containerization/helm/monitor_helm_deployment_metrics/","text":"Monitor the Digital Experience deployment using metrics This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments. Prometheus metrics and Grafana The Digital Experience 9.5 Helm deployment supports monitoring the deployment activity with advanced metrics and visualization, by exposing standards-based Prometheus -compatible metrics. Prometheus metrics components can scrape the metrics of most of the DX 9.5 container applications. The collected data is queried from Prometheus and are visualized in operations dashboard solutions, such as Grafana . The following information can advise administrators which Digital Experience 9.5 applications can use these tools with some usage examples. Digital Experience 9.5 applications and Prometheus metrics The following Digital Experience 9.5 applications expose metrics that can be tracked with Prometheus metrics. Core Remote Search Content Composer Design Studio Digital Asset Management Image Processor Experience API DAM persistence Ambassador Application Port Route Core 10038 /metrics Remote Search 9060 /metrics Content Composer 3000 /probe/metrics Design Studio 3000 /probe/metrics Digital Asset Management 3000 /probe/metrics Image Processor 3000 /probe/metrics Ring API 3000 /probe/metrics DAM Persistence 9187 /metrics Ambassador 8877 /metrics Important: HCL Digital Experience 9.5 does not include a deployment of Prometheus or Grafana . When metrics are enabled in the DX 9.5 Helm chart , the application exposes Prometheus-compatible metrics. Those metrics can be consumed by any common Prometheus installation. HCL DX 9.5 metrics are compatible with the following deployment and discovery types of Prometheus in Kubernetes environments: Prometheus - Discovers metrics by evaluating the annotation of the services Prometheus Operator - Discovers metrics using the ServiceMonitor custom resources Administrators can configure the HCL DX 9.5 metrics depending on their specific Prometheus deployment, as outlined in the following sections. Configure Prometheus metrics To configure the metrics for the Digital Experience 9.5 applications in the DX 9.5 Helm chart, enable scraping in the custom-values.yaml used for the DX 9.5 deployment. The metrics are configured independently for each DX 9.5 application. Parameter Description Default value metrics.<application>.scrape Determines if the metrics of this application are scraped by Prometheus. false metrics.<application>.prometheusDiscoveryType Determines how Prometheus discovers the metrics of a service. Accepts \"annotation\" and \"serviceMonitor\" . The \"serviceMonitor\" setting requires that the ServiceMonitor CRD (which comes with the Prometheus Operator), is installed in the cluster. \"annotation\" Example configurations : Enable the metrics for DX 9.5 core and add the appropriate annotation for Prometheus: ``` metrics: core: scrape: true # prometheusDiscoveryType is optional here as \"annotation\" is the default prometheusDiscoveryType: \"annotation\" ``` Enable the metrics for DX 9.5 Core and create a ServiceMonitor for Prometheus Operator: ``` metrics: core: scrape: true prometheusDiscoveryType: \"serviceMonitor\" ``` Grafana dashboards The exposed DX 9.5 Applications metrics are compatible with a set of existing Grafana operations dashboards that are available from the Grafana dashboard page, as well as a set of Granada-supported custom dashboards provided in JSON format. See following examples, which can be imported directly into Grafana. Publicly available operations dashboards You can directly download or import the following dashboards from the Grafana community page using the IDs or links. ID Dashboard Applications 14151 WebSphere Application Server PMI metrics dashboard Core, Remote Search 11159 NodeJS application dashboard Content Composer, Design Studio, Digital Asset Management, Image Processor, Experience API 9628 PostgreSQL database DAM persistence 10850 Ambassador dashboard Ambassador HCL Digital Experience custom dashboards The following dashboards are provided by HCL Software for use with HCL Digital Experience 9.5 deployments. These examples expose custom metrics for DX applications or provide enhanced features for existing dashboards. They are available in the public HCL Software Github repository . Dashboard Application(s) dam_dashboard.json Digital Asset Management References to Prometheus and Grafana installations Important: The resources outline here are optional deployment examples. HCL Software does not provide direct support for any issues related to the Prometheus metrics or the Grafana visualization tools. To leverage the full potential of the Digital Experience 9.5 applications metrics, an existing Prometheus and Grafana deployment can be used. Following is a list of additional metrics tracking and visualization services (non-exhaustive) that you can consider when developing solutions according to your deployment needs: kube-prometheus-stack Helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus adapter for Kubernetes metrics APIs kube-state-metrics Grafana Note: The kube-prometheus-stack Helm chart is based on the kube-prometheus repository, and comes with a set of tools to monitor the Kubernetes cluster, as well as pre-installed Grafana dashboards for visualization. prometheus and grafana are provided as independent Helm charts. Parent topic: Troubleshooting your Helm deployment","title":"Monitor the Digital Experience deployment using metrics"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#monitor-the-digital-experience-deployment-using-metrics","text":"This topic outlines the use of standards-based metrics to monitor activity and performance of DX container deployments.","title":"Monitor the Digital Experience deployment using metrics"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#prometheus-metrics-and-grafana","text":"The Digital Experience 9.5 Helm deployment supports monitoring the deployment activity with advanced metrics and visualization, by exposing standards-based Prometheus -compatible metrics. Prometheus metrics components can scrape the metrics of most of the DX 9.5 container applications. The collected data is queried from Prometheus and are visualized in operations dashboard solutions, such as Grafana . The following information can advise administrators which Digital Experience 9.5 applications can use these tools with some usage examples.","title":"Prometheus metrics and Grafana"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#digital-experience-95-applications-and-prometheus-metrics","text":"The following Digital Experience 9.5 applications expose metrics that can be tracked with Prometheus metrics. Core Remote Search Content Composer Design Studio Digital Asset Management Image Processor Experience API DAM persistence Ambassador Application Port Route Core 10038 /metrics Remote Search 9060 /metrics Content Composer 3000 /probe/metrics Design Studio 3000 /probe/metrics Digital Asset Management 3000 /probe/metrics Image Processor 3000 /probe/metrics Ring API 3000 /probe/metrics DAM Persistence 9187 /metrics Ambassador 8877 /metrics Important: HCL Digital Experience 9.5 does not include a deployment of Prometheus or Grafana . When metrics are enabled in the DX 9.5 Helm chart , the application exposes Prometheus-compatible metrics. Those metrics can be consumed by any common Prometheus installation. HCL DX 9.5 metrics are compatible with the following deployment and discovery types of Prometheus in Kubernetes environments: Prometheus - Discovers metrics by evaluating the annotation of the services Prometheus Operator - Discovers metrics using the ServiceMonitor custom resources Administrators can configure the HCL DX 9.5 metrics depending on their specific Prometheus deployment, as outlined in the following sections.","title":"Digital Experience 9.5 applications and Prometheus metrics"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#configure-prometheus-metrics","text":"To configure the metrics for the Digital Experience 9.5 applications in the DX 9.5 Helm chart, enable scraping in the custom-values.yaml used for the DX 9.5 deployment. The metrics are configured independently for each DX 9.5 application. Parameter Description Default value metrics.<application>.scrape Determines if the metrics of this application are scraped by Prometheus. false metrics.<application>.prometheusDiscoveryType Determines how Prometheus discovers the metrics of a service. Accepts \"annotation\" and \"serviceMonitor\" . The \"serviceMonitor\" setting requires that the ServiceMonitor CRD (which comes with the Prometheus Operator), is installed in the cluster. \"annotation\" Example configurations : Enable the metrics for DX 9.5 core and add the appropriate annotation for Prometheus: ``` metrics: core: scrape: true # prometheusDiscoveryType is optional here as \"annotation\" is the default prometheusDiscoveryType: \"annotation\" ``` Enable the metrics for DX 9.5 Core and create a ServiceMonitor for Prometheus Operator: ``` metrics: core: scrape: true prometheusDiscoveryType: \"serviceMonitor\" ```","title":"Configure Prometheus metrics"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#grafana-dashboards","text":"The exposed DX 9.5 Applications metrics are compatible with a set of existing Grafana operations dashboards that are available from the Grafana dashboard page, as well as a set of Granada-supported custom dashboards provided in JSON format. See following examples, which can be imported directly into Grafana. Publicly available operations dashboards You can directly download or import the following dashboards from the Grafana community page using the IDs or links. ID Dashboard Applications 14151 WebSphere Application Server PMI metrics dashboard Core, Remote Search 11159 NodeJS application dashboard Content Composer, Design Studio, Digital Asset Management, Image Processor, Experience API 9628 PostgreSQL database DAM persistence 10850 Ambassador dashboard Ambassador","title":"Grafana dashboards"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#hcl-digital-experience-custom-dashboards","text":"The following dashboards are provided by HCL Software for use with HCL Digital Experience 9.5 deployments. These examples expose custom metrics for DX applications or provide enhanced features for existing dashboards. They are available in the public HCL Software Github repository . Dashboard Application(s) dam_dashboard.json Digital Asset Management","title":"HCL Digital Experience custom dashboards"},{"location":"containerization/helm/monitor_helm_deployment_metrics/#references-to-prometheus-and-grafana-installations","text":"Important: The resources outline here are optional deployment examples. HCL Software does not provide direct support for any issues related to the Prometheus metrics or the Grafana visualization tools. To leverage the full potential of the Digital Experience 9.5 applications metrics, an existing Prometheus and Grafana deployment can be used. Following is a list of additional metrics tracking and visualization services (non-exhaustive) that you can consider when developing solutions according to your deployment needs: kube-prometheus-stack Helm chart that includes: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus adapter for Kubernetes metrics APIs kube-state-metrics Grafana Note: The kube-prometheus-stack Helm chart is based on the kube-prometheus repository, and comes with a set of tools to monitor the Kubernetes cluster, as well as pre-installed Grafana dashboards for visualization. prometheus and grafana are provided as independent Helm charts. Parent topic: Troubleshooting your Helm deployment","title":"References to Prometheus and Grafana installations"},{"location":"containerization/helm/persistent_volumes_helm/","text":"PersistentVolumes and related operations considerations This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Digital Experience 9.5 container-based stateful applications (DX 9.5 Core, Digital Asset Management, and Persistence) require PersistentVolumes (PVs) to store their data. Refer to the Deploy DX 9.5 applications to container platforms using Helm topic for a description of the DX 9.5 Applications details. As Kubernetes and OpenShift Pods do not have their own persistent file storage, the use of PVs is a must. For more information on PVs, consult the Kubernetes documentation . Note: You need to consider the type of PVs before you perform an installation. The type of volumes used depends on the type of deployment you are performing. If you are using only ReadWriteOnce (RWO) persistent volumes for all applications, you will not be able to scale them up to multiple Pods. Core The DX 9.5 Core application requires multiple PersistentVolumes (PVs) of different types and sizes for its operation. From an application perspective, there are three different things that need to be persisted: Profiles (called wp_profile and cw_profile ) Logs Transaction logs While the profile needs to be shared between all DX 9.5 Core Pods, the logs and transaction logs are per Pod and not shared. This means that the persistent volume used for the profile must be ReadWriteMany (RWX). The persistent volumes used for logs and transaction logs are established as ReadWriteOnce (RWO) for proper operations performance. For example, in a DX 9.5 Core deployment: To deploy one Core Pod, 3 PVs will be needed. To deploy two Core Pods, the number of required PVs increases by two, resulting in 5 PVs needed, since the second Pod shares the existing profile PV with the first Pod, but requires its own log and transaction log PVs. The following formula example can be used to calculate the required PV count per Core Pods to be deployed: # Formula to calculate PV count n(PV) = 1 + m(Core Pods) * 2 # E.g. for 3 Pods: 1 + 3 * 2 = 7 PVs In typical operations, the persistent volumes for logs and transaction logs are relatively small. Digital Asset Management The Digital Asset Management (DAM) application requires one (1) PV for storing binary asset data. This persistent volume is shared between all Digital Asset Management Pods. The PV used must be ReadWriteMany (RWX) . Persistence Persistence consists of at least two Pods. One which acts as a read/write primary node, and at least one that acts as a read-only fallback. All Persistence Pods work with ReadWriteOnce (RWO) persistent volumes, since there is no sharing of storage between the Pods. Therefore, the minimum required amount of PVs for Persistence is 2. Remote Search Remote Search requires 1 persistent volume for storing the profile (called prs_profile ) with the type ReadWriteOnce (RWO). Remote Search is limited to only one Pod, therefore, requires one PV for that Pod. Refer to Networking configuration for next steps. Parent topic: Overview of the Helm architecture","title":"PersistentVolumes and related operations considerations"},{"location":"containerization/helm/persistent_volumes_helm/#persistentvolumes-and-related-operations-considerations","text":"This topic provides details covering the PersistentVolumes (PVs) and related operations considerations in storing data for DX 9.5 stateful applications. Digital Experience 9.5 container-based stateful applications (DX 9.5 Core, Digital Asset Management, and Persistence) require PersistentVolumes (PVs) to store their data. Refer to the Deploy DX 9.5 applications to container platforms using Helm topic for a description of the DX 9.5 Applications details. As Kubernetes and OpenShift Pods do not have their own persistent file storage, the use of PVs is a must. For more information on PVs, consult the Kubernetes documentation . Note: You need to consider the type of PVs before you perform an installation. The type of volumes used depends on the type of deployment you are performing. If you are using only ReadWriteOnce (RWO) persistent volumes for all applications, you will not be able to scale them up to multiple Pods.","title":"PersistentVolumes and related operations considerations"},{"location":"containerization/helm/persistent_volumes_helm/#core","text":"The DX 9.5 Core application requires multiple PersistentVolumes (PVs) of different types and sizes for its operation. From an application perspective, there are three different things that need to be persisted: Profiles (called wp_profile and cw_profile ) Logs Transaction logs While the profile needs to be shared between all DX 9.5 Core Pods, the logs and transaction logs are per Pod and not shared. This means that the persistent volume used for the profile must be ReadWriteMany (RWX). The persistent volumes used for logs and transaction logs are established as ReadWriteOnce (RWO) for proper operations performance. For example, in a DX 9.5 Core deployment: To deploy one Core Pod, 3 PVs will be needed. To deploy two Core Pods, the number of required PVs increases by two, resulting in 5 PVs needed, since the second Pod shares the existing profile PV with the first Pod, but requires its own log and transaction log PVs. The following formula example can be used to calculate the required PV count per Core Pods to be deployed: # Formula to calculate PV count n(PV) = 1 + m(Core Pods) * 2 # E.g. for 3 Pods: 1 + 3 * 2 = 7 PVs In typical operations, the persistent volumes for logs and transaction logs are relatively small.","title":"Core"},{"location":"containerization/helm/persistent_volumes_helm/#digital-asset-management","text":"The Digital Asset Management (DAM) application requires one (1) PV for storing binary asset data. This persistent volume is shared between all Digital Asset Management Pods. The PV used must be ReadWriteMany (RWX) .","title":"Digital Asset Management"},{"location":"containerization/helm/persistent_volumes_helm/#persistence","text":"Persistence consists of at least two Pods. One which acts as a read/write primary node, and at least one that acts as a read-only fallback. All Persistence Pods work with ReadWriteOnce (RWO) persistent volumes, since there is no sharing of storage between the Pods. Therefore, the minimum required amount of PVs for Persistence is 2.","title":"Persistence"},{"location":"containerization/helm/persistent_volumes_helm/#remote-search","text":"Remote Search requires 1 persistent volume for storing the profile (called prs_profile ) with the type ReadWriteOnce (RWO). Remote Search is limited to only one Pod, therefore, requires one PV for that Pod. Refer to Networking configuration for next steps. Parent topic: Overview of the Helm architecture","title":"Remote Search"},{"location":"containerization/helm/planning/docker/","text":"Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. Docker container update file list The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF201 Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table. HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"Docker image list"},{"location":"containerization/helm/planning/docker/#docker-image-list","text":"This section presents the latest HCL DX 9.5 Docker container update images available.","title":"Docker image list"},{"location":"containerization/helm/planning/docker/#docker-container-update-file-list","text":"The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Docker container update file list"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf201","text":"Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF201"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table.","title":"HCL DX 9.5 CF199"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"containerization/helm/planning/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"HCL DX 9.5 CF181"},{"location":"containerization/helm/planning/helm_additional_tasks/","text":"Additional Helm tasks This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node. Prepare cluster nodes You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: ``` Edit Node kubectl edit node k8s-node-4 ``` OpenShift Client: ``` Edit Node kubectl edit node k8s-node-4 ``` The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: ``` Execute lookup via kubectl kubectl get node k8s-node-4 --show-labels Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress Execute lookup via kubectl oc get node k8s-node-4 --show-labels Command output NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress ``` Configure nodes You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> Select DX applications to deploy HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed. Refer to the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic for related information. Disabling or enabling specific applications You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one. Supported LDAP configuration You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Parameter Values Description type - none - other - dx |- Determines which type of LDAP to use. - Accepts none , dx or other - none : No LDAP configuration - dx : use and configure DX Open LDAP - other : use other LDAP | |bindUser| |- User used to connect to LDAP - Only used if ldap type is other | |bindPassword| |- Password used to connect to LDAP - Only used if ldap type is other | |suffix| |- Suffix in LDAP - Only used if ldap type is other | |host| |- Host of LDAP - Only used if ldap type is other | |port| |- Port of LDAP - Only used if ldap type is other | |attributeMappingLdap|- mail - title - userPassword |- Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) | |attributeMappingPortal|- ibm-primaryEmail - ibm-jobTitle - password |- Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) | |attributeNonSupported|- certificate - members |- Non-supported LDAP attributes (comma-separated list) | |serverType|- CUSTOM |- Supported LDAP Server types | |id|- dx_ldap |- LDAP configuration id | Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option Authoring/Rendering configuration You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true Configuration Wizard configuration You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true OpenLDAP configuration If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\" Remote Search configuration You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true Configure scaling The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note: You are not able to use more than one (1) Core Pod until you have performed a database transfer. Configuring pod count Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3 Configuring HorizontalPodAutoscalers The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications. Configure credentials HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment. Adjusting default credentials You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\" Configure Core sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" Configure Remote Search sidecar logging Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\" Configure Labels and Annotations This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: ``` release ``` ``` helm.sh/chart ``` ``` app.kubernetes.io/version ``` ``` app.kubernetes.io/managed-by ``` ``` app.kubernetes.io/name ``` app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Configure environment variables for DX resources This section explains the configuration of environment variables for different DX resources. Environment variables To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note: Additional environment values are not mandatory for a deployment. Sample environment variables for core pods To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2 Incubator section in the values.yaml file The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Note: All features within the incubator section are not recommended to be used in the production environments. Refer to Install and uninstall commands for the next steps. Use ImagePullSecrets To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note: Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to. Configure deployment to use ImagePullSecrets In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images. Parent topic: Deploying container platforms using Helm","title":"Additional Helm tasks"},{"location":"containerization/helm/planning/helm_additional_tasks/#additional-helm-tasks","text":"This topic shows you how to leverage NodeSelectors to allow deploying specific DX 9.5 application Pods only on a specific node.","title":"Additional Helm tasks"},{"location":"containerization/helm/planning/helm_additional_tasks/#prepare-cluster-nodes","text":"You must label your Kubernetes or OpenShift cluster nodes to use NodeSelectors . You can do this by editing the node in Kubernetes or OpenShift. The following steps shows how to modify cluster nodes. As the examples here may differ from those given by your cloud provider, you are encouraged to review the documentation reference accompanying your cloud subscription. For this example, the following setup is assumed: The target cluster has multiple nodes. A label purpose is added to a node called k8s-node-4 and assigned the value ingress This can be done using the following commands: Kubectl: ```","title":"Prepare cluster nodes"},{"location":"containerization/helm/planning/helm_additional_tasks/#edit-node","text":"kubectl edit node k8s-node-4 ``` OpenShift Client: ```","title":"Edit Node"},{"location":"containerization/helm/planning/helm_additional_tasks/#edit-node_1","text":"kubectl edit node k8s-node-4 ``` The following label is added using the Kubernetes syntax (and other configurations are changed): metadata: labels: purpose: ingress The node is now labeled with the desired target label: Kubectl: ```","title":"Edit Node"},{"location":"containerization/helm/planning/helm_additional_tasks/#execute-lookup-via-kubectl","text":"kubectl get node k8s-node-4 --show-labels","title":"Execute lookup via kubectl"},{"location":"containerization/helm/planning/helm_additional_tasks/#command-output","text":"NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress","title":"Command output"},{"location":"containerization/helm/planning/helm_additional_tasks/#execute-lookup-via-kubectl_1","text":"oc get node k8s-node-4 --show-labels","title":"Execute lookup via kubectl"},{"location":"containerization/helm/planning/helm_additional_tasks/#command-output_1","text":"NAME STATUS ROLES AGE VERSION LABELS k8s-node-4 Ready 123d v1.20.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-4,kubernetes.io/os=linux,purpose=ingress ```","title":"Command output"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-nodes","text":"You can assign all pods (deployed by the Helm Chart of HCL Digital Experience 9.5) to specific nodes by using NodeSelectors . Modify your custom-values.yaml file to include the NodeSelector configuration. Make sure to use the proper indentation as YAML is indent-sensitive. Example for Ambassador: nodeSelector: ambassadorIngress: purpose: ingress ambassadorRedis: purpose: ingress This configuration directs the Ambassador Ingress and Ambassador Redis to run nodes with the label purpose: ingress . Once install is completed, the pods are running on your desired node. For example k8s-node-4 . Kubectl: # Use this command to see running Pods incl. Nodes kubectl get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none> <none> OpenShift Client: # Use this command to see running Pods incl. Nodes oc get pods -o wide -n my-deployment # Command output NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE dx-ambassador-769b86f6ff-knhgt 1/1 Running 0 2m12s 10.244.4.111 k8s-node-4 <none> dx-ambassador-769b86f6ff-qtqmv 1/1 Running 0 2m12s 10.244.4.110 k8s-node-4 <none> dx-ambassador-769b86f6ff-whmw6 1/1 Running 0 2m12s 10.244.4.112 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-gtqwv 1/1 Running 0 2m12s 10.244.4.106 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-j8v4d 1/1 Running 0 2m12s 10.244.4.107 k8s-node-4 <none> dx-ambassador-redis-6cbbf58649-qtgqp 1/1 Running 0 2m12s 10.244.4.109 k8s-node-4 <none>","title":"Configure nodes"},{"location":"containerization/helm/planning/helm_additional_tasks/#select-dx-applications-to-deploy","text":"HCL Digital Experience 9.5 consists of multiple applications and services that can be deployed. Depending on your needs, it might not be necessary to have all applications deployed. Refer to the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic for related information. Disabling or enabling specific applications You can easily enable or disable specific applications by adding the following parts to your custom-values.yaml file: # Controls which application is deployed and configured applications: # Deploys Content Composer contentComposer: true # Deploys Core core: true # Deploys Design Studio designStudio: false # Deploys Digital Asset Management digitalAssetManagement: true # Deploys the Image Processor # Enabling digitalAssetManagement will override this setting with: true imageProcessor: true # Deploy Open LDAP # Setting the ldap type in the core application configuration to dx will override this setting with: true openLdap: true # Deploys the Persistence Layer # Enabling digitalAssetManagement will override this setting with: true persistence: true # Deploys the Remote Search remoteSearch: true # Deploys the Ring API # Enabling either digitalAssetManagement or contentComposer will override this setting with: true ringApi: true # Deploys the Ambassador Ingress and Redis ambassador: true # Deploys the Runtime Controller runtimeController: true You can set applications that you do not want to be deployed to false . As noted in the Applications overview - Deploy DX 9.5 to container platforms using Helm Help Center topic, some DX applications are pre-requisites for others to be deployed. It can appear that you have disabled an application, but it still gets deployed. This is due to other applications requiring that one.","title":"Select DX applications to deploy"},{"location":"containerization/helm/planning/helm_additional_tasks/#supported-ldap-configuration","text":"You can specify a LDAP configuration that can be used by HCL Digital Experience 9.5. The Helm chart provides a ldap section under the configuration and core section. This section can be used to configure a none , dx or other LDAP. This defaults to none, so there is no LDAP configured. If you adjust this to other , you can configure an external LDAP that you want to connect to. Core is then configured to use this LDAP. Currently, the configuration capability is quite limited. For more complex configurations, use the ConfigWizard instead. Parameter Values Description type - none - other - dx |- Determines which type of LDAP to use. - Accepts none , dx or other - none : No LDAP configuration - dx : use and configure DX Open LDAP - other : use other LDAP | |bindUser| |- User used to connect to LDAP - Only used if ldap type is other | |bindPassword| |- Password used to connect to LDAP - Only used if ldap type is other | |suffix| |- Suffix in LDAP - Only used if ldap type is other | |host| |- Host of LDAP - Only used if ldap type is other | |port| |- Port of LDAP - Only used if ldap type is other | |attributeMappingLdap|- mail - title - userPassword |- Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) | |attributeMappingPortal|- ibm-primaryEmail - ibm-jobTitle - password |- Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) | |attributeNonSupported|- certificate - members |- Non-supported LDAP attributes (comma-separated list) | |serverType|- CUSTOM |- Supported LDAP Server types | |id|- dx_ldap |- LDAP configuration id | Example Configuration You can use the following syntax in your custom-values.yaml file to adjust LDAP settings: # Application configuration configuration: # Application specific configuration for Core core: # Settings for LDAP configuration ldap: # Determines which type of LDAP to use # Accepts: \"none\", \"dx\" or \"other\" # \"none\" - no LDAP configuration # \"dx\" - use DX openLDAP and configure it # \"other\" - use provided configuration for other LDAP type: \"none\" # User used to connect to LDAP, only used if ldap type is \"other\" bindUser: \"\" # Password used to connect to LDAP, only used if ldap type is \"other\" bindPassword: \"\" # Suffix in LDAP, only used if ldap type is \"other\" suffix: \"\" # Host of LDAP, only used if ldap type is \"other\" host: \"\" # Port of LDAP, only used if ldap type is \"other\" port: \"\" # Supported LDAP Server types - CUSTOM serverType: \"CUSTOM\" # LDAP configuration id id: \"dx_ldap\" # Mapping attributes between LDAP and DX, LDAP attribute names (comma-separated list) attributeMappingLdap: \"mail,title,userPassword\" # Mapping attributes between LDAP and DX, DX attribute names (comma-separated list) attributeMappingPortal: \"ibm-primaryEmail,ibm-jobTitle,password\" # Non-supported LDAP attributes (comma-separated list) attributeNonSupported: \"certificate,members\" Refer to the following Help Center documentation for more information about LDAP and Configuration Wizard configuration: Configuration Wizard Enable federated security Troubleshooting: Enable federated security option","title":"Supported LDAP configuration"},{"location":"containerization/helm/planning/helm_additional_tasks/#authoringrendering-configuration","text":"You can choose if the environment you deploy is configured as a WCM authoring or rendering type. This has implications on things like caching of Core. As default, this defaults to true. The deployment is configured as an authoring environment. If you want to adjust this to deploy a rendering environment, you can use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the environment should be configured for authoring or not authoring: true","title":"Authoring/Rendering configuration"},{"location":"containerization/helm/planning/helm_additional_tasks/#configuration-wizard-configuration","text":"You can select whether the Config Wizard is started together with the Core application. This defaults to true. If you want to adjust this setting, you can use the following syntax in your file: # Application configuration configuration: # Application specific configuration for Core core: # Settings for tuning tuning: # Configures if the server for configWizard and dxconnect is started configWizard: true","title":"Configuration Wizard configuration"},{"location":"containerization/helm/planning/helm_additional_tasks/#openldap-configuration","text":"If you choose to deploy the OpenLDAP container in your deployment, you can change country, organization and suffix, that may be configured in OpenLDAP for use. Use the following syntax in your custom-values.yaml file to adjust the configuration: # Application configuration configuration: # Application specific configuration for Open LDAP openLdap: # Country configuration for Open LDAP country: \"US\" # Org configuration for Open LDAP org: \"DX\" # Suffix configuration for Open LDAP suffix: \"dc=dx,dc=com\"","title":"OpenLDAP configuration"},{"location":"containerization/helm/planning/helm_additional_tasks/#remote-search-configuration","text":"You can configure whether the Remote Search configuration through the IBM WebSphere Application Server Solution Console is exposed as an additional port on the Ambassador Ingress or not. This defaults to true. If set to true, you can access the Solution Console using: https://yourhost:9043/ibm/console Use the following syntax in your custom-values.yaml file: # Application configuration configuration: # Application specific configuration for Remote Search remoteSearch: # Should the configuration interface be exposed exposeConfigurationConsole: true","title":"Remote Search configuration"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-scaling","text":"The HCL Digital Experience 9.5 Kubernetes deployment using Helm allows you to configure the pod count of each individual application. In addition, it is possible to configure the use of HorizontalPodAutoscalers that scales up and down the applications by adding or removing Pods based on the pod metrics. Refer to the Scaling DX 9.5 container deployments using Helm Help Center topic for detailed overview information. Note: You are not able to use more than one (1) Core Pod until you have performed a database transfer. Configuring pod count Even if you don't want to automatically scale your DX 9.5 deployment based on CPU and memory utilization, you still can control the amount of pods per application. You can use the following syntax to reconfigure the pod count per application in your custom-values.yaml file: # Scaling settings for deployed applications scaling: # The default amount of replicas per application replicas: contentComposer: 1 core: 1 designStudio: 1 digitalAssetManagement: 3 imageProcessor: 5 ringApi: 3 ambassadorIngress: 3 ambassadorRedis: 3 Configuring HorizontalPodAutoscalers The use of HorizontalPodAutoscalers requires your cluster to have the Kubernetes Metrics running. Ensure that this is the case, and reference your cloud provider documentation for further information. You can set up the use of HorizontalPodAutoscalers on a per application basis using the following syntax in your custom-values.yaml file, showing Content Composer, as an example: # Scaling settings for deployed applications scaling: # Automated scaling using HorizontalPodAutoscaler horizontalPodAutoScaler: # Autoscaling settings for Content Composer contentComposer: # Enable or disable autoscaling enabled: true # Minimum and maximum Pod count minReplicas: 1 maxReplicas: 3 # Target CPU utilization scaling threshold targetCPUUtilizationPercentage: 75 # Target Memory utilization scaling threshold targetMemoryUtilizationPercentage: 80 The example configures a HorizontalPodAutoscaler for Content Composer, that scales up to 3 pods maximum. It considers scaling when a CPU utilization of 75% or Memory utilization of 80% per pod is reached. Refer to the default values.yaml file for all configurable applications.","title":"Configure scaling"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-credentials","text":"HCL Digital Experience 9.5 uses several credentials in its deployment to manage access between applications and from outside the container deployment. Adjusting default credentials You can adjust the default credentials that HCL Digital Experience 9.5 is using by adding the following syntax to your custom-values.yaml file and changing the values you need: # Security related configuration, e.g. default credentials security: # Security configuration for Core core: # Credentials used for IBM WebSphere Application Server administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials. wasUser: \"REDACTED\" wasPassword: \"REDACTED\" # Credentials used for HCL Digital Experience Core administrative access, needs to be adjusted if different credentials are already in place # This setting does currently NOT adjust the existing user credentials wpsUser: \"REDACTED\" wpsPassword: \"REDACTED\" # Security configuration for Digital Asset Management digitalAssetManagement: # Credentials used by the Digital Asset Management to access the persistence database. dbUser: \"REDACTED\" dbPassword: \"REDACTED\" # Credentials used by the persistence database to perform replication between database nodes. replicationUser: \"REDACTED\" replicationPassword: \"REDACTED\" # Security configuration for Open LDAP openLdap: # Admin user for Open LDAP, can not be adjusted currently. ldapUser: \"REDACTED\" # Admin password for Open LDAP ldapPassword: \"REDACTED\"","title":"Configure credentials"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-core-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files by the DX Core application. The deployment uses sidecar containers, which access the same logs volume as the Core, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-core-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Core container and sidecar containers share the same volume. This allows DX Core to write its logs, and have the sidecar containers read those logs. The logs are mounted at /opt/HCL/logs (and symbolically linked from /opt/HCL/wp_profile/logs) in the DX Core container, and at /var/logs/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Core under its logs directory. Files in other directories (such as the profile) are not available to the sidecars. Default configuration Two sidecar containers are launched with Core: system-out-log - exposes the log file at /var/logs/WebSphere_Portal/SystemOut.log. system-err-log - exposes the log file at /var/logs/WebSphere_Portal/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/logs/ directory. logging: # Core specific logging configuration core: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath` # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/logs/WebSphere_Portal/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/logs/WebSphere_Portal/trace.log. logging: core: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/logs/WebSphere_Portal/trace.log\"","title":"Configure Core sidecar logging"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-remote-search-sidecar-logging","text":"Beginning with HCL Digital Experience 9.5 CF199, Kubernetes deployment using Helm allows you to expose logs that are written to files on its PersistentVolumes (PVs) by the DX Remote Search application. The deployment uses sidecar containers, which access the PersistentVolume as the Remote Search container, read the log files, and expose them as their standard output. You can access logs with commands like kubectl logs -n <namespace> <pod-name> <sidecar-container-name> , for example, kubectl logs -n dxns dx-deployment-remote-search-0 system-err-log . See the official Kubernetes documentation for more information. Shared volume The DX Remote Search container and sidecar containers in the same pod share the same volume. This allows DX Remote Search to write its logs, and have the sidecar containers read those logs. The profile volume containing the logs is mounted at /opt/HCL/AppServer/profiles/prs_profile/ in the DX Remote Search container, and at /var/profile/ in the sidecar containers. The different directory paths emphasize that sidecar containers can only read files written by Remote Search under its profile directory. Files in other directories are not available to the sidecars. Default configuration Two sidecar containers are launched with Remote Search: system-out-log - exposes the log file at /var/profile/logs/server1/SystemOut.log. system-err-log - exposes the log file at /var/profile/logs/server1/SystemErr.log. Configure custom sidecar containers Use the following syntax to configure more sidecar containers for additional log files in the custom-values.yaml file. Important: You can only expose log files inside of the /var/profile/ directory. logging: remoteSearch: # List of sidecar containers mapping a container name to a file path for a log file to be exposed # Each element must consist of a `containerName` and a `logFilePath`, the latter must be located in /var/profile # Example: # customLogSidecarContainers: # - containerName: \"trace\" # logFilePath: \"/var/profile/logs/server1/trace.log\" customLogSidecarContainers: [] Example: The following example starts a new sidecar container, and exposes the logs in /var/profile/logs/server1/trace.log. logging: remoteSearch: customLogSidecarContainers: - containerName: \"trace\" logFilePath: \"/var/profile/logs/server1/trace.log\"","title":"Configure Remote Search sidecar logging"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-labels-and-annotations","text":"This section documents the configuration of labels and annotations for different DX resources. Annotations Services and Pods To configure annotations for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional annotations are not mandatory for a deployment. Ensure you do not overwrite existing DX annotations such as the following: meta.helm.sh/release-name meta.helm.sh/release-namespace Sample annotations for core service To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on the core service, add the following to your custom-values.yaml file: annotations: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample annotations for core pods To set annotation KEY1 with value VALUE1 and annotation KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: annotations: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Labels Services and Pods To configure labels for kubernetes services and pods, update your custom-values.yaml file as follows: Notes: Additional labels are not mandatory for a deployment. Ensure that you do not overwrite existing DX Labels such as the following: ``` release ``` ``` helm.sh/chart ``` ``` app.kubernetes.io/version ``` ``` app.kubernetes.io/managed-by ``` ``` app.kubernetes.io/name ``` app.kubernetes.io/instance Sample labels for core services To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on the core services, add the following to your custom-values.yaml file: label: service: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2 Sample labels for core pods To set label KEY1 with value VALUE1 and label KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: label: pod: core: - key: KEY1 value: VALUE1 - key: KEY2 value: VALUE2","title":"Configure Labels and Annotations"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-environment-variables-for-dx-resources","text":"This section explains the configuration of environment variables for different DX resources. Environment variables To configure environment variables for kubernetes pods, update your custom-values.yaml file as below. Note: Additional environment values are not mandatory for a deployment. Sample environment variables for core pods To set environment variable KEY1 with value VALUE1 and environment variable KEY2 with value VALUE2 on core pods, add the following to your custom-values.yaml file: environment: pod: core: - name: KEY1 value: VALUE1 - name: KEY2 value: VALUE2","title":"Configure environment variables for DX resources"},{"location":"containerization/helm/planning/helm_additional_tasks/#incubator-section-in-the-valuesyaml-file","text":"The Incubator section is in the root level of the values.yaml file in the Helm charts. This section contains the capabilities that are planned to be made available for production environments in the future releases. The configurations within the incubator section are subject to change. The default values within this section are defined in such a way that they do not interfere with the existing deployments. The features and functions within the incubator section are considered experimental and might not be fully documented yet. Note: All features within the incubator section are not recommended to be used in the production environments. Refer to Install and uninstall commands for the next steps.","title":"Incubator section in the values.yaml file"},{"location":"containerization/helm/planning/helm_additional_tasks/#use-imagepullsecrets","text":"To use a container image registry that has access restrictions and requires credentials, you need to leverage ImagePullSecrets in your deployment. Refer to the Kubernetes Documentation for additional information on this topic. In addition, reference your Cloud Provider documentation on how to create ImagePullSecrets . Note: Ensure that the ImagePullSecret has been created in the same namespace that your DX deployment is installed to.","title":"Use ImagePullSecrets"},{"location":"containerization/helm/planning/helm_additional_tasks/#configure-deployment-to-use-imagepullsecrets","text":"In order for the HCL Digital Experience 9.5 deployment to leverage ImagePullSecrets you need to adjust your custom-values.yaml file to include the following syntax: images: imagePullSecrets: - name: regcred The name regcred can be different, depending on how you have created your ImagePullSecret and how it is named. Ensure that you reference the correct name in the configuration. It is assume that you have moved the HCL Digital Experience 9.5 images to your registry; make sure it is also configured properly in your custom-values.yaml : images: repository: \"your-repo:port\" All pods created now have that secret configured for pulling DX container images. Parent topic: Deploying container platforms using Helm","title":"Configure deployment to use ImagePullSecrets"},{"location":"containerization/helm/planning/helm_configure_networking/","text":"Configure networking This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster. Full Kubernetes or OpenShift deployment If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured. Core host In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly. Configure Cross Origin Resource Sharing (CORS) The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured. Hybrid host Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details. Configure Ingress certificate To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading. Generate self-signed certificate It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment. Use certificate Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note: The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience Configure secret in deployment You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note: Verify you have entered the correct name. Configure minimum TLS version for Ingress From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure networking"},{"location":"containerization/helm/planning/helm_configure_networking/#configure-networking","text":"This section explains what must be configured from a networking perspective to get HCL Digital Experience 9.5 running in your Kubernetes or OpenShift cluster, and to provide accessibility to your deployment from outside the Cluster.","title":"Configure networking"},{"location":"containerization/helm/planning/helm_configure_networking/#full-kubernetes-or-openshift-deployment","text":"If you deploy both Core and all other applications inside OpenShift or Kubernetes, this section shows you what needs to be configured.","title":"Full Kubernetes or OpenShift deployment"},{"location":"containerization/helm/planning/helm_configure_networking/#core-host","text":"In a full deployment, the host for both the Core and the other applications are the same. It is recommended to configure the host before you run the deployment. This is only possible if you know the fully qualified domain name (FQDN) or the IP address that the Ambassador Ingress assigns in your deployment beforehand. If that is the case, define the host using the following syntax: # Networking specific configuration networking: # Networking configuration specific to Core core: # Host of Core host: \"your-dx-instance.whateverdomain.com\" If you do not know the hostname beforehand, you can leave it blank and run an additional step later in the installation, that retrieves the assigned hostname from the Ambassador Ingress and configure all applications accordingly.","title":"Core host"},{"location":"containerization/helm/planning/helm_configure_networking/#configure-cross-origin-resource-sharing-cors","text":"The HCL Digital Experience 9.5 Helm Chart allows you to configure CORS configuration for all the addon to Core applications such as Digital Asset Management or Ring API. This allows you to access the APIs provided by those applications in other applications with ease. You can define a list of allowed hosts for a specific application using the following syntax in your custom-values.yaml : # Networking specific configuration networking: # Networking configurations specific to all addon applications addon: contentComposer: # CORS Origin configuration for Content Composer, comma separated list corsOrigin: \"https://my-different-application.net,https://the-other-application.com\" Refer to the HCL DX 9.5 values.yaml detail for all possible applications that can be configured.","title":"Configure Cross Origin Resource Sharing (CORS)"},{"location":"containerization/helm/planning/helm_configure_networking/#hybrid-host","text":"Configuring Hybrid Host In a Hybrid deployment, the host for the on-premise DX Core will be added in the core configuration section and the other applications host will be placed under the add-on section. See the following example: networking: # Networking configuration specific to Core core: # Host of Core, must be specified as a FQDN # If you are running hybrid, you need to specify the FQDN of the on-premise Core host # Example: eks-hybrid.dx.com host: \"your-dx-core-instance.whateverdomain.com\" port: \"10042\" contextRoot: \"wps\" personalizedHome: \"myportal\" home: \"portal\" addon: # Host of the addon applications # If you are not running hybrid, you can leave this value empty and the Core host will be used # If you are running hybrid, you need to specify the FQDN of the Kubernetes deployment # Example: eks-hybrid.apps.dx.com host: \"your-dx-apps-instance.whateverdomain.com\" # Port of the addon applications # If you are running hybrid, you can specify a port # If left empty, no specific port will be added to the host port: \"443\" # Setting if SSL is enabled for addon applications # If you are running hybrid, make sure to set this accordingly to the Kubernetes deployment configuration # Will default to true if not set ssl: \"true\" Please refer to the original values.yaml for all available applications that can be configured. See the Planning your container deployment using Helm topic for details.","title":"Hybrid host"},{"location":"containerization/helm/planning/helm_configure_networking/#configure-ingress-certificate","text":"To have the Ambassador Ingress allow forward requests to your applications, you must provide it with a TLS Certificate. This certificate is used for incoming/outgoing traffic from the outside of the Kubernetes or OpenShift cluster to your applications. Ambassador performs TLS offloading.","title":"Configure Ingress certificate"},{"location":"containerization/helm/planning/helm_configure_networking/#generate-self-signed-certificate","text":"It is recommended that you use a properly signed certificate for the Ambassador Ingress . However, it is also possible to create and use a self-signed certificate, for example, for staging or testing environment. Creation of that certificate can be achieved using the following commands for OpenSSL: # Creation of a private key openssl genrsa -out my-key.pem 2048 # Creation of a certificate signed by the private key created before openssl req -x509 -key my-key.pem -out my-cert.pem -days 365 -subj '/CN=my-cert This provides you with a key and cert file that can be used in the next step, creation of the certificate to your deployment.","title":"Generate self-signed certificate"},{"location":"containerization/helm/planning/helm_configure_networking/#use-certificate","text":"Create secret To have your deployment and the Ambassador Ingress use the certificate, you must store it in the Kubernetes or OpenShift cluster as a secret. The secret can be created using the following commands: Note: The secret name can be chosen by you and must be referenced in the next configuration step (the following example uses dx-tls-cert ). The namespace is the Kubernetes namespace where you want to deploy HCL Digital Experience 9.5 to (the example uses digital-experience ). # Create secret with the name \"dx-tls-cert\" # Secret will be created in the namespace \"digital-experience\" # You can either reference the cert and key file created before, or a proper signed certificate e.g. from your CA kubectl create secret tls dx-tls-cert --cert=my-cert.pem --key=my-key.pem -n digital-experience","title":"Use certificate"},{"location":"containerization/helm/planning/helm_configure_networking/#configure-secret-in-deployment","text":"You need to make sure that the reference to the secret is set up correctly in your custom-values.yaml . Otherwise your Ambassador Ingress is not able to answer HTTPS requests due to a missing certificate. You can set the name of the certificate used with the following syntax, the default value is dx-tls-cert : # Networking specific configuration networking: # TLS Certificate secret used for Ambassador Ingress tlsCertSecret: \"dx-tls-cert\" Note: Verify you have entered the correct name.","title":"Configure secret in deployment"},{"location":"containerization/helm/planning/helm_configure_networking/#configure-minimum-tls-version-for-ingress","text":"From CF201 and onwards the default minimum TLS version for the Ambassador Ingress is set to v1.2 . TLS v1.2 or higher is recommended to increase security. If support for older TLS versions is still required, then it can be adjusted via the custom-values.yaml . # Networking specific configuration networking: # Set the minimum acceptable TLS version for Amassador Ingress: v1.0, v1.1, # v1.2, or v1.3. It defaults to v1.2 minTlsVersion: \"v1.2\" Refer to Additional tasks for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure minimum TLS version for Ingress"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/","text":"Configure PersistentVolumeClaims (PVCs) To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note: The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information. Persistent Volume Types Important note: Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count. Configuration parameters To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important note: Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to. Sample PVC configurations The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" Sample Persistent Volume definitions Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps. Parent topic: Deploying container platforms using Helm","title":"Configure PersistentVolumeClaims (PVCs)"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/#configure-persistentvolumeclaims-pvcs","text":"To run HCL Digital Experience 9.5 Container deployments in your Kubernetes or OpenShift cluster, you need to set up PersistentVolumes (PVs) on your cluster and configure the Helm Chart to create the appropriate PersistentVolumeClaims (PVCs). Before you proceed, review the Persistent Volumes and related operations considerations topic in the DX Help Center. Note: The provisioning of PersistentVolumes (PVs) may differ based on your cluster configuration and your cloud provider. Please reference the documentation of your cloud provider for additional information.","title":"Configure PersistentVolumeClaims (PVCs)"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/#persistent-volume-types","text":"Important note: Ensure that your PersistentVolumes (PVs) are created with the Reclaim Policy set to RETAIN. This allows for the reuse of PVs after a PersistentVolumeClaim (PVC) is deleted. This is important to keep data persisted, for example, between deployments or tests. Refrain from using the Reclaim Policy DELETE unless you have the experience in managing these operations successfully, to avoid unpredictable results. This is not recommended in production use, as deleting PVCs causes the Kubernetes or OpenShift cluster to delete the bound PV as well, thus, deleting all the data on it. ReadWriteOnce (RWO) ReadWriteOnce PVs allow only one pod per volume to perform reading and writing transactions. This means that the data on that PV cannot be shared with other pods and is linked to one pod at a time. In the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm, the only DX applications leveraging RWO PVs are Core and Persistence. Information regarding how to calculate the number of required volumes for the DX Core and Persistence applications is presented in the Persistent Volumes and related operations considerations topic in the DX Help Center. Since Core requires RWO PVs per pod, it may be necessary to have auto-provisioning of such volumes configured in your cluster if you don't know the final maximum number of possible Core pods running at the same time. Each Core pod requires 2 RWO PVs. Since the number of pods for Persistence is limited by design, you need 2 RWO PVs for Persistence. ReadWriteMany (RWX) ReadWriteMany PVs support read and write operations by multiple pods. This means the data on that PV can be shared with other pods and can be linked to multiple pods at a time. In the HCL Digital Experience 9.5 Kubernetes and OpenShift deployment using Helm the only DX applications leveraging RWX PVs are Core and Digital Asset Management. Since the PV can be shared between all Core pods, you need one (1) RWX PV for Core, regardless of the pod count. Since the PV can be shared between all Digital Asset Management pods, you need one (1) RWX PV for Digital Asset Management, regardless of the pod count.","title":"Persistent Volume Types"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/#configuration-parameters","text":"To access the PersistentVolumes (PVs) on your cluster, the HCL Digital Experience 9.5 Kubernetes or OpenShift deployment using Helm creates PersistentVolumeClaims (PVCs) that binds the PVs to the corresponding pods. Each PVC that applications require allows you to configure the following parameters, as shown below. For a PVC of the Core application: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"manual\" requests: storage: \"10Gi\" # Optional volume name to specifically map to volumeName: Important note: Make sure to properly define the PVC configuration in your custom-values.yaml file before running the deployment. This avoids issues when trying to get your deployment up and running. *StorageClassName * Depending on your Cluster configuration, you may have configured a specific StorageClass that should be used for your PVs and the PVCs of HCL Digital Experience. This property allows you to enter the name of the StorageClass you want the deployment to use. PVCs then only accepts PVs that match the StorageClassName you have defined in the configuration. If there are no PVs that match, the pods remain pending and do not start until a fitting PV is provided by the cluster. If you enter an empty StorageClassName , Kubernetes falls back to the default StorageClass configured in your Cluster. Refer to your cloud provider for additional information about your default StorageClass, since this depends on your Kubernetes or OpenShift environment. Reference the original values.yaml file you have extracted as outlined in the Prepare configuration topic for all configurable PVCs. Requests Storage Storage allows you to define the amount of space that is required by the PVC. Once defined, it only accepts PVs that have the same or more storage capacity as requested. If there are no PVs matching the definitions, the pods remain pending and do not start until a properly-sized PV is provided by the cluster. VolumeName If you want your deployment to pick up a specific PV that you have created, use of the VolumeName can define that instruction. Ensure that the PV you created has a unique name. Then, add that name as a configuration parameter for the PVC. The PVCs only matches with a PV of that name, matching the other requirements-like type ( RWO/RWX , as defined by the deployment itself), storage capacity, and StorageClassName . This allows you to properly prepare your PVs beforehand and ensure that the applications store their data where you want them to.","title":"Configuration parameters"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/#sample-pvc-configurations","text":"The following are some examples for configuration of the PersistentVolumeClaims (PVCs) using your custom-values.yaml: Fallback to default StorageClass for all applications Leaving an empty StorageClassName causes Kubernetes or OpenShift to fall back to the StorageClass that has been configured as the default one in your cluster: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"\" # Application Log PVC, one per Core pod log: storageClassName: \"\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"\" Specific StorageClasses for all applications Setting the StorageClassName to mycloudstorage causes Kubernetes or OpenShift to create PVCs that only accepts PVs with the StorageClass mycloudstorage : # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Persistent Volumes for Persistence persistence: # Database PVC, one per Persistence pod database: storageClassName: \"mycloudstorage\" # Persistent Volumes for Open LDAP openLdap: # slapd directory PVC, one per Open LDAP pod slapd: storageClassName: \"mycloudstorage\" # certificate directory, on per Open LDAP pod certificate: storageClassName: \"mycloudstorage\" # ldap directory PVC, one per Open LDAP pod ldap: storageClassName: \"mycloudstorage\" # Persistent Volumes for Remote Search remoteSearch: # Remote Search profile PVC, one per Remote Search pod prsprofile: storageClassName: \"mycloudstorage\" Specific volume names Specifying a name ensures that Kubernetes or OpenShift only assigns PVs with the matching name to the PVCs created for the applications: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"core-profile\" # Persistent Volumes for Digital Asset Management digitalAssetManagement: # Binary storage PVC, shared by all Digital Asset Management Pods binaries: storageClassName: \"mycloudstorage\" # Optional volume name to specifically map to volumeName: \"dam-binaries\" Adjusted volume size for Core PVCs You may override the default sizes for PVCs by adjusting the storage requests: # Persistent Volume Setup volumes: # Persistent Volumes for Core core: # Shared profile PVC shared by all Core pods profile: storageClassName: \"mycloudstorage\" requests: storage: \"150Gi\" # Transaction Log PVC, one per Core pod tranlog: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\" # Application Log PVC, one per Core pod log: storageClassName: \"mycloudstorage\" requests: storage: \"1Gi\"","title":"Sample PVC configurations"},{"location":"containerization/helm/planning/helm_persistent_volume_claims/#sample-persistent-volume-definitions","text":"Sample StorageClass It is recommended to have a separate StorageClass for HCL Digital Experience 9.5 deployments in order to prevent other deployed applications in the same Kubernetes or OpenShift cluster to interfere with Persistent Volumes (PVs) that should only be used by HCL Digital Experience. The following example shows a StorageClass with the name dx-deploy-stg that can be created in your cluster for that purpose: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: dx-deploy-stg provisioner: kubernetes.io/no-provisioner reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Applying this yaml on your Kubernetes or OpenShift cluster creates the StorageClass as a cluster-wide resource. Sample Persistent Volume To leverage the StorageClass you created, you can use the following Persistent Volume example, which connects to an NFS Server of your choice to provide a PV: kind: PersistentVolume apiVersion: v1 metadata: name: wp-profile-volume spec: capacity: storage: 100Gi nfs: server: your_nfs_server.com path: /exports/volume_name accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: dx-deploy-stg mountOptions: - hard - nfsvers=4.1 - rsize=8388608 - wsize=8388608 - timeo=600 - retrans=2 - noresvport volumeMode: Filesystem Refer to Networking configuration for the next steps. Parent topic: Deploying container platforms using Helm","title":"Sample Persistent Volume definitions"},{"location":"containerization/helm/planning/helm_planning_deployment/","text":"Planning your container deployment using Helm Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Before proceeding with your HCL DX 9.5 deployment using Helm, review the following Help Center topics: Deploying HCL DX CF196 to container platforms using Helm for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 Container Update CF196 and later deployment using Helm . Prepare your HCL DX 9.5 target environment. This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 to Google Kubernetes Engine using Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Mandatory tasks: The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm. Prepare a namespace. Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster. You need to create a namespace in your Kubernetes cluster that contains all the resources related to your HCL DX 9.5 Container deployment. It is recommended that this is created before deployment as you may need to add an ImagePullSecret or configure the TLS certificate for the Ambassador Ingress before deployment. Identify a name for your namespace and create it using the following syntax: On Kubernetes platforms Kubectl ``` Command to create a namespace using kubectl This example creates a namespace called \"my-namespace\" kubectl create ns my-namespace ``` OpenShift For OpenShift, you must create a namespace with specific settings. Use the following namespace definition and save it as namespace.yaml. You must replace my-namespace in the template with the name of the namespace you are using. apiVersion: v1 kind: Namespace metadata: name: my-namespace annotations: openshift.io/sa.scc.mcs: \"s0:c24,c4\" openshift.io/sa.scc.supplemental-groups: \"1001/10000\" openshift.io/sa.scc.uid-range: \"1000/10000\" OpenShift client ``` Command to create namespace from template file oc apply -f namespace.yaml ``` Prepare the Helm deployment configuration file. Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. The configuration flow Helm provides multiple ways to define values that can be processed to run an installation. Processing involves a three-step approach, that is ordered sequentially within a hierarchy. Helm Chart values.yaml Every Helm Chart contains a values.yaml file. It defines all configurable parameters that a Helm Chart accepts and the default values that are used during an installation. If you do not provide any other configuration during an installation, Helm extracts all deployment information from the values.yaml file inside the Helm Chart. All parameters that were not overwritten using any other configuration methods return to their default values from the values.yaml file inside the Helm Chart. Custom value files Helm provides you with a way to maintain your own custom values files. You can specify a custom values file you want to use when running an installation. This custom values file only needs to contain the parameters that you want to overwrite with your preferred settings. Note: There is no need to have the same complete set of parameters inside your custom values file, as there are available by default in the Helm Chart values.yaml . As outlined previously in this section, everything that is not defined in your custom values file are applied using the defaults from values.yaml inside the Helm Charts. Please be aware that the parameters you can configure using your custom values file need to exactly align with those provided by the Helm Charts own values.yaml. You cannot configure anything that is not exposed in the values.yaml definition. Override parameters It is possible to define values using a --set parameter in the Helm CLI during the installation of a Helm Chart. Since there are many values that can be configured in the HCL Digital Experience deployment, we do not recommend this technique, since it makes installation commands very large and confusing. The default HCL DX 9.5 Container values.yaml file HCL DX 9.5 Helm Chart provides a default values.yaml, which contains all possible configuration parameters. To access this file, you may use the following command when you have the HCL DX 9.5 CF196 or later Helm Chart tar.gz file on hand: ``` Command to extract values.ymal from Helm Chart helm show values hcl-dx-deployment.tar.gz > values.yaml ``` The file contains all configurable parameters and their default values. You may use this file as a blueprint to create your own custom-values.yaml . You may also just rename the extracted values.yaml to custom-values.yaml . Note: Having a complete copy of the default values.yaml is not necessary and may bloat your configuration file with values that are already present in the DX Helm Chart. A custom configuration file Helm allows you to provide a custom configuration file during the installation or upgrade process. That file only overwrites settings that are defined within it. For parts of the configuration that are not defined in your custom configuration file, Helm returns to the default values in the values.yaml file inside the DX Helm Chart. This allows you to create a file that only overwrites settings that are required, keeping the overall size of your configuration file small and the maintainability high. This Help Center documentation refers to the custom configuration file as custom-values.yaml . You may name your custom configuration file as preferred. Load container images. This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same. Extract HCL Digital Experience 9.5 package. The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note: The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. ``` Unzip of HCL Digital Experience 9.5 CFxxx package unzip hcl-dx-kubernetes-v95-CF196.zip ``` The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: ``` hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt Notices file dx-dx-ambassador-image-154.tar.gz Image for the Ambassador Ingress hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for the Core Operator (not needed for Helm deployments) hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip Cloud deployment scripts incl. dxctl (not needed for Helm deployments) hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Content Composer hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for Core hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for the Digital Asset Management Operator (not needed for Helm deployments) hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Digital Asset Management hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip Sample UI for Experience API hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Image Processor hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz Image for OpenLDAP hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Digital Asset Management Persistence hcl-dx-redis-image-X.X.X.tar.gz Image for Ambassador Ingress Redis hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz Image for Remote Search hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz Image for Ring API hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz Image for Runtime Controller hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz Helm Charts ``` Load images locally. To load the individual image files, you may use the following command: ``` Command to load container image into local repository docker load < image-file-name.tar.gz docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz ``` If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: ``` Command to load all images at once Since HCL Digital Experience images are all containing the word \"images\", we can filter for fitting tar.gz files ls -f | grep image | xargs -L 1 docker load -i ``` This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: ``` List all images docker images Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Re-tag images. If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Important Note: Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: ``` Re-tag an existing loaded image docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION Example command for DX Core: docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: ``` Command to prefix all HCL Digital Experience container images export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' ``` The output may be verified by using the following command: ``` List all images docker images Command output (minified, example) REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Push to repository. You may use the following command to push the container images to your repository: ``` Push the new tagged images docker push NEW_IMAGE_TAG:VERSION Example command for core: docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to push all your locally processed images, you may use the following command: ``` Command to push all HCL Digital Experience images to a remote repository export the prefix for the repository structure, without tailing slash export REMOTE_REPO_PREFIX=\"my/test/repository\" Push the images, first we filter for the ones necessary Second we execute a docker push for each image docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' ``` After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster. Adjust deployment configuration. After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note: If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. ``` Fill in the values fitting to your configuration Ensure to use the correct image version tags images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\" ``` Additional tasks: If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Configure Networking topic for instructions on how to configure this. Refer to PersistentVolumeClaims (PVCs) for the next steps. Parent topic: Deploying container platforms using Helm","title":"Planning your container deployment using Helm"},{"location":"containerization/helm/planning/helm_planning_deployment/#planning-your-container-deployment-using-helm","text":"Helm is a software package manager that simplifies deployment of applications and services to Red Hat OpenShift and Kubernetes container platforms. Before proceeding with your HCL DX 9.5 deployment using Helm, review the following Help Center topics: Deploying HCL DX CF196 to container platforms using Helm for an understanding of the capabilities, deployment structures, configuration and scaling options available for HCL DX 9.5 CF196 and later deployments. Containerization requirements and limitations for an understanding of the requirements, including capacity planning, and current limitations for an HCL Digital Experience 9.5 Container Update CF196 and later deployment using Helm . Prepare your HCL DX 9.5 target environment. This section outlines mandatory and optional tasks that need to be done before installation of the HCL Digital Experience 9.5 Container Update CF196 to Google Kubernetes Engine using Helm. Support to deploy to Red Hat OpenShift, Amazon Elastic Kubernetes Service (Amazon EKS), and Microsoft Azure Kubernetes Service (AKS) using Helm is added in Container Update CF197. This includes preparing your cluster to have proper access to application container images, creating a custom configuration file that fits your deployment needs and configuring network and application settings to allow your HCL Digital Experience 9.5 CF196 and later deployment to work properly. Mandatory tasks: The following tasks are mandatory for HCL Digital Experience 9.5 Container deployment to operate in your Kubernetes cluster using Helm. Prepare a namespace. Before you can deploy HCL Digital Experience, it is recommended that you create a namespace inside your Kubernetes Cluster. You need to create a namespace in your Kubernetes cluster that contains all the resources related to your HCL DX 9.5 Container deployment. It is recommended that this is created before deployment as you may need to add an ImagePullSecret or configure the TLS certificate for the Ambassador Ingress before deployment. Identify a name for your namespace and create it using the following syntax: On Kubernetes platforms Kubectl ```","title":"Planning your container deployment using Helm"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-create-a-namespace-using-kubectl","text":"","title":"Command to create a namespace using kubectl"},{"location":"containerization/helm/planning/helm_planning_deployment/#this-example-creates-a-namespace-called-my-namespace","text":"kubectl create ns my-namespace ``` OpenShift For OpenShift, you must create a namespace with specific settings. Use the following namespace definition and save it as namespace.yaml. You must replace my-namespace in the template with the name of the namespace you are using. apiVersion: v1 kind: Namespace metadata: name: my-namespace annotations: openshift.io/sa.scc.mcs: \"s0:c24,c4\" openshift.io/sa.scc.supplemental-groups: \"1001/10000\" openshift.io/sa.scc.uid-range: \"1000/10000\" OpenShift client ```","title":"This example creates a namespace called \"my-namespace\""},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-create-namespace-from-template-file","text":"oc apply -f namespace.yaml ``` Prepare the Helm deployment configuration file. Create a configuration file that fits the needs of your target HCL DX 9.5 Container deployment. The configuration file is the heart of your deployment using Helm. It defines how HCL Digital Experience 9.5 is deployed to supported platforms, and how it behaves during runtime operations. This section explains how to create your own configuration file and how to leverage the existing values.yaml inside the Helm Chart. It also explains how to optionally overwrite settings in case the default set may not be sufficient. Important: Modification to any files (chart.yaml, templates, crds) in hcl-dx-deployment-vX.X.X_XXXXXXXX-XXXX.tar.gz, except custom-values.yaml or values.yaml, is not supported. The configuration flow Helm provides multiple ways to define values that can be processed to run an installation. Processing involves a three-step approach, that is ordered sequentially within a hierarchy. Helm Chart values.yaml Every Helm Chart contains a values.yaml file. It defines all configurable parameters that a Helm Chart accepts and the default values that are used during an installation. If you do not provide any other configuration during an installation, Helm extracts all deployment information from the values.yaml file inside the Helm Chart. All parameters that were not overwritten using any other configuration methods return to their default values from the values.yaml file inside the Helm Chart. Custom value files Helm provides you with a way to maintain your own custom values files. You can specify a custom values file you want to use when running an installation. This custom values file only needs to contain the parameters that you want to overwrite with your preferred settings. Note: There is no need to have the same complete set of parameters inside your custom values file, as there are available by default in the Helm Chart values.yaml . As outlined previously in this section, everything that is not defined in your custom values file are applied using the defaults from values.yaml inside the Helm Charts. Please be aware that the parameters you can configure using your custom values file need to exactly align with those provided by the Helm Charts own values.yaml. You cannot configure anything that is not exposed in the values.yaml definition. Override parameters It is possible to define values using a --set parameter in the Helm CLI during the installation of a Helm Chart. Since there are many values that can be configured in the HCL Digital Experience deployment, we do not recommend this technique, since it makes installation commands very large and confusing. The default HCL DX 9.5 Container values.yaml file HCL DX 9.5 Helm Chart provides a default values.yaml, which contains all possible configuration parameters. To access this file, you may use the following command when you have the HCL DX 9.5 CF196 or later Helm Chart tar.gz file on hand: ```","title":"Command to create namespace from template file"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-extract-valuesymal-from-helm-chart","text":"helm show values hcl-dx-deployment.tar.gz > values.yaml ``` The file contains all configurable parameters and their default values. You may use this file as a blueprint to create your own custom-values.yaml . You may also just rename the extracted values.yaml to custom-values.yaml . Note: Having a complete copy of the default values.yaml is not necessary and may bloat your configuration file with values that are already present in the DX Helm Chart. A custom configuration file Helm allows you to provide a custom configuration file during the installation or upgrade process. That file only overwrites settings that are defined within it. For parts of the configuration that are not defined in your custom configuration file, Helm returns to the default values in the values.yaml file inside the DX Helm Chart. This allows you to create a file that only overwrites settings that are required, keeping the overall size of your configuration file small and the maintainability high. This Help Center documentation refers to the custom configuration file as custom-values.yaml . You may name your custom configuration file as preferred. Load container images. This section presents how to load the DX 9.5 Container Update CF196 or later images into your container image repository, tag them to fit your repository structure, and push them to your repository, so that all Nodes in your Kubernetes or OpenShift cluster can deploy HCL Digital Experience 9.5 Pods. To use HCL Digital Experience 9.5 in your Kubernetes or OpenShift cluster, you have to make the container images available to all nodes of your cluster. Usually this is done by providing them through a container image repository. Depending on your cloud provider, there may be different types of default container image repositories already configured. Refer to the documentation of your cloud provider for setup and use of such platform container image repository. It is assumed that you have a repository configured and running, and is technically reachable from all your Kubernetes or OpenShift cluster nodes. In the following guidance, the docker CLI is used as a command reference. Tools like Podman may also be used, but are not described in this documentation. The procedure for the use of such tools are the same. Extract HCL Digital Experience 9.5 package. The HCL Digital Experience 9.5 Container Update packages are provided in a compressed .zip file, that can easily be unzipped using a utility of your choice. Refer to the latest HCL DX 9.5 Container Update Release CF196 and later file listings in the Docker deployment topic: Note: The following are examples using Container Update CF196 files. Replace those references with the HCL DX 9.5 Container Update CFxxx release files you are deploying. ```","title":"Command to extract values.ymal from Helm Chart"},{"location":"containerization/helm/planning/helm_planning_deployment/#unzip-of-hcl-digital-experience-95-cfxxx-package","text":"unzip hcl-dx-kubernetes-v95-CF196.zip ``` The package includes all DX 9.5 container images, and Helm Charts as tar.gz files. The content of the package looks similar to the following structure: ``` hcl-dx-kubernetes-v95-CF196.zip HCL DX notices V9.5 CF196.txt","title":"Unzip of HCL Digital Experience 9.5 CFxxx package"},{"location":"containerization/helm/planning/helm_planning_deployment/#notices-file","text":"dx-dx-ambassador-image-154.tar.gz","title":"Notices file"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-the-ambassador-ingress","text":"hcl-dx-cloud-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for the Ambassador Ingress"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-the-core-operator-not-needed-for-helm-deployments","text":"hcl-dx-cloud-scripts-v95_CFXXX_XXXXXXXX-XXXX.zip","title":"Image for the Core Operator (not needed for Helm deployments)"},{"location":"containerization/helm/planning/helm_planning_deployment/#cloud-deployment-scripts-incl-dxctl-not-needed-for-helm-deployments","text":"hcl-dx-content-composer-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Cloud deployment scripts incl. dxctl (not needed for Helm deployments)"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-content-composer","text":"hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Content Composer"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-core","text":"hcl-dx-digital-asset-management-operator-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Core"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-the-digital-asset-management-operator-not-needed-for-helm-deployments","text":"hcl-dx-digital-asset-manager-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for the Digital Asset Management Operator (not needed for Helm deployments)"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-digital-asset-management","text":"hcl-dx-experience-api-sample-ui-vX.X.X.XXXXXXXX-XXXX.zip","title":"Image for Digital Asset Management"},{"location":"containerization/helm/planning/helm_planning_deployment/#sample-ui-for-experience-api","text":"hcl-dx-image-processor-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Sample UI for Experience API"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-image-processor","text":"hcl-dx-openldap-image-v1.1.0-master_XXXXXXXX_XXXXXXXXXX.tar.gz","title":"Image for Image Processor"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-openldap","text":"hcl-dx-postgres-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for OpenLDAP"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-digital-asset-management-persistence","text":"hcl-dx-redis-image-X.X.X.tar.gz","title":"Image for Digital Asset Management Persistence"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-ambassador-ingress-redis","text":"hcl-dx-remote-search-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz","title":"Image for Ambassador Ingress Redis"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-remote-search","text":"hcl-dx-ringapi-image-vX.X.X_XXXXXXXX-XXXX.tar.gz","title":"Image for Remote Search"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-ring-api","text":"hcl-dx-runtime-controller-image-vX.X.X_XXXXXXXX-XXX.tar.gz","title":"Image for Ring API"},{"location":"containerization/helm/planning/helm_planning_deployment/#image-for-runtime-controller","text":"hcl-dx-deployment-vX.X.X_XXXXXXXX-XXX.tar.gz","title":"Image for Runtime Controller"},{"location":"containerization/helm/planning/helm_planning_deployment/#helm-charts","text":"``` Load images locally. To load the individual image files, you may use the following command: ```","title":"Helm Charts"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-load-container-image-into-local-repository","text":"","title":"Command to load container image into local repository"},{"location":"containerization/helm/planning/helm_planning_deployment/#docker-load-image-file-nametargz","text":"docker load < hcl-dx-core-image-v95_CFXXX_XXXXXXXX-XXXX.tar.gz ``` If you want to load all DX 9.5 CFxxx image files via one command, you may use the following command: ```","title":"docker load &lt; image-file-name.tar.gz"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-load-all-images-at-once","text":"","title":"Command to load all images at once"},{"location":"containerization/helm/planning/helm_planning_deployment/#since-hcl-digital-experience-images-are-all-containing-the-word-images","text":"","title":"Since HCL Digital Experience images are all containing the word \"images\","},{"location":"containerization/helm/planning/helm_planning_deployment/#we-can-filter-for-fitting-targz-files","text":"ls -f | grep image | xargs -L 1 docker load -i ``` This loads all images to your local repository, ready for further usage. You may verify if the loading is successful with the following command: ```","title":"we can filter for fitting tar.gz files"},{"location":"containerization/helm/planning/helm_planning_deployment/#list-all-images","text":"docker images","title":"List all images"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-output-minified-example","text":"REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Re-tag images. If you are using a Kubernetes cluster that is not configured to operate on your local machine, you may need to push the HCL Digital Experience 9.5 container images to a remote repository. To do so, you need to re-tag the images to point to your remote repository. Important Note: Do not change the version tags of the DX 9.5 images, as they are used for uniquely identifying which versions of DX applications are running in your cluster. You may re-tag any image using the following command: ```","title":"Command output (minified, example)"},{"location":"containerization/helm/planning/helm_planning_deployment/#re-tag-an-existing-loaded-image","text":"","title":"Re-tag an existing loaded image"},{"location":"containerization/helm/planning/helm_planning_deployment/#docker-tag-old_image_pathversion-new_image_tagversion","text":"","title":"docker tag OLD_IMAGE_PATH:VERSION NEW_IMAGE_TAG:VERSION"},{"location":"containerization/helm/planning/helm_planning_deployment/#example-command-for-dx-core","text":"docker tag hcl/dx/core:v95_CF195_20210514-1708 my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to prefix all HCL Digital Experience 9.5 container images with your repository structure, you may use the following command: ```","title":"Example command for DX Core:"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-prefix-all-hcl-digital-experience-container-images","text":"","title":"Command to prefix all HCL Digital Experience container images"},{"location":"containerization/helm/planning/helm_planning_deployment/#export-the-prefix-for-the-repository-structure-without-tailing-slash","text":"export REMOTE_REPO_PREFIX=\"my/test/repository\"","title":"export the prefix for the repository structure, without tailing slash"},{"location":"containerization/helm/planning/helm_planning_deployment/#first-we-list-all-hcl-digital-experience-95-images-then-we-remove-the-first-line-containing-the-header","text":"","title":"First we list all HCL Digital Experience 9.5 Images, then we remove the first line containing the header"},{"location":"containerization/helm/planning/helm_planning_deployment/#then-we-execute-the-docker-tag-command-prefixing-each-image-with-the-remote_repo_prefix","text":"docker images hcl/dx/* | tail -n +2 | awk -F ' ' '{system(\"docker tag \" $1 \":\" $2 \" $REMOTE_REPO_PREFIX/\" $1 \":\" $2) }' ``` The output may be verified by using the following command: ```","title":"Then we execute the docker tag command, prefixing each image with the $REMOTE_REPO_PREFIX"},{"location":"containerization/helm/planning/helm_planning_deployment/#list-all-images_1","text":"docker images","title":"List all images"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-output-minified-example_1","text":"REPOSITORY TAG IMAGE ID CREATED SIZE hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25GB my/test/repository/hcl/dx/remote-search v95_CF195_20210514-1708 e4c46618f404 4 weeks ago 2.25 hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB my/test/repository/hcl/dx/cloud-operator v95_CF195_20210515-0201 62cc304706a3 4 weeks ago 220MB hcl/dx/core v95_CF195_20210514-1708 36e30c620cdd 4 weeks ago 6.29GB my/test/repository/hcl/dx/core v95_CF195_20210514-1708 6e30c620cdd 4 weeks ago 6.29GB hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB my/test/repository/hcl/dx/openldap v1.1.0-master_20210514_1621013302 a5519e06dd17 4 weeks ago 772MB hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB my/test/repository/hcl/dx/image-processor v1.8.0_20210514-1712 d5d99d86f81a 4 weeks ago 507MB hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB my/test/repository/hcl/dx/digital-asset-manager v1.8.0_20210514-1711 19c8b76b1cad 4 weeks ago 547MB hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/digital-asset-management-operator v95_CF195_20210514-1714 bc0f5638817a 4 weeks ago 218MB my/test/repository/hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/content-composer v1.8.0_20210514-1707 62b7b54d3895 4 weeks ago 427MB hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB my/test/repository/hcl/dx/postgres v1.8.0_20210514-1708 d94672f395ad 4 weeks ago 498MB hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB my/test/repository/hcl/dx/ringapi v1.8.0_20210514-1709 505eebb52ebf 4 weeks ago 397MB ``` Push to repository. You may use the following command to push the container images to your repository: ```","title":"Command output (minified, example)"},{"location":"containerization/helm/planning/helm_planning_deployment/#push-the-new-tagged-images","text":"","title":"Push the new tagged images"},{"location":"containerization/helm/planning/helm_planning_deployment/#docker-push-new_image_tagversion","text":"","title":"docker push NEW_IMAGE_TAG:VERSION"},{"location":"containerization/helm/planning/helm_planning_deployment/#example-command-for-core","text":"docker push my/test/repository/hcl/dx/core:v95_CF195_20210514-1708 ``` If you want to push all your locally processed images, you may use the following command: ```","title":"Example command for core:"},{"location":"containerization/helm/planning/helm_planning_deployment/#command-to-push-all-hcl-digital-experience-images-to-a-remote-repository","text":"","title":"Command to push all HCL Digital Experience images to a remote repository"},{"location":"containerization/helm/planning/helm_planning_deployment/#export-the-prefix-for-the-repository-structure-without-tailing-slash_1","text":"export REMOTE_REPO_PREFIX=\"my/test/repository\"","title":"export the prefix for the repository structure, without tailing slash"},{"location":"containerization/helm/planning/helm_planning_deployment/#push-the-images-first-we-filter-for-the-ones-necessary","text":"","title":"Push the images, first we filter for the ones necessary"},{"location":"containerization/helm/planning/helm_planning_deployment/#second-we-execute-a-docker-push-for-each-image","text":"docker images $REMOTE_REPO_PREFIX/hcl/dx/* | awk -F ' ' '{system(\"docker push \" $1 \":\" $2)}' ``` After running this command, Docker goes ahead and pushes the images to your remote repository. After the push, the container images are now ready for use by your Kubernetes or OpenShift cluster. Adjust deployment configuration. After you have successfully prepared all DX 9.5 images, you need to configure the images inside your custom-values.yaml. The following syntax may be used to define the correct image configuration for your environment: Note: If deploying to a Hybrid environment, with DX 9.5 Container Update CF198 or later, the Core needs to be set as false, since Core is already installed to an On-premise Server. ```","title":"Second we execute a docker push for each image"},{"location":"containerization/helm/planning/helm_planning_deployment/#fill-in-the-values-fitting-to-your-configuration","text":"","title":"Fill in the values fitting to your configuration"},{"location":"containerization/helm/planning/helm_planning_deployment/#ensure-to-use-the-correct-image-version-tags","text":"images: repository: \"my/test/repository\" # Image tag for each application tags: contentComposer: \"v95_CFXXX_XXXXXXXX-XXXX\" core: \"v95_CFXXX_XXXXXXXX-XXXX\" designStudio: \"vX.X.X_XXXXXXXX-XXXX\" digitalAssetManagement: \"vX.X.X_XXXXXXXX-XXXX\" imageProcessor: \"vX.X.X_XXXXXXXX-XXXX\" openLdap: \"vX.X.X_XXXXXXXX-XXXX\" persistence: \"vX.X.X_XXXXXXXX-XXXX\" remoteSearch: \"v95_CFXXX_XXXXXXXX-XXXX\" ringApi: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorIngress: \"vX.X.X_XXXXXXXX-XXXX\" ambassadorRedis: \"vX.X.X_XXXXXXXX-XXXX\" runtimeController: \"vX.X.X_XXXXXXXX-XXXX\" # Image name for each application names: contentComposer: \"hcl/dx/content-composer\" core: \"hcl/dx/core\" designStudio: \"hcl/dx/design-studio\" digitalAssetManagement: \"hcl/dx/digital-asset-manager\" imageProcessor: \"hcl/dx/image-processor\" openLdap: \"hcl/dx/openldap\" persistence: \"hcl/dx/postgres\" remoteSearch: \"hcl/dx/remote-search\" ringApi: \"hcl/dx/ringapi\" ambassadorIngress: \"hcl/dx/ambassador\" ambassadorRedis: \"hcl/dx/redis\" runtimeController: \"hcl/dx/runtime-controller\" ``` Additional tasks: If your remote repository requires access credentials, it is necessary to configure an ImagePullSecret to allow your cluster nodes to have proper access to the HCL DX 9.5 container images. Please refer to Configure Networking topic for instructions on how to configure this. Refer to PersistentVolumeClaims (PVCs) for the next steps. Parent topic: Deploying container platforms using Helm","title":"Ensure to use the correct image version tags"},{"location":"containerization/helm/planning/limitations_requirements/","text":"Containerization requirements and limitations This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels. Requirements and Limitations for Helm-based deployments This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0 Requirements and Limitations for Operator-based deployments Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Containerization requirements and limitations"},{"location":"containerization/helm/planning/limitations_requirements/#containerization-requirements-and-limitations","text":"This section describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations. Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . Consult the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages for the latest updates on supported platforms, components, and release levels.","title":"Containerization requirements and limitations"},{"location":"containerization/helm/planning/limitations_requirements/#requirements-and-limitations-for-helm-based-deployments","text":"This section describes requirements and current limitations for HCL Digital Experience 9.5 Container Update CF200 and later deployments using Helm. HCL DX 9.5 CF200 and later is architected to run on any Certified Kubernetes platform ( https://www.cncf.io/certification/software-conformance ), provided that, the Kubernetes platform is hosted on x86_64 hardware the Kubernetes platform is officially supported by Helm ( https://helm.sh/docs/topics/kubernetes_distros/ ). For the list of Kubernetes versions that are tested and supported by HCL, refer to the HCL DX supported hardware and software statements page. Even though the platforms might be Certified Kubernetes platforms, you might find the environments varying slightly based on the vendors. HCL Support will make a reasonable effort to assist the customer in problem resolution in scenarios where the Kubernetes version is still under support by the vendor. If there are any unresolved issues, HCL Support will provide alternative implementation recommendations or open Feature Requests for the problem scenario. Internally, HCL tests DX against a range of Kubernetes platforms that is regularly reviewed and updated. We do not test with every single platform vendor, but aim to cover a representative sample of popular Kubernetes implementations. See the HCL DX supported hardware and software statements on the HCL Support Knowledge Base for additional details. To deploy HCL Digital Experience 9.5 CF200 to the supported Kubernetes platforms using Helm, the following are required: Helm installation : Download and install Helm to your target environment. HCL DX 9.5 CF200 and later container deployment is tested and is supported with Helm v3. For more information regarding the supported Helm version for individual Kubernetes versions, refer Helm documentation . Migration : For information about migrating from Operator-based to Helm-based deployments, see Migrating from Operator-based to Helm-based deployments . Container platform capacity resource requirements : The following table outlines the default minimum and maximum capacity of container resources requested by the HCL DX 9.5 Container Components in the Helm-based deployments. Component Resource name Pod Minimum CPU Pod Minimum Memory No. of Pods Minimum Core core 0.8 3072MB 1 Ring API ringApi 0.1 128MB 1 Content Composer contentComposer 0.1 128MB 1 Design Studio designStudio 0.1 128MB 1 Digital Asset Management digitalAssetManagement 0.25 1024MB 1 DAM Persistence Connection Pool persistenceConnectionPool 0.5 512MB 1 DAM Persistence Node persistenceNode 1 1024MB 1 DAM Persistence Metrics Exporter persistenceMetricsExporter 0.1 128MB 0 Image processor imageProcessor 0.1 1280MB 1 Open LDAP openLdap 0.2 512MB 1 Remote search remoteSearch 0.25 768MB 1 (Max 1 Pod) Runtime Controller runtimeController 0.1 256MB 1 Ambassador Ingress ambassadorIngress 0.2 300MB 1 Ambassador Redis ambassadorRedis 0.1 256MB 0 Sidecar sidecar 0.1 64MB 0","title":"Requirements and Limitations for Helm-based deployments"},{"location":"containerization/helm/planning/limitations_requirements/#requirements-and-limitations-for-operator-based-deployments","text":"Attention: Beginning with HCL Digital Experience 9.5 Container Update CF200, HCL has discontinued releasing the HCL Digital Experience (DX) Operator-based deployments and will provide support only for Helm-based deployments . There will be no further updates or code fixes provided for the Operator-based deployments. HCL requires all customers to migrate to Helm-based deployments for their DX installations. HCL will work with our customers as they transition from Operator-based to Helm-based deployments. For more information on the migration process, see Migrating from Operator-based to Helm-based deployments . The following describes the requirements to deploy the HCL Digital Experience 9.5 images to container platforms and current limitations using the Operator-based deployment process: HCL Digital Experience 9.5 is supported on Docker, Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), and Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE). Other Kubernetes platforms are not fully supported. The HCL Operator is not likely to work, however, support for additional Kubernetes as a Service (KaaS) is ongoing and additions is reflected in the HCL Digital Experience 9.5 Support Statements. Additional features and functions may be tied to the use of the HCL DX Operators for deployment. HCL highly recommends following the deployment strategies outlined within this documentation. HCL Digital Experience 9.5 containerization is focused on deployment and it uses an operator-based deployment. The goals are: To introduce a supported containerized deployment that HCL can continually extend; To provide customers with the best possible experience; To provide a high level of customization in the deployment and continue to expand on that, along with increased automation; and To maintain separation of product and custom code. Customers need to follow the recommended deployment model to ensure the availability of future functions and prevent potential conflicts. Notes: HCL Digital Experience is a database-intensive application, it is not recommended to use Apache Derby for production use. For specific versions of databases supported for production, see the HCL Digital Experience 9.5 Support Statements on the HCL Digital Experience Support pages. Creation of Virtual Portals take longer when implemented in Red Hat OpenShift. Plan for adequate time to allow processing, and re-verify the results are completed by refreshing the web browser administrative panel. Customers should not modify the HCL Digital Experience 9.5 Docker images provided by HCL for deployment. This restriction includes use of these images as a base to create a new image, which results in a new image ID and an unsupported configuration. Instead, customers deploying the images should follow best practices and maintain customizations in the wp_profile and the deployment database. Scripts and custom files should be stored in wp_profile (/opt/HCL/wp_profile/). See the Deployment Help Center topics for more information Customers should not run multiple HCL Digital Experience 9.5 container deployments in a single Kubernetes namespace (in the case of Red Hat OpenShift, in a single OpenShift project). This configuration is not supported at this time. It is not supported to run two different versions of HCL Digital Experience 9.5 container deployments in a single Kubernetes cluster. Use of Web Application Bridge is currently unsupported on HCL Digital Experience 9.5 deployments to container platforms such as Kubernetes and Red Hat OpenShift, using the Operator-based deployment method. Beginning with HCL DX Container Update CF199, Web Application Bridge can be used in container deployments using the Helm deployment method. Supported file system requirements : Requires an **AccessMode** of **ReadWriteMany** . Requires a minimum of 40 GB , with the default request set to 100 GB . Note: HCL Digital Experience is input-output (I/O) intensive and requires a high performing file system for optimization. Container platform capacity requirements : The following table outlines the minimum and maximum capacity requested and managed by HCL DX 9.5 Container Components: Component Pod minimum CPU Pod maximum CPU Pod minimum memory Pod maximum memory No. of minimum pods DX 9.5 Core 2 5 6 GB 8 GB 1 Experience API 0.5 1 1 GB 2 GB 1 Content Composer 0.5 1 1 GB 2 GB 1 Digital Asset Management 0.5 2 1 GB 2 GB 3 Persistence 1 2 1 GB 3 GB 1 Image processor 1 2 2 GB 2 GB 1 Remote search 1 3 1 GB 4 GB 1 Operators Shared - minimal Shared - minimal Shared - minimal Shared - minimal 2 Ambassador 0.3 1 400 MB 600 MB 3 Redis 0.3 1 400 MB 600 MB 3 Postgres-RO 1 2 1 GB 3 GB 1 Additional considerations in implementation : ConfigEngine and ConfigWizard should only be used when there is a single instance When more than one instance is running, the ConfigEngine is disabled and the ConfigWizard route is removed. As an example, the Site Builder is calling the ConfigEngine in the background. But because multiple instances are running, an Error 500 occurs because the ConfigEngine is disabled. AllConfigEngine.sh tasks should be run in configure mode with only one instance running. JavaServer Faces (JSF) portlet bridge With DX 9.5 Container Update CF171 and higher, WebSphere Application Server 9.0.5.2 is included and that IBM fix pack removed the IBM JSF portlet bridge. If you are using JSF portlets and leverage the JSF portlet bridge, proceed to the HCL DX 9.5 Container Update CF18 for the required JavaServer Faces Bridge support before moving to a container-based deployment. The HCL JavaServer Faces Bridge is added to HCL Digital Experience offerings with Container Update CF18 and CF18 on-premises platform CF update. For more information please see What's New in Container Update CF18 . Note: For information about the limitations related to JSF 2.2 support, see Limitations when running HCL DX Portlet Bridge on WebSphere Application Server 9.0 . Parent topic: Digital Experience on containerized platforms","title":"Requirements and Limitations for Operator-based deployments"},{"location":"docker/docker/","text":"Docker image list This section presents the latest HCL DX 9.5 Docker container update images available. Docker container update file list The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository HCL DX 9.5 CF201 Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz HCL DX 9.5 CF199 Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table. HCL DX 9.5 CF198 CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz HCL DX 9.5 CF197 CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz HCL DX 9.5 CF196 CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz HCL DX 9.5 CF195 CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz HCL DX 9.5 CF193 CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz HCL DX 9.5 CF182 CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz HCL DX 9.5 CF181 CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"Docker image list"},{"location":"docker/docker/#docker-image-list","text":"This section presents the latest HCL DX 9.5 Docker container update images available.","title":"Docker image list"},{"location":"docker/docker/#docker-container-update-file-list","text":"The HCL DX 9.5 container update releases may be obtained from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Video: How to upload HCL Digital Experience 9.5 CF container images to a private repository","title":"Docker container update file list"},{"location":"docker/docker/#hcl-dx-95-cf201","text":"Container Update CF201 If deploying the HCL DX 9.5 CF201 release, the package name and images are as follows: HCL DX 9.5 CF201 DXClient files : ``` hcl-dxclient-image-v95_CF201_20220207-1614.zip - ``` hcl-dxclient-v95_CF201_20220207-1613.zip **HCL DX 9.5 CF\\_201-hcl-dx-kubernetes-v95-CF201.zip**: - ``` HCL DX notices V9.5 CF201.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.14.0_20220207-1550.tar.gz - ``` hcl-dx-core-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-deployment-v2.4.0_20220207-1606.tgz - ``` hcl-dx-design-studio-image-v0.7.0_20220207-1549.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.13.0_20220207-1609.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20220207-1549.zip - ``` hcl-dx-image-processor-image-v1.14.0_20220207-1606.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20220207-1556.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.14.0_20220207-1612.tar.gz - ``` hcl-dx-persistence-image-v1.14.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.12.0_20220207-1611.tar.gz - ``` hcl-dx-persistence-node-image-v1.4_20220207-1549.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF201_20220206-1331.tar.gz - ``` hcl-dx-ringapi-image-v1.14.0_20220207-1554.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF201_20220207-1558.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz ## HCL DX 9.5 CF200 - **Container Update CF200** If deploying the HCL DX 9.5 CF200 release, the package name and images are as follows: **HCL DX 9.5 CF200 DXClient files**: - ``` hcl-dxclient-image-v95_CF200_20211213-1500.zip - ``` hcl-dxclient-v95_CF200_20211213-1459.zip **HCL DX 9.5 CF\\_200-hcl-dx-kubernetes-v95-CF200.zip** **Important:** With the Operator-based deployment being removed starting in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. For more information, see [List of image files and changes from CF198 and later](#simpletable_cst_4lf_yrb). - ``` HCL DX notices V9.5 CF200.txt - ``` hcl-dx-ambassador-image-1142.tar.gz - ``` hcl-dx-content-composer-image-v1.13.0_20211213-1443.tar.gz - ``` hcl-dx-core-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-deployment-v2.2.0_20211213-1446.tgz - ``` hcl-dx-design-studio-image-v0.6.0_20211213-1448.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.12.0_20211213-1448.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211213-1454.zip - ``` hcl-dx-image-processor-image-v1.13.0_20211213-1446.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211213-1444.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.11.0_20211213-1458.tar.gz - ``` hcl-dx-persistence-node-image-v1.3_20211213-1454.tar.gz - ``` hcl-dx-redis-image-5.0.14.tar.gz - ``` hcl-dx-remote-search-image-v95_CF200_20211213-1442.tar.gz - ``` hcl-dx-ringapi-image-v1.13.0_20211213-1457.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF200_20211213-1444.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz","title":"HCL DX 9.5 CF201"},{"location":"docker/docker/#hcl-dx-95-cf199","text":"Container Update CF199 If deploying the HCL DX 9.5 CF199 release, the package name and images are as follows: HCL DX 9.5 CF199 DXClient files : ``` hcl-dxclient-image-v95_CF199_20211029-1357.zip - ``` hcl-dxclient-v95_CF199_20211029-1357.zip **HCL DX 9.5 CF\\_199-hcl-dx-kubernetes-v95-CF199.zip** - ``` HCL DX notices V9.5 CF199.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF199_20211029-1700.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF199_20211029-1700.zip - ``` hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz - ``` hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-deployment-v2.1.0_20211029-1346.tgz - ``` hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF199_20211029-1342.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip - ``` hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz - ``` hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz - ``` hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz - ``` hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz - ``` hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz - ``` hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz - ``` hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Important: With the Operator-based deployment being deprecated in CF198 and planned to be removed in HCL DX Container Update 9.5 CF200, you will find some changes in the list of files and their names in CF199 and later releases. Component Image name CF198 CF199 DX Core hcl-dx-core-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz hcl-dx-core-image-v95_CF199_20211029-1348.tar.gz Ring API hcl-dx-ringapi-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz hcl-dx-ringapi-image-v1.12.0_20211029-1357.tar.gz Content Composer hcl-dx-content-composer-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz hcl-dx-content-composer-image-v1.12.0_20211029-1341.tar.gz DX Deployment hcl-dx-deployment-vx.x.x_xxxxxxxx-xxxx.tgz hcl-dx-deployment-v2.0.0_20210917-1441.tgz hcl-dx-deployment-v2.1.0_20211029-1346.tgz Design Studio hcl-dx-design-studio-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz hcl-dx-design-studio-image-v0.5.0_20211029-0013.tar.gz Digital Asset Management hcl-dx-digital-asset-manager-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz hcl-dx-digital-asset-manager-image-v1.11.0_20211029-1350.tar.gz Persistence Connection Pool hcl-dx-persistence-connection-pool-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz hcl-dx-persistence-connection-pool-image-v1.11.0_20211029-0224.tar.gz Persistence Node hcl-dx-persistence-node-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz hcl-dx-persistence-node-image-v1.1_20211029-0148.tar.gz Persistence Metrics Exporter hcl-dx-persistence-metrics-exporter-image-vx.x.x_xxxxxxxx-xxxx.tar.gz NA hcl-dx-persistence-metrics-exporter-image-v1.10.0_20211029-1352.tar.gz Persistence hcl-dx-persistence-image- vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz hcl-dx-persistence-image-v1.11.0_20211029-1349.tar.gz DX Experience API hcl-dx-experience-api-sample-ui-vx.x.x.xxxxxxxx-xxxx.zip hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip hcl-dx-experience-api-sample-ui-v0.2.0.20211029-1354.zip Image processor hcl-dx-image-processor-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz hcl-dx-image-processor-image-v1.12.0_20211029-1346.tar.gz Open LDAP hcl-dx-openldap-image-vx.x.x_xxxxxxxx-xxxx.tar.gz hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz hcl-dx-openldap-image-v1.2.0_20211029-1342.tar.gz Remote search hcl-dx-remote-search-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz hcl-dx-remote-search-image-v95_CF199_20211029-1348.tar.gz Runtime Controller hcl-dx-runtime-controller-image-v95_CFxxx_xxxxxxxx-xxxx.tar.gz hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz hcl-dx-runtime-controller-image-v95_CF199_20211029-1344.tar.gz Ambassador hcl-dx-ambassador-image-xxx.tar.gz hcl-dx-ambassador-image-154.tar.gz hcl-dx-ambassador-image-154.tar.gz Redis hcl-dx-redis-image-x.x.x.tar.gz hcl-dx-redis-image-5.0.1.tar.gz hcl-dx-redis-image-5.0.1.tar.gz Sidecar hcl-dx-sidecar-image-vx.x._x.x-xxx.tar.gz NA hcl-dx-sidecar-image-v1.0.0_8.4-205.tar.gz Note: The new image files and the change in file names are highlighted in the table.","title":"HCL DX 9.5 CF199"},{"location":"docker/docker/#hcl-dx-95-cf198","text":"CF198 If deploying HCL DX 9.5 CF198 release, the package name and images are as follows: HCL DX 9.5 CF198 DXClient files : ``` hcl-dxclient-image-v95_CF198_20210917-1455.zip - ``` hcl-dxclient-v95_CF198_20210917-1455.zip **HCL DX 9.5 CF198 - hcl-dx-kubernetes-v95-CF198.zip**: - ``` HCL DX notices V9.5 CF198.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF198_20210917-1749.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF198_20210917-1749.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210917-1437.tar.gz - ``` hcl-dx-core-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210917-1441.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF198_20210917-1437.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210917-1444.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210917-1439.zip - ``` hcl-dx-image-processor-image-v1.11.0_20210917-1449.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210917-1437.tar.gz - ``` hcl-dx-persistence-postgres-repmgr-image-v1.1_20210916-0148.tar.gz - ``` hcl-dx-postgres-image-v1.11.0_20210917-1444.tar.gz - ``` hcl-dx-postgres-persistence-pgpool-image-v1.11.0_20210916-0224.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF198_20210917-1438.tar.gz - ``` hcl-dx-ringapi-image-v1.11.0_20210917-1441.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF198_20210917-1441.tar.gz - ``` hcl-dx-site-manager-image-v0.4.0_20210917-1445.tar.gz","title":"HCL DX 9.5 CF198"},{"location":"docker/docker/#hcl-dx-95-cf197","text":"CF197 If deploying HCL DX 9.5 CF197 release, the package name and images are as follows: HCL DX 9.5 CF197 Client - hcl-dx-client-v95-CF197.zip : ``` hcl-dxclient-image-v95_CF197_20210806-1311.zip - ``` hcl-dxclient-v95_CF197_20210806-1311.zip **HCL DX 9.5 CF197 - hcl-dx-kubernetes-v95-CF197.zip**: - ``` HCL DX notices V9.5 CF197.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF197_20210806-1310.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF197_20210806-1310.zip - ``` hcl-dx-content-composer-image-v1.10.0_20210806-1258.tar.gz - ``` hcl-dx-core-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-deployment-v2.0.0_20210806-1300.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210806-1308.zip - ``` hcl-dx-image-processor-image-v1.10.0_20210806-1300.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210806-1258.tar.gz - ``` hcl-dx-postgres-image-v1.10.0_20210806-1302.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF197_20210806-1259.tar.gz - ``` hcl-dx-ringapi-image-v1.10.0_20210806-1311.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF197_20210806-1258.tar.gz - ``` hcl-dx-site-manager-image-v0.3.0_20210806-1308.tar.gz","title":"HCL DX 9.5 CF197"},{"location":"docker/docker/#hcl-dx-95-cf196","text":"CF196 If deploying HCL DX 9.5 CF196 release, the package name and images are as follows: HCL DX 9.5 CF196 Client - hcl-dx-client-v95-CF196.zip : ``` hcl-dxclient-image-v95_CF196_20210625-2028.zip - ``` hcl-dxclient-v95_CF196_20210625-2029.zip **HCL DX 9.5 CF196 - hcl-dx-kubernetes-v95-CF196.zip**: - ``` HCL DX notices V9.5 CF196.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF196_20210625-2033.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF196_20210625-2033.zip - ``` hcl-dx-content-composer-image-v1.9.0_20210625-2012.tar.gz - ``` hcl-dx-core-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-deployment-v1.0.0_20210625-2026.tgz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210625-2023.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210625-2015.tar.gz - ``` hcl-dx-openldap-image-v1.2.0_20210625-2013.tar.gz - ``` hcl-dx-postgres-image-v1.9.0_20210625-2016.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF196_20210625-2011.tar.gz - ``` hcl-dx-ringapi-image-v1.9.0_20210625-2026.tar.gz - ``` hcl-dx-runtime-controller-image-v95_CF196_20210625-2013.tar.gz - ``` hcl-dx-site-manager-image-v0.2.0_20210625-2023.tar.gz","title":"HCL DX 9.5 CF196"},{"location":"docker/docker/#hcl-dx-95-cf195","text":"CF195 If deploying HCL DX 9.5 CF195 release, the package name and images are as follows: HCL DX 9.5 CF195 Client - hcl-dx-client-v95-CF195.zip : ``` dxclient_v1.4.0_20210514-1713.zip **HCL DX 9.5 CF195 - hcl-dx-kubernetes-v95-CF195.zip**: - ``` HCL DX notices V9.5 CF195.txt - ``` dxclient_v1.4.0_20210514-1713.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF195_20210515-0201.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF195_20210514-1707.zip - ``` hcl-dx-content-composer-image-v1.8.0_20210514-1707.tar.gz - ``` hcl-dx-core-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF195_20210514-1714.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.8.0_20210514-1711.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210514-1708.zip - ``` hcl-dx-image-processor-image-v1.8.0_20210514-1712.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210514_1621013302.tar.gz - ``` hcl-dx-postgres-image-v1.8.0_20210514-1708.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF195_20210514-1708.tar.gz - ``` hcl-dx-ringapi-image-v1.8.0_20210514-1709.tar.gz ## HCL DX 9.5 CF194 - **CF194** Important note: Please consult the HCL DX Support Knowledge Base article, [Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update](https://support.hcltechsw.com/csm?id=kb_article&sysparm_article=KB0089699), to apply the certificate update to your HCL DX 9.5 container deployment. If deploying HCL DX 9.5 CF194 release, the package name and images are as follows: **HCL DX 9.5 CF194 Client - hcl-dx-client-v95-CF194.zip**: - ``` dxclient_v1.3.0_20210415-2128.zip **HCL DX 9.5 CF194 - hcl-dx-kubernetes-v95-CF194.zip**: - ``` HCL DX notices V9.5 CF194.txt - ``` dxclient_v1.3.0_20210415-2128.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF194_20210416-0233.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF194_20210416-0233.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210415-2121.tar.gz - ``` hcl-dx-core-image-v95_CF194_20210415-2120.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF194_20210415-2127.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210415-2121.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210416_1618540820.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210415-2120.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF194_20210415-2120.tar.gz","title":"HCL DX 9.5 CF195"},{"location":"docker/docker/#hcl-dx-95-cf193","text":"CF193 If deploying HCL DX 9.5 CF193 release, the package name and images are as follows: HCL DX 9.5 CF193 Client - hcl-dx-client-v95-CF193.zip : ``` dxclient_v1.3.0_20210331-1335.zip **HCL DX 9.5 CF193 - hcl-dx-kubernetes-v95-CF193.zip**: - ``` HCL DX notices V9.5 CF193.txt - ``` dxclient_v1.3.0_20210331-1335.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF193_20210331-1847.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF193_20210331-1847.zip - ``` hcl-dx-content-composer-image-v1.7.0_20210331-1333.tar.gz - ``` hcl-dx-core-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF193_20210331-1335.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.7.0_20210331-1339.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210331-1343.zip - ``` hcl-dx-image-processor-image-v1.7.0_20210331-1336.tar.gz - ``` hcl-dx-openldap-image-v1.1.0-master_20210331_1617216873.tar.gz - ``` hcl-dx-postgres-image-v1.7.0_20210331-1337.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF193_20210331-1336.tar.gz - ``` hcl-dx-ringapi-image-v1.7.0_20210331-1339.tar.gz ## HCL DX 9.5 CF192 - **CF192** If deploying HCL DX 9.5 CF192 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF192.zip**: - ``` HCL DX notices V9.5 CF192.txt - ``` dxclient_v1.2.0_20210305-1758.zip - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF192_20210305-2309.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF192_20210305-2309.zip - ``` hcl-dx-content-composer-image-v1.6.0_20210305-1756.tar.gz - ``` hcl-dx-core-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF192_20210305-1757.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.6.0_20210305-1802.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20210305-1805.zip - ``` hcl-dx-image-processor-image-v1.6.0_20210305-1758.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20210305_1614986151.tar.gz - ``` hcl-dx-postgres-image-v1.6.0_20210305-1800.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF192_20210305-1758.tar.gz - ``` hcl-dx-ringapi-image-v1.6.0_20210305-1802.tar.gz ## HCL DX 9.5 CF191 - **CF191** If deploying HCL DX 9.5 CF191 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF191.zip** file: - ``` HCL DX notices V9.5 CF191.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF191_20201214-1527.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF191_20201214-1527.zip - ``` hcl-dx-content-composer-image-v1.5.0_20201211-2151.tar.gz - ``` hcl-dx-core-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF191_20201211-2152.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201211-2205.zip - ``` hcl-dx-image-processor-image-v1.5.0_20201211-2154.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201212_1607741365.tar.gz - ``` hcl-dx-postgres-image-v1.5.0_20201211-2155.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF191_20201212-1421.tar.gz - ``` hcl-dx-ringapi-image-v1.5.0_20201211-2200.tar.gz - ``` dxclient_v1.1.0_20201211-2153.zip **Note:** HCL DX 9.5 CF191 images are available and may be installed. HCL DX 9.5 CF191 is supported for new deployments only. DX administrators should not upgrade DX 9.5 container deployments to this release. ## HCL DX 9.5 CF19 - **CF19** If deploying HCL DX 9.5 CF19 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF19.zip file**: - ``` HCL DX notices V9.5 CF19.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF19_20201110-0401.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF19_20201110-0401.zip - ``` hcl-dx-content-composer-image-v1.4.0_20201109-2203.tar.gz - ``` hcl-dx-core-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF19_20201109-2204.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-experience-api-sample-ui-v0.2.0.20201109-2208.zip - ``` hcl-dx-image-processor-image-v1.4.0_20201109-2204.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20201110_1604981292.tar.gz - ``` hcl-dx-postgres-image-v1.4.0_20201109-2206.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF19_20201109-2312.tar.gz - ``` hcl-dx-ringapi-image-v1.4.0_20201109-2209.tar.gz - ``` dxclient_v1.0.0_20201110-2010.zip ## HCL DX 9.5 CF184 - **HCL DX 9.5 Container Update CF184** If deploying HCL DX 9.5 CF184 release, the package name and images are as follows: **hcl-dx-kubernetes-v95-CF184.zip files**: - ``` HCL DX notices V9.5 CF184.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF184_20200917-0054.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF184_20200917-0054.zip - ``` hcl-dx-content-composer-image-v1.3.0_20200916-1952.tar.gz - ``` hcl-dx-core-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF184_20200916-2034.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-image-processor-image-v1.3.0_20200916-1953.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200917_1600304449.tar.gz - ``` hcl-dx-postgres-image-v1.3.0_20200916-2003.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz - ``` hcl-dx-remote-search-image-v95_CF184_20200916-2009.tar.gz - ``` hcl-dx-ringapi-image-v1.3.0_20200916-1953.tar.gz ## HCL DX 9.5 CF183 - **HCL DX 9.5 Container Update CF183** If deploying HCL DX 9.5 CF183 release, the image and package names are as follows: **CF183-core.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-ambassador-image-154.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF183_20200818-1852.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF183_20200818-1852.zip - ``` hcl-dx-core-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-redis-image-5.0.1.tar.gz **CF183-other.zip files**: - ``` HCL DX notices V9.5 CF183.txt - ``` hcl-dx-content-composer-image-v1.2.0_20200818-1343.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF183_20200818-1344.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.2.0_20200818-1346.tar.gz - ``` hcl-dx-image-processor-image-v1.2.0_20200818-1345.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200818_1597758965.tar.gz - ``` hcl-dx-postgres-image-v1.2.0_20200818-1349.tar.gz - ``` hcl-dx-remote-search-image-v95_CF183_20200818-1342.tar.gz - ``` hcl-dx-ringapi-image-v1.2.0_20200818-1351.tar.gz","title":"HCL DX 9.5 CF193"},{"location":"docker/docker/#hcl-dx-95-cf182","text":"CF182 If deploying HCL DX 9.5 CF182 release, the image and package names are as follows: hcl-dx-kubernetes-v95-CF182-core.zip : ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-ambassador-image-0850.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF182_20200720-1708.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF182_20200720-1708.zip - ``` hcl-dx-core-image-v95_CF182_20200720-1645.tar.gz **hcl-dx-kubernetes-v95-CF182-other.zip**: - ``` HCL DX notices V9.5 CF182.txt - ``` hcl-dx-content-composer-image-v1.1.0_20200720-1708.tar.gz - ``` hcl-dx-digital-asset-management-operator-image-v95_CF182_20200720-1716.tar.gz - ``` hcl-dx-digital-asset-manager-image-v1.1.0_20200720-1712.tar.gz - ``` hcl-dx-image-processor-image-v1.1.0_20200720-1716.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-master_20200720_1595265588.tar.gz - ``` hcl-dx-postgres-image-v1.1.0_20200720-1715.tar.gz - ``` hcl-dx-remote-search-image-v95_CF182_20200720-1645.tar.gz - ``` hcl-dx-ringapi-image-v1.1.0_20200720-1707.tar.gz","title":"HCL DX 9.5 CF182"},{"location":"docker/docker/#hcl-dx-95-cf181","text":"CF181 If deploying HCL DX 9.5 CF181 and later container update release, the image and package names add HCL DX 9.5 Content Composer and Digital Asset Management components and supporting services images, as well as Remote Search and OpenLDAP images. See the following list of images presented within the HCL DX 9.5 Container Update CF181 package: hcl-dx-kubernetes-v95-CF181-core.zip : ``` hcl-dx-cloud-scripts-v95_CF181_xxxxxxxx-xxxx.zip - ``` hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-ambassador-image-xxxx.tar.gz - ``` HCL DX notices V9.5 CF181.txt **hcl-dx-kubernetes-v95-CF181-other.zip**: - ``` hcl-dx-ringapi-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-postgres-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-openldap-image-v1.0.0-release_xxxxxxxx_xxxxxxxxxx.tar.gz - ``` hcl-dx-medialibrary-operator-image-v95_CF181_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-image-processor-image-v1.0.0_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz ## HCL DX 9.5 CF18 - **CF18** If deploying HCL DX 9.5 CF18 and later container update release, the image and package names included for this are: - ``` hcl-dx-kubernetes-v95-CF18.zip - ``` hcl-dx-core-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-operator-image-v95_CF18_xxxxxxxx-xxxx.tar.gz - ``` hcl-dx-cloud-scripts-v95_CF18_xxxxxxxx-xxxx.zip - ``` hcl-dx-ambassador-image-xxxx.tar.gz See the following sections for additional information: Docker image deployment Customizing the container deployment Requirements and limitations Parent topic: Digital Experience on containerized platforms","title":"HCL DX 9.5 CF181"},{"location":"docker/docker_compose/","text":"Docker image deployment using Docker Compose This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"docker/docker_compose/#docker-image-deployment-using-docker-compose","text":"This section presents availability of a new option to deploy HCL Digital Experience 9.5 Docker images for non-production using Docker Compose . This approach streamlines deployment and configuration of HCL DX 9.5 components. For more information about Docker Compose, see the Docker Compose documentation . Pre-requisite: Download the Docker images for the HCL DX 9.5 Container Update version you wish to deploy. From your HCL Digital Experience entitlements in the HCL Software License Portal , the Docker images are in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Find the HCL DX 9.5 Docker Compose scripts, installation and configuration instructions for non-production use in the repositories on the HCL Software Github . Use the HCL-DX tag to find the DX Docker Compose entry. Video: Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use","title":"Docker image deployment using Docker Compose"},{"location":"docker/docker_image_deployment/","text":"Docker image deployment This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"docker/docker_image_deployment/#docker-image-deployment","text":"This section describes the steps in deploying HCL Digital Experience 9.5 containers using Docker. Follow these steps to deploy the HCL Digital Experience 9.5 and later CF container update releases in Docker. Download the Docker image from your HCL Digital Experience entitlements in the HCL Software License Portal , in the HCL_Portal_8.5_9.0_9.5_CFs download package entry. Refer to the Docker image list for the latest HCL DX 9.5 container update releases. Open a terminal window and change to the root directory of the extracted package. Load the container into your Docker repository: docker load < hcl-dx-core-image-v95-xxxxxxxx-xxxx.tar.gz Run the HCL DX Docker container using either of the following commands: ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 hcl/dx/core:v95_xxxxxxxx-xxxx - ``` docker run -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/ wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx In HCL DX 9.5 CF171, Administrators can use this command to run the container if credentials have been updated: - ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx The additional syntax adds the ability for users to pass updated credentials for the HCL Portal Administrators. - ``` -e WAS_ADMIN=wasadmin - ``` -e WAS_PASSWORD=wasadminpwd - ``` -e DX_ADMIN=dxadmin - ``` -e DX_PASSWORD=dxadminpwd ``` **Notes:** - Make sure the ~/dx-store/wp\\_profile directory is created before you start the Docker container. This is required for persistence \\(for using `-v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/ core:v95_xxxxxxxx-xxxx`\\). - To use the HCL DX Configuration Wizard, start the Java virtual machine \\(JVM\\) within the running container with the following command: ``` docker exec <CONTAINER ID> /opt/HCL/AppServer/profiles/cw_profile/bin/startServer.sh server1 ``` - For HCL DX 9.5 CF171 and later, access the Configuration Wizard at https://localhost:10202/hcl/wizard. **Note:** For HCL DX 9.5 release earlier than CF171, access the Configuration Wizard at https://localhost:10202/ibm/wizard. - Upgrading an existing HCL DX 9.5 Docker container, using a persisted volume, to HCL DX 9.5 CF171 or HCL DX 9.5 CF172 may require launching the upgraded container twice. For example, if the following command fails with an error, re-running the command allows a successful upgrade and launch the container: ``` docker run -e WAS_ADMIN=wasadmin -e WAS_PASSWORD=wasadminpwd -e DX_ADMIN=dxadmin -e DX_PASSWORD=dxadminpwd -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -p 10200:10200 -p 10202:10202 -v ~/dx-store/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_xxxxxxxx-xxxx ``` This issue is fixed in HCL DX 9.5 CF173. See the following sections for additional information: - [How to upload HCL Digital Experience 9.5 CF container images to a private repository](https://youtu.be/XJONRdpgCuo) - [Docker image list](docker_image_deployment.md) - [Customizing the container deployment](customizing_container_deployment.md) - [Containerization Limitations/Requirements](limitations_requirements.md)","title":"Docker image deployment"},{"location":"docker/docker_overview/","text":"Docker images for HCL Digital Experience 9.5 HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section. Overview Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process. Installation, Deployment and Migration Guidance Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance Parent topic: DX on Docker","title":"Docker images for HCL Digital Experience 9.5"},{"location":"docker/docker_overview/#docker-images-for-hcl-digital-experience-95","text":"HCL Digital Experience 9.5 supports deployments on Docker and popular Kubernetes platforms. Learn more about the latest list of container images and supported deployment platforms. HCL Digital Experience 9.5 core and related component images are provided in your HCL Digital Experience entitlements in the HCL Software Licensing Portal . For the latest list of container images and supported deployment platforms please consult the Docker containers Deployment topic pages in this section.","title":"Docker images for HCL Digital Experience 9.5"},{"location":"docker/docker_overview/#overview","text":"Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. Containerization is the use of Linux containers to deploy applications. While the use of containers to deploy applications is not new, containers are favored due to the ease of deploying applications like the latest version of HCL Digital Experience. The HCL Digital Experience containers are launched by running a runtime instance of an image. An image is an executable package that includes everything needed to run the HCL Digital Experience 9.5 application, including the code, a runtime, libraries, environment variables, and configuration files. Because it runs a discrete process, it does not take any more memory other than the executable image with state or user process.","title":"Overview"},{"location":"docker/docker_overview/#installation-deployment-and-migration-guidance","text":"Proceed to the Deployment topic page and follow the installation steps outlined in the Docker or supported Kubernetes platform of choice. Documentation resource: Deployment To migrate an existing on-premises platform Digital Experience deployment to a supported Kubernetes platform, access the Staging topic page in this section. Documentation resource: Staging Once you have completed a Digital Experience 9.5 Container deployment, to update the DX 9.5 container images to the latest Container Update releases, follow steps outlined in the Container Maintenance Help Center topic in this section. Documentation resource: Maintenance Parent topic: DX on Docker","title":"Installation, Deployment and Migration Guidance"},{"location":"docker/docker_remote_search/","text":"Configure Remote Search in Docker This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers. Introduction To support search services when deployed to Docker container platforms in Kubernetes, administrators should configure Remote Search services. This requires a different setup and configuration steps than those used to set up Remote Search on a non-Docker container platform . Some differences in the non-Docker container platform procedures are outlined below: The serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Since Docker dynamically allocates the host names for the containers, the /etc/hosts file doesn\u2019t have static entries for the HCL Digital Experience 9.5 container-based server nor the Remote Search server. The WebSphere Application Server ND Administration console will not have correct host names for the HCL Digital Experience container. Deploying Remote Search in HCL Digital Experience 9.5 Docker containers Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and higher packages will include a core software and Remote search container. Load both of these images into your Docker repository via the \u201cdocker load\u201d command. Note that if your organization has deployed these containers to a corporate Docker repository, you might use \u201cdocker pull\u201d instead to put it into your local repository. In this example, two Docker images and names are referred to. If a higher HCL Digital Experience 9.5 Container Update release is used, for example, HCL Digital Experience 9.5 CF181, the image names may vary. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz EJBs and host names HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force Docker to write the /etc/hosts file at container initialization time is needed. This is done via the Docker switch add-host . The situation is further complicated (at least on Linux) by the fact that containers on the default bridge network of Docker cannot DNS name resolve other containers on the same subnet. Therefore, one uses the Docker host as a proxy and starts both containers with the following: docker run \u2013add-host=dockerhost:172.19.0.1 \u2026 This has the effect of adding an entry in the /etc/hosts file on the HCL Digital Experience 9.5 Container core like this: 172.19.0.1 dockerhost Those familiar with Docker deployment practices will recognize 171.19.0.1 as the IP bridge address of the host machine that starts the Docker containers. Since all Docker containers have unique ports and the Docker host machine is not allowed to use these unique ports, one can refer to a port on any container as dockerhost:port_number . Launch the HCL Digital Experience 9.5 Core and Remote Search containers To deploy, following is the complete docker run command for both the HCL Digital Experience 9.5 Core and Remote Search containers. In these examples, wpsadmin / wpsadmin are used as the HCL Digital Experience and WebSphere Application Server admin user ID and password credentials. docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 9043:9043 -p 9403:9403 -p 2809:2809 -p 9060:9060 -v /home/dxengineer/Documents/prs_profile:/opt/HCL/AppServer/profiles/prs_profile hcl/dx/dxrs:v95_CF181_20200622-1550 docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -v /home/dxengineer/Documents/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_CF181_20200622-1550 The persisted profile for each container is located at /home/dxengineer/Documents/. The HCL Digital Experience admin username and password are passed as environment variables. Defining serverindex.xml on the Remote Search server On the HCL DX 9.5 container Remote search server, the serverindex.xml file is located at: /opt/HCL/AppServer/profiles/prs_profile/config/cells/{cell name}/nodes/{node name} Note that immediately after the Remote Search server is started (and since the profile is persisted on a persisted sub-directory), this file can be found at: {persisted volume for remote search profile}/prs_profile/config/cells/{cell name}/nodes/{node name} The serverindex.xml file contains 5 lines: host=\u201d{some hostname}\u201d where {some hostname} might likely be \"localhost\" or some host name like \u201c33b7e5004319\u201d . However, remote search will not work correctly until this host name field is replaced with a host name exactly like the host name in the \u201ciiop\u201d url in the search service is coded. So, for example, if your Portal search service has coded the \"iiop\" URL as \u201ciiop://some.host.com:2809\u201d , the host in serverindex.hml needs to be host=\u201dsome.host.com\u201d . It could be that your \"iiop\" url has the host name \"dockerhost\" if your iiop url lets your docker host route that URL. In that case, change these 5 lines to the following: host=\u201ddockerhost\u201d (apply the string \u201cdockerhost\u201d) Upon completion, when the HCL Digital Experience 9.5 Container core server communicates to the container Remote Search server over \"iiop\", the Remote Search server will return \u201cdockerhost\u201d as the host name of the Remote Search server. The HCL Digital Experience 9.5 Container has a configuration that will append the port to the host name that was just returned. Digital Experience instructions for Remote Search configuration The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic should also be completed in a Docker-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX Docker deployment. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use dockerhost (as the host) and the appropriate port for the SSL access. In the examples, the Remote Search server is on dockerhost:9443 and the DX host is on dockerhost:10042. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Docker container deployment. See the section on Creating a new search service for more information. Item Value IIOP_URL iiop://dockerhost:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Portal Search Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&amp;Action=GetDocuments&amp;Range=100&amp;locale=en-US Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Use the following parameters to create a JCR search collection . Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Action=GetDocuments&amp;Format=ATOM&amp;Locale=en_US&amp;Range=100&amp;Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&amp;Start=0&amp;SeedlistId=1@OOTB_CRAWLER1 Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Configure WCM Authoring Portlet search function. Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dockerhost:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Parent topic: Customizing your container deployment","title":"Configure Remote Search in Docker"},{"location":"docker/docker_remote_search/#configure-remote-search-in-docker","text":"This section shows how to configure Remote Search for your HCL Digital Experience 9.5 Docker containers.","title":"Configure Remote Search in Docker"},{"location":"docker/docker_remote_search/#introduction","text":"To support search services when deployed to Docker container platforms in Kubernetes, administrators should configure Remote Search services. This requires a different setup and configuration steps than those used to set up Remote Search on a non-Docker container platform . Some differences in the non-Docker container platform procedures are outlined below: The serverindex.xml file on the Remote Search server when deployed to on-premises environments may have a host name that is not accurate in a container environment with respect to the actual host name of the server hosting the Remote Search server. Since Docker dynamically allocates the host names for the containers, the /etc/hosts file doesn\u2019t have static entries for the HCL Digital Experience 9.5 container-based server nor the Remote Search server. The WebSphere Application Server ND Administration console will not have correct host names for the HCL Digital Experience container.","title":"Introduction"},{"location":"docker/docker_remote_search/#deploying-remote-search-in-hcl-digital-experience-95-docker-containers","text":"Prerequisite : Download the HCL Digital Experience 9.5 Docker containers from your HCL Digital Experience entitlements from the HCL Software License Portal . The HCL DX 9.5 container update CF181 and higher packages will include a core software and Remote search container. Load both of these images into your Docker repository via the \u201cdocker load\u201d command. Note that if your organization has deployed these containers to a corporate Docker repository, you might use \u201cdocker pull\u201d instead to put it into your local repository. In this example, two Docker images and names are referred to. If a higher HCL Digital Experience 9.5 Container Update release is used, for example, HCL Digital Experience 9.5 CF181, the image names may vary. hcl-dx-core-image-v95_CF181_xxxxxxxx-xxxx.tar.gz hcl-dx-dxrs-image-v95_CF181_xxxxxxxx-xxxx.tar.gz","title":"Deploying Remote Search in HCL Digital Experience 9.5 Docker containers"},{"location":"docker/docker_remote_search/#ejbs-and-host-names","text":"HCL Digital Experience 9.5 Container core and Portal Remote Search each use WebSphere Application Server as a base. As these components are on different hosts (containers), they need to communicate via IP. The initial conversation between HCL Digital Experience 9.5 core and the Remote Search server takes place over IIOP (rmi) which is the internet protocol of EJBs. Ideally, the /etc/hosts file of both containers would have the host name of the other. In other words, the /etc/hosts file of the HCL Digital Experience Container core would have a host reference for the Remote Search and vice versa. However, three factors make this impossible. The containers are based on Red Hat UBI, the /etc/hosts file is owned by root , and the root password (and sudo ) is not available. Apply the command below to define host references for the Remote Search service from the Digital Experience Container core. Therefore, a way to force Docker to write the /etc/hosts file at container initialization time is needed. This is done via the Docker switch add-host . The situation is further complicated (at least on Linux) by the fact that containers on the default bridge network of Docker cannot DNS name resolve other containers on the same subnet. Therefore, one uses the Docker host as a proxy and starts both containers with the following: docker run \u2013add-host=dockerhost:172.19.0.1 \u2026 This has the effect of adding an entry in the /etc/hosts file on the HCL Digital Experience 9.5 Container core like this: 172.19.0.1 dockerhost Those familiar with Docker deployment practices will recognize 171.19.0.1 as the IP bridge address of the host machine that starts the Docker containers. Since all Docker containers have unique ports and the Docker host machine is not allowed to use these unique ports, one can refer to a port on any container as dockerhost:port_number .","title":"EJBs and host names"},{"location":"docker/docker_remote_search/#launch-the-hcl-digital-experience-95-core-and-remote-search-containers","text":"To deploy, following is the complete docker run command for both the HCL Digital Experience 9.5 Core and Remote Search containers. In these examples, wpsadmin / wpsadmin are used as the HCL Digital Experience and WebSphere Application Server admin user ID and password credentials. docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 9043:9043 -p 9403:9403 -p 2809:2809 -p 9060:9060 -v /home/dxengineer/Documents/prs_profile:/opt/HCL/AppServer/profiles/prs_profile hcl/dx/dxrs:v95_CF181_20200622-1550 docker run --add-host=dockerhost:172.19.0.1 -d -e WAS_ADMIN=wpsadmin -e WAS_PASSWORD=wpsadmin -e DX_ADMIN=wpsadmin -e DX_PASSWORD=wpsadmin -p 10038:10038 -p 10039:10039 -p 10041:10041 -p 10042:10042 -v /home/dxengineer/Documents/wp_profile:/opt/HCL/wp_profile hcl/dx/core:v95_CF181_20200622-1550 The persisted profile for each container is located at /home/dxengineer/Documents/. The HCL Digital Experience admin username and password are passed as environment variables.","title":"Launch the HCL Digital Experience 9.5 Core and Remote Search containers"},{"location":"docker/docker_remote_search/#defining-serverindexxml-on-the-remote-search-server","text":"On the HCL DX 9.5 container Remote search server, the serverindex.xml file is located at: /opt/HCL/AppServer/profiles/prs_profile/config/cells/{cell name}/nodes/{node name} Note that immediately after the Remote Search server is started (and since the profile is persisted on a persisted sub-directory), this file can be found at: {persisted volume for remote search profile}/prs_profile/config/cells/{cell name}/nodes/{node name} The serverindex.xml file contains 5 lines: host=\u201d{some hostname}\u201d where {some hostname} might likely be \"localhost\" or some host name like \u201c33b7e5004319\u201d . However, remote search will not work correctly until this host name field is replaced with a host name exactly like the host name in the \u201ciiop\u201d url in the search service is coded. So, for example, if your Portal search service has coded the \"iiop\" URL as \u201ciiop://some.host.com:2809\u201d , the host in serverindex.hml needs to be host=\u201dsome.host.com\u201d . It could be that your \"iiop\" url has the host name \"dockerhost\" if your iiop url lets your docker host route that URL. In that case, change these 5 lines to the following: host=\u201ddockerhost\u201d (apply the string \u201cdockerhost\u201d) Upon completion, when the HCL Digital Experience 9.5 Container core server communicates to the container Remote Search server over \"iiop\", the Remote Search server will return \u201cdockerhost\u201d as the host name of the Remote Search server. The HCL Digital Experience 9.5 Container has a configuration that will append the port to the host name that was just returned.","title":"Defining serverindex.xml on the Remote Search server"},{"location":"docker/docker_remote_search/#digital-experience-instructions-for-remote-search-configuration","text":"The following guidance aligns with the Remote Search services configuration instructions available in the Remote Search services topic for deployment to non-container HCL Digital Experience servers. All of the instructions contained in the Remote Search services topic should also be completed in a Docker-based HCL Digital Experience deployment. The following guidance outlines specific settings that were used in the Remote Search service DX Docker deployment. Create a single sign-on (SSO) domain between HCL Digital Experience 9.5 container and the Remote Search service container by following the non-container on-premises procedure for Creating a single sign-on domain between HCL Portal and the Remote Search service . This entails exchanging SSL certificates and LTPA domain tokens. Note: When retrieving the SSL certificates from the host server, use dockerhost (as the host) and the appropriate port for the SSL access. In the examples, the Remote Search server is on dockerhost:9443 and the DX host is on dockerhost:10042. You must also complete Setting the search user ID and Removing search collections before creating a new search service. Create a new search service and use the following values for a Remote Search services configuration to a Docker container deployment. See the section on Creating a new search service for more information. Item Value IIOP_URL iiop://dockerhost:2809 PSE TYPE Select ejb from the pull down. EJB ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome DefaultCollectionsDirectory Leave empty. Search service implementation Select Portal Search Service Type from the pull down. CONFIG_FOLDER_PATH Did not set (differs from non-container instructions). Note: Once completed and saved, the HCL Digital Experience 9.5 container deployment has a new search service called Remote PSE service EJB , with a green check mark confirming that the service was correctly set up and is able to communicate with the Remote Search container. Based on the previously created Remote Search service, create a Portal Search Collection and a JCR Search Collection using the following parameters. Use the following parameters to create a Portal search collection . Parameter Value Search collection name Portal Search Collection Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/PortalSearchCollection Portal Search Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Source=com.ibm.lotus.search.plugins.seedlist.retriever.portal.PortalRetrieverFactory&amp;Action=GetDocuments&amp;Range=100&amp;locale=en-US Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Use the following parameters to create a JCR search collection . Parameter Value Search collection name JCRCollection1 Search collection location /opt/HCL/AppServer/profiles/prs_profile/SearchCollections/JCRCollection1 JCR Content Source Configuration. Use the following URL for Collect documents linked from this URL : https://dockerhost:10042/wps/seedlist/myserver?Action=GetDocuments&amp;Format=ATOM&amp;Locale=en_US&amp;Range=100&amp;Source=com.ibm.lotus.search.plugins.seedlist.retriever.jcr.JCRRetrieverFactory&amp;Start=0&amp;SeedlistId=1@OOTB_CRAWLER1 Note: The host and port are the Docker host and port to which 10042 is mapped. In this case, 10042 is the HttpQueueInboundDefaultSecure port on the HCL Digital Experience server. Note that you can also put this URL in a browser (on the Docker host) and confirm that the response is an ATOM feed. On the Security panel, use dockerhost as the host name, along with the username wpsadmin and the associated password for wpsadmin . Configure WCM Authoring Portlet search function. Note: Even though the documents are gathered by the Remote Search function from the JCR, additional configuration is needed in order for the HCL Web Content Manager (WCM) Authoring Portlet search to use document search. Set the following values for this configuration. Set the Custom properties for the WebSphere Application Server Resource Environment Provider, JCR ConfigService , using the following values: Property Value jcr.textsearch.enabled true jcr.textsearch.indexdirectory /opt/HCL/AppServer/profiles/prs_profile/SearchCollections jcr.textsearch.PSE.type ejb jcr.textsearch.EJB.IIOP.URL iiop://dockerhost:2809 jcr.textsearch.EJB.EJBName ejb/com/ibm/hrl/portlets/WsPse/WebScannerLiteEJBHome Note: On the jcr.textsearch.indexdirectory , the sub-directory JCRCollection1 is NOT included in the path. Parent topic: Customizing your container deployment","title":"Digital Experience instructions for Remote Search configuration"},{"location":"overview/intr_new95/","text":"What's new in HCL Digital Experience 9.5? Learn about the new features, updates, and fixes in the HCL Digital Experience 9.5 release. New HCL Digital Experience 9.5 Docker Image Using the new HCL Digital Experience 9.5 Docker image deployment option, HCL Digital Experience developers and administrators can launch development and production DX container instances on Docker, also Kubernetes operating on the OpenShift and with DX 9.5 CF171 and higher release container update, Amazon EKS supported platforms. Learn more about HCL Digital Experience 9.5 Docker image deployment by clicking on the topic links below. Documentation resource: HCL Digital Experience 9.5 Docker image deployment Video demonstration: HCL Digital Experience 9.5 on OpenShift New HCL Digital Experience 9.5 Practitioner Studio Using the new HCL Portal Practitioner Studio centralized management user interfaces, content authors, developers, and administrators can manage DX tasks more quickly. Learn more about Practitioner Studio by clicking on the topic link below. Documentation resource: Practitioner Studio The Woodburn Studio The Woodburn Studio is a fully functioning demonstration website that demonstrates best practices in content and site development with HCL Digital Experience tools. Learn more about Woodburn Studio by clicking on the topic link below. Documentation resource: The Woodburn Studio Migrate to WebSphere Application Server 9.0.5 HCL Digital Experience 9.5 supports IBM WebSphere Application Server 9.0.5 and you can migrate to it if you are running on IBM WebSphere Application Server 8.5.x or 9.0.0.x. Documentation resource: Migrating to WebSphere Application Server 9.0.5 The New HCL Experience API HCL Experience API is a set of OpenAPI-compliant REST APIs developers can use to the existing set of HCL Digital Experience APIs. Documentation resource: The HCL Experience API The New Content Composer Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. Documentation resource: Content Composer The New Digital Asset Management Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. Documentation resource: Digital Asset Management How to provide feedback on the HCL Digital Experience release components HCL Digital Experience customers and business partners may provide feedback on their experience with the Tech Preview release versions of the Experience API and the Content Composer by using this HCL Digital Experience release feedback reporting application . Unsupported features for HCL Digital Experience 9.5 If you are migrating from an earlier release of HCL Digital Experience, you should be aware of various features and themes that are unsupported per each release. Deprecated features for HCL Digital Experience 9.5 If you are migrating from an earlier release of HCL Digital Experience, you should be aware of various features and themes that are deprecated per each release. List of software fixes 9.5 This includes new features and improvements for the latest version of HCL Digital Experience. Fixes for IBM WebSphere Application Server 9.0.5 Learn about the latest fixes for IBM WebSphere Application Server 9.0.5. Parent topic: What's new in HCL Digital Experience?","title":"What's new in HCL Digital Experience 9.5?"},{"location":"overview/intr_new95/#whats-new-in-hcl-digital-experience-95","text":"Learn about the new features, updates, and fixes in the HCL Digital Experience 9.5 release.","title":"What's new in HCL Digital Experience 9.5?"},{"location":"overview/intr_new95/#new-hcl-digital-experience-95-docker-image","text":"Using the new HCL Digital Experience 9.5 Docker image deployment option, HCL Digital Experience developers and administrators can launch development and production DX container instances on Docker, also Kubernetes operating on the OpenShift and with DX 9.5 CF171 and higher release container update, Amazon EKS supported platforms. Learn more about HCL Digital Experience 9.5 Docker image deployment by clicking on the topic links below. Documentation resource: HCL Digital Experience 9.5 Docker image deployment Video demonstration: HCL Digital Experience 9.5 on OpenShift","title":"New HCL Digital Experience 9.5 Docker Image"},{"location":"overview/intr_new95/#new-hcl-digital-experience-95-practitioner-studio","text":"Using the new HCL Portal Practitioner Studio centralized management user interfaces, content authors, developers, and administrators can manage DX tasks more quickly. Learn more about Practitioner Studio by clicking on the topic link below. Documentation resource: Practitioner Studio","title":"New HCL Digital Experience 9.5 Practitioner Studio"},{"location":"overview/intr_new95/#the-woodburn-studio","text":"The Woodburn Studio is a fully functioning demonstration website that demonstrates best practices in content and site development with HCL Digital Experience tools. Learn more about Woodburn Studio by clicking on the topic link below. Documentation resource: The Woodburn Studio","title":"The Woodburn Studio"},{"location":"overview/intr_new95/#migrate-to-websphere-application-server-905","text":"HCL Digital Experience 9.5 supports IBM WebSphere Application Server 9.0.5 and you can migrate to it if you are running on IBM WebSphere Application Server 8.5.x or 9.0.0.x. Documentation resource: Migrating to WebSphere Application Server 9.0.5","title":"Migrate to WebSphere Application Server 9.0.5"},{"location":"overview/intr_new95/#the-new-hcl-experience-api","text":"HCL Experience API is a set of OpenAPI-compliant REST APIs developers can use to the existing set of HCL Digital Experience APIs. Documentation resource: The HCL Experience API","title":"The New HCL Experience API"},{"location":"overview/intr_new95/#the-new-content-composer","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. Documentation resource: Content Composer","title":"The New Content Composer"},{"location":"overview/intr_new95/#the-new-digital-asset-management","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. Documentation resource: Digital Asset Management","title":"The New Digital Asset Management"},{"location":"overview/intr_new95/#how-to-provide-feedback-on-the-hcl-digital-experience-release-components","text":"HCL Digital Experience customers and business partners may provide feedback on their experience with the Tech Preview release versions of the Experience API and the Content Composer by using this HCL Digital Experience release feedback reporting application . Unsupported features for HCL Digital Experience 9.5 If you are migrating from an earlier release of HCL Digital Experience, you should be aware of various features and themes that are unsupported per each release. Deprecated features for HCL Digital Experience 9.5 If you are migrating from an earlier release of HCL Digital Experience, you should be aware of various features and themes that are deprecated per each release. List of software fixes 9.5 This includes new features and improvements for the latest version of HCL Digital Experience. Fixes for IBM WebSphere Application Server 9.0.5 Learn about the latest fixes for IBM WebSphere Application Server 9.0.5. Parent topic: What's new in HCL Digital Experience?","title":"How to provide feedback on the HCL Digital Experience release components"},{"location":"overview/intr_ovr/","text":"Product capabilities HCL Digital Experience capabilities let you quickly implement web experiences that are engaging, flexible, and high performing. Digital Experience platform Digital Experience is HCL integration software platform. It includes the entire middleware infrastructure - such as servers, services, and tools - needed to write, run, and monitor 24x7 industrial-strength, on-demand Web applications and cross-platform, cross-product solutions. WebSphere provides reliable, flexible, and robust integration software. Digital Experience provides software for Service Oriented Architecture (SOA) environments that enables dynamic, interconnected business processes, and delivers highly effective application infrastructures for all business situations. IBM\u00ae WebSphere\u00ae Application Server drives business agility by providing millions of developers and IT Architects with an innovative, performance-based foundation to build, reuse, run, integrate, and manage Service Oriented Architecture (SOA) applications and services. From business critical and key enterprise-wide applications to the smallest departmental level applications, WebSphere Application Server offers the highest levels of reliability, availability, security, and scalability. Customization You can customize the portal to meet the needs of your organization, users, and user groups. You can adapt the look and feel of the portal to fit the standards of your organization and to customize page content for users and groups in accordance with business rules and user profiles. Users, such as business partners, customers, or employees, can further customize their own views of the portal. Users can add portlets to pages and arrange them as they want and control portlet color schemes. By aggregating portlets in one place and giving users the power to customize their own desktops, the portal gives users a means for doing business efficiently and with high satisfaction. Portlets Portlets are central to the portal. As special reusable Java servlet that appear as defined regions on portal pages, portlets provide access to many different applications, services, and web content. The portal ships a rich set of standard portlets, including portlets for displaying syndicated content, transforming XML, and accessing search engines and web pages. More portlet solutions are available on the IBM Lotus and HCL Portal Business Solutions Catalog. These portlets are used to access HCL Notes\u00ae iNotes, HCL Connections, and Microsoft Exchange. Several third-party portlets are also available. Examples include Enterprise Resource Planning (ERP), Dashboards, Business Intelligence, Process Management, and Customer Relationship Management (CRM) portlets. It is possible to develop custom portlets, too. The portal supports the Java Standard API (JSR 286) that portlet developers can use to create custom portlets. Widgets Widgets are now included inside the portal. You can even create mash-ups that consist of both portlets and widgets. Widgets are highly interactive user interface components that are written in JavaScript. These widgets are typically very narrow in scope and can easily be created using a script-based language. Widgets can also be a solution for creating a mashup between different backend technologies like a Java EE-based portal server and a PHP-based server. For additional information about new features, main components, and what each component provides to the overall solution, explore the What's New section. Streamlined site creation You can use the Site Builder to generate your own portal site. Web analytics HCL Digital Experience (DX) includes a number of solutions to help you understand how visitors use your site, including server-side analytics and client-side analytics. Client-side analytics is also called active site analytics. Web content HCL Portal and HCL Web Content Manager help you manage content, share information, and communicate your message. Social business HCL Portal offers wikis, blogs, and tagging and rating capabilities. In addition, you can integrate existing collaboration applications with your portal site, such as HCL Connections. Integration HCL Portal integrates with many products. Mobile The HCL Digital Experience platform presents mobile-enabled services support that enables you to deliver consistent, personalized cross-channel experiences quickly. Versatile framework HCL Digital Experience provides users a consistent view of portal applications and allows users to define specific sets of applications that are presented in a single context. Depending on the requesting device, the rendering of this application set must vary to fulfill the requirements of the device. Parent topic: Overview 9.5","title":"Product capabilities"},{"location":"overview/intr_ovr/#product-capabilities","text":"HCL Digital Experience capabilities let you quickly implement web experiences that are engaging, flexible, and high performing.","title":"Product capabilities"},{"location":"overview/intr_ovr/#digital-experience-platform","text":"Digital Experience is HCL integration software platform. It includes the entire middleware infrastructure - such as servers, services, and tools - needed to write, run, and monitor 24x7 industrial-strength, on-demand Web applications and cross-platform, cross-product solutions. WebSphere provides reliable, flexible, and robust integration software. Digital Experience provides software for Service Oriented Architecture (SOA) environments that enables dynamic, interconnected business processes, and delivers highly effective application infrastructures for all business situations. IBM\u00ae WebSphere\u00ae Application Server drives business agility by providing millions of developers and IT Architects with an innovative, performance-based foundation to build, reuse, run, integrate, and manage Service Oriented Architecture (SOA) applications and services. From business critical and key enterprise-wide applications to the smallest departmental level applications, WebSphere Application Server offers the highest levels of reliability, availability, security, and scalability.","title":"Digital Experience platform"},{"location":"overview/intr_ovr/#customization","text":"You can customize the portal to meet the needs of your organization, users, and user groups. You can adapt the look and feel of the portal to fit the standards of your organization and to customize page content for users and groups in accordance with business rules and user profiles. Users, such as business partners, customers, or employees, can further customize their own views of the portal. Users can add portlets to pages and arrange them as they want and control portlet color schemes. By aggregating portlets in one place and giving users the power to customize their own desktops, the portal gives users a means for doing business efficiently and with high satisfaction.","title":"Customization"},{"location":"overview/intr_ovr/#portlets","text":"Portlets are central to the portal. As special reusable Java servlet that appear as defined regions on portal pages, portlets provide access to many different applications, services, and web content. The portal ships a rich set of standard portlets, including portlets for displaying syndicated content, transforming XML, and accessing search engines and web pages. More portlet solutions are available on the IBM Lotus and HCL Portal Business Solutions Catalog. These portlets are used to access HCL Notes\u00ae iNotes, HCL Connections, and Microsoft Exchange. Several third-party portlets are also available. Examples include Enterprise Resource Planning (ERP), Dashboards, Business Intelligence, Process Management, and Customer Relationship Management (CRM) portlets. It is possible to develop custom portlets, too. The portal supports the Java Standard API (JSR 286) that portlet developers can use to create custom portlets.","title":"Portlets"},{"location":"overview/intr_ovr/#widgets","text":"Widgets are now included inside the portal. You can even create mash-ups that consist of both portlets and widgets. Widgets are highly interactive user interface components that are written in JavaScript. These widgets are typically very narrow in scope and can easily be created using a script-based language. Widgets can also be a solution for creating a mashup between different backend technologies like a Java EE-based portal server and a PHP-based server. For additional information about new features, main components, and what each component provides to the overall solution, explore the What's New section. Streamlined site creation You can use the Site Builder to generate your own portal site. Web analytics HCL Digital Experience (DX) includes a number of solutions to help you understand how visitors use your site, including server-side analytics and client-side analytics. Client-side analytics is also called active site analytics. Web content HCL Portal and HCL Web Content Manager help you manage content, share information, and communicate your message. Social business HCL Portal offers wikis, blogs, and tagging and rating capabilities. In addition, you can integrate existing collaboration applications with your portal site, such as HCL Connections. Integration HCL Portal integrates with many products. Mobile The HCL Digital Experience platform presents mobile-enabled services support that enables you to deliver consistent, personalized cross-channel experiences quickly. Versatile framework HCL Digital Experience provides users a consistent view of portal applications and allows users to define specific sets of applications that are presented in a single context. Depending on the requesting device, the rendering of this application set must vary to fulfill the requirements of the device. Parent topic: Overview 9.5","title":"Widgets"},{"location":"whatsnew/container_update_releases/","text":"Container Update releases 9.5 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Beginning with Container Update release CF196, release updates for both on-premises deployments and container platforms are available. What's new in Container Update CF201? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. What's new in Container Update CF200? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more. What's new in Container Update CF199? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. What's new in Container Update CF198? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. What's new in Container Update CF197? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. What's new in Container Update CF196? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. What's new in Container Update CF195? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. What's new in Container Update CF194? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. What's new in Container Update CF193? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. What's new in Container Update CF192? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. What's new in Container Update CF191? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. What's new in Container Update CF19? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF184? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF183? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF182? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. What's new in Container Update CF181? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. What's new in CF18? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. What's new in Container Update CF173? 9.5 Containers This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. What's new in Container Update CF172? 9.5 Containers The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. What's new in Container Update CF171? 9.5 Containers The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Parent topic: Latest Combined CF and 9.5 Container Updates","title":"Cumulative Fixes for DX Containers"},{"location":"whatsnew/container_update_releases/#container-update-releases-95","text":"The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Beginning with Container Update release CF196, release updates for both on-premises deployments and container platforms are available. What's new in Container Update CF201? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. What's new in Container Update CF200? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more. What's new in Container Update CF199? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. What's new in Container Update CF198? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. What's new in Container Update CF197? 9.5 Containers This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. What's new in Container Update CF196? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. What's new in Container Update CF195? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. What's new in Container Update CF194? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. What's new in Container Update CF193? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. What's new in Container Update CF192? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. What's new in Container Update CF191? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. What's new in Container Update CF19? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF184? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF183? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. What's new in Container Update CF182? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. What's new in Container Update CF181? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. What's new in CF18? 9.5 Containers This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. What's new in Container Update CF173? 9.5 Containers This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. What's new in Container Update CF172? 9.5 Containers The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. What's new in Container Update CF171? 9.5 Containers The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Parent topic: Latest Combined CF and 9.5 Container Updates","title":"Container Update releases 9.5"},{"location":"whatsnew/new_cf17/","text":"What's new in CF17? 9.5 Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and 9.5 Container Updates","title":"What's new in CF17? 9.5"},{"location":"whatsnew/new_cf17/#whats-new-in-cf17-95","text":"Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Go to the HCL Software Support Site for the list of software fixes for HCL Digital Experience. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Also, see the following link for Portal maintenance guidance: HCL Digital Experience Roadmap: Applying maintenance Parent topic: Latest Combined CF and 9.5 Container Updates","title":"What's new in CF17? 9.5"},{"location":"whatsnew/new_cf171/","text":"What's new in Container Update CF171? The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments. Password override in Docker Added option to override password in Docker. See Docker deployment . Password override in OpenShift Added option to override password in OpenShift. See OpenShift deployment . Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS) Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS . Support for Auto-scaling and Route configuration Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment . Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF171?"},{"location":"whatsnew/new_cf171/#whats-new-in-container-update-cf171","text":"The Container Update releases include new features and updates for HCL Digital Experience 9.5 container deployments.","title":"What's new in Container Update CF171?"},{"location":"whatsnew/new_cf171/#password-override-in-docker","text":"Added option to override password in Docker. See Docker deployment .","title":"Password override in Docker"},{"location":"whatsnew/new_cf171/#password-override-in-openshift","text":"Added option to override password in OpenShift. See OpenShift deployment .","title":"Password override in OpenShift"},{"location":"whatsnew/new_cf171/#support-for-kubernetes-as-verified-in-amazon-elastic-container-service-for-kubernetes-eks","text":"Added support for Kubernetes on AWS EKS. See Deploy HCL Digital Experience 9.5 Container to Amazon EKS .","title":"Support for Kubernetes as verified in Amazon Elastic Container Service for Kubernetes (EKS)"},{"location":"whatsnew/new_cf171/#support-for-auto-scaling-and-route-configuration","text":"Added support for auto-scaling based on available CPU and memory utilization and route configuration. See Customizing the container deployment .","title":"Support for Auto-scaling and Route configuration"},{"location":"whatsnew/new_cf171/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases 9.5","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/new_cf172/","text":"What's new in Container Update CF172? The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates. Web Content Manager (WCM) Support Tools The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details. New Web Content Query Parameter APIs New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details. New Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Updated HCL Digital Experience 9.5 platform support statements Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details. Downloading DX products and accessing Customer Support You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF172?"},{"location":"whatsnew/new_cf172/#whats-new-in-container-update-cf172","text":"The Container Update release include new features and updates for HCL Digital Experience 9.5 container deployments, including DX tool, Support and API updates.","title":"What's new in Container Update CF172?"},{"location":"whatsnew/new_cf172/#web-content-manager-wcm-support-tools","text":"The HCL WCM Support Tools portlet is provided to investigate and analyze information related to the WCM Java Content Repository (JCR) nodes. The WCM Support Tools Portlet is provided in the HCL Digital Experience 9.5 CF172 Container Update release, and is accessible from the standard Digital Experience administration panel in the CF172 release. See HCL Web Content Manager Support Tools for details.","title":"Web Content Manager (WCM) Support Tools"},{"location":"whatsnew/new_cf172/#new-web-content-query-parameter-apis","text":"New Web Content Query Parameter APIs are added in HCL Digital Experience 9.5 CF172. See REST Query service for web content for details.","title":"New Web Content Query Parameter APIs"},{"location":"whatsnew/new_cf172/#new-enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"New Enhanced Content Template API"},{"location":"whatsnew/new_cf172/#updated-hcl-digital-experience-95-platform-support-statements","text":"Read the updates to HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. See This HCL Software Support article for details.","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"whatsnew/new_cf172/#downloading-dx-products-and-accessing-customer-support","text":"You can go to the HCL Software Licensing Portal to access and download product software. See the Step-by-step guide to downloading DX products and accessing Customer Support for more information. Parent topic: Container Update releases 9.5","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/new_cf173/","text":"What's new in Container Update CF173? This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more. Web Content Manager Mirror syndication - Disable full build option An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items . New WCM Restore Version REST API The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions . New Enhanced WCM Content Template API Element Configuration The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST . New WCM Export Digital Asset Management references API The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items . New Experience API samples Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls . New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details. New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173 Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details. Downloading DX products and accessing Customer Support The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF173?"},{"location":"whatsnew/new_cf173/#whats-new-in-container-update-cf173","text":"This HCL Digital Experience 9.5 Container Update release include new WCM REST APIs, and Tech Previews for Content Composer and Digital Asset Management, and more.","title":"What's new in Container Update CF173?"},{"location":"whatsnew/new_cf173/#web-content-manager-mirror-syndication-disable-full-build-option","text":"An option to disable the rebuild with the WCM mirror syndication option is now available. This option can be set using the WCM Configuration service on the syndicator. See Manually syndicating items .","title":"Web Content Manager Mirror syndication - Disable full build option"},{"location":"whatsnew/new_cf173/#new-wcm-restore-version-rest-api","text":"The Restore version API supports restoring content versions to a previous level. See How to use REST with Versions .","title":"New WCM Restore Version REST API"},{"location":"whatsnew/new_cf173/#new-enhanced-wcm-content-template-api-element-configuration","text":"The Enhanced Content Template API Element Configuration Updates allows the configuration of template elements to be updated. See How to set default content values for content templates by using REST .","title":"New Enhanced WCM Content Template API Element Configuration"},{"location":"whatsnew/new_cf173/#new-wcm-export-digital-asset-management-references-api","text":"The Web Content Manager Export DAM references API REST service can be used to retrieve content or components with references to externally managed resources, using the Digital Asset Manager plugin. See How to use REST with content items .","title":"New WCM Export Digital Asset Management references API"},{"location":"whatsnew/new_cf173/#new-experience-api-samples","text":"Two new samples are provided for use with the HCL Digital Experience 9.5 Experience API, supporting Sample login and content update process flow, and Get roles with authentication functions. See the Experience API Sample Calls .","title":"New Experience API samples"},{"location":"whatsnew/new_cf173/#new-hcl-content-composer-tech-preview-in-hcl-digital-experience-95-cf173","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Content Composer \u2013 Tech Preview for details.","title":"New HCL Content Composer \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"whatsnew/new_cf173/#new-digital-asset-management-tech-preview-in-hcl-digital-experience-95-cf173","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 CF173 Digital Asset Management \u2013 Tech Preview for details.","title":"New Digital Asset Management \u2013 Tech Preview in HCL Digital Experience 9.5 CF173"},{"location":"whatsnew/new_cf173/#downloading-dx-products-and-accessing-customer-support","text":"The HCL Digital Experience 9.5 detailed system support statements are updated and published on the HCL Digital Experience Support site. You can go to the HCL Software Licensing Portal to access and download product software. For more information, see the Step-by-step guide to downloading DX products and accessing Customer Support . Parent topic: Container Update releases 9.5","title":"Downloading DX products and accessing Customer Support"},{"location":"whatsnew/new_cf18/","text":"What's new in CF18? This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. JavaServer Faces (JSF) Bridge With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information. Apply Content Template REST API The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details. Enhanced Content Template API The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details. Rich Text Editor Textbox I/O Updates Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details. Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details. HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details. Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details. Updated HCL Digital Experience 9.5 platform support statements See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases 9.5","title":"What's new in CF18?"},{"location":"whatsnew/new_cf18/#whats-new-in-cf18","text":"This HCL Digital Experience 9.5 Container Update release includes new WCM REST APIs, updated Tech Preview releases of Content Composer and Digital Asset Management, Rich Text Editor and Java Server Faces bridge updates, and more. Go to the HCL Software Support Site for the list of software fixes, including CF18. See What's New in CF18 for HCL Digital Experience for a list of updates in this release. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in CF18?"},{"location":"whatsnew/new_cf18/#javaserver-faces-jsf-bridge","text":"With the HCL Digital Experience CF18 Container and CF update, an updated JSF Portlet Bridge is introduced and installed in the HCL Digital Experience software. Customers installing HCL Digital Experience 8.5, 9, or 9.5 CF18 or Container Update release CF18, and using an IBM WebSphere Application Server Network Deployment version later than 8.5.5.17 or 9.0.5.2 can utilize the new JSF Portlet Bridge. See JavaServer Faces implementation for more information.","title":"JavaServer Faces (JSF) Bridge"},{"location":"whatsnew/new_cf18/#apply-content-template-rest-api","text":"The Apply Content Template API allows a developer to apply a content template to a set of specific content items or all content items of a certain template. It is equivalent to the functionality in the user interface. See How to set default content values for content templates by using REST for details.","title":"Apply Content Template REST API"},{"location":"whatsnew/new_cf18/#enhanced-content-template-api","text":"The Enhanced Web Content Manager Content Template API adds the ability to retrieve the configuration details of content template elements. See How to retrieve the settings of the elements for content templates by using REST for details.","title":"Enhanced Content Template API"},{"location":"whatsnew/new_cf18/#rich-text-editor-textbox-io-updates","text":"Updates are provided for the Textbox I/O Rich Text Editor, requiring use of Java 8. See Rich text editor toolbar configuration options for details.","title":"Rich Text Editor Textbox I/O Updates"},{"location":"whatsnew/new_cf18/#sample-guidance-to-set-storage-class-and-volume-to-deploy-hcl-digital-experience-95-containers-to-amazon-elastic-kubernetes-service-eks-and-red-hat-openshift","text":"Additional \"Sample Storage Class and Volume\" guidance is available to HCL Digital Experience 9.5 container administrators. See Sample Storage Class and Volume topic for details.","title":"Sample Guidance to set Storage Class and Volume to deploy HCL Digital Experience 9.5 Containers to Amazon Elastic Kubernetes Service (EKS) and Red Hat OpenShift"},{"location":"whatsnew/new_cf18/#hcl-content-composer-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can access a Tech Preview of the Content Composer features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Content Composer \u2013 Tech Preview for details.","title":"HCL Content Composer \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"whatsnew/new_cf18/#digital-asset-management-tech-preview-for-hcl-digital-experience-95-cf173-or-later","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access a Tech Preview of the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF173 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management \u2013 Tech Preview for details.","title":"Digital Asset Management \u2013 Tech Preview for HCL Digital Experience 9.5 CF173 or later"},{"location":"whatsnew/new_cf18/#updated-hcl-digital-experience-95-platform-support-statements","text":"See the updates to the HCL Digital Experience 9.5 detailed system support statements published to the HCL Digital Experience Support site. Parent topic: Container Update releases 9.5","title":"Updated HCL Digital Experience 9.5 platform support statements"},{"location":"whatsnew/new_cf181/","text":"What's new in Container Update CF181? This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Content Composer Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details. Digital Asset Management Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details. Experience API The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details. OpenLDAP Container integration OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details. Transfer default Container database to IBM DB2 HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details. Remote Search services Docker deployment To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details. New Digital Experience WCM Workflow REST APIs Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details. New Web Content Manager Reference REST API The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details. New Web Content Text Search REST API The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details. New Digital Experience Core Configuration REST API The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details. Web Developer Toolkit The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF181?"},{"location":"whatsnew/new_cf181/#whats-new-in-container-update-cf181","text":"This HCL Digital Experience 9.5 Container Update release includes new Production releases of Content Composer, Digital Asset Management, and Experience API, new WCM REST APIs, guidance to deploy OpenLDAP, Remote Search and Database transfer processes, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in Container Update CF181?"},{"location":"whatsnew/new_cf181/#content-composer","text":"Content Composer delivers simplified processes for creating and managing Digital Experience site content. Users can work with Content Composer features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Content Composer for details.","title":"Content Composer"},{"location":"whatsnew/new_cf181/#digital-asset-management","text":"Digital Asset Management delivers a central platform to store and include rich media assets in Digital Experience site content to present engaging, consistently branded experiences across digital channels. Users can access the Digital Asset Management features in HCL Digital Experience 9.5 Container Update CF181 and higher releases. See HCL Digital Experience 9.5 Digital Asset Management for details.","title":"Digital Asset Management"},{"location":"whatsnew/new_cf181/#experience-api","text":"The HCL Experience API is a set of OpenAPI compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. The HCL Experience API includes REST APIs that serve as a wrapper around previously published HCL Digital Experience HTTP based APIs. See HCL Experience API for details.","title":"Experience API"},{"location":"whatsnew/new_cf181/#openldap-container-integration","text":"OpenLDAP Software is an open source implementation of the Lightweight Directory Access Protocol. The HCL Digital Experience 9.5 Container Update release CF181 and higher includes an OpenLDAP container and a customization of the operator to deploy and configure the LDAP container to the HCL Digital Experience 9.5 container deployment. See Configure the OpenLDAP container image to the HCL Digital Experience 9.5 Container Deployment for details.","title":"OpenLDAP Container integration"},{"location":"whatsnew/new_cf181/#transfer-default-container-database-to-ibm-db2","text":"HCL Digital Experience 9.5 installs a copy of Derby as the default database. HCL Digital Experience administrators can apply guidance to transfer the default database configuration to IBM DB2, if preferred for use as the relational database for Digital Experience 9.5 Container deployment data. See Transfer Digital Experience 9.5 Container default database to IBM DB2 for details.","title":"Transfer default Container database to IBM DB2"},{"location":"whatsnew/new_cf181/#remote-search-services-docker-deployment","text":"To support search services when deployed to Docker, Digital Experience administrators can configure Remote search services. This will require some different setup and configuration steps than used to set up remote search on a non-Docker container platform. See Deploy Remote Search services on Docker for details.","title":"Remote Search services Docker deployment"},{"location":"whatsnew/new_cf181/#new-digital-experience-wcm-workflow-rest-apis","text":"Two new WCM REST APIs are added to handle Process Now and Remove Workflow from an item functionality. See How to use REST with drafts and workflows for details.","title":"New Digital Experience WCM Workflow REST APIs"},{"location":"whatsnew/new_cf181/#new-web-content-manager-reference-rest-api","text":"The new WCM Content Manager Reference REST API can be used by developers to find references to a Web Content or Digital Asset Management item identified by its UUID. See How to use REST with content items for details.","title":"New Web Content Manager Reference REST API"},{"location":"whatsnew/new_cf181/#new-web-content-text-search-rest-api","text":"The Text Search REST API Content Authors search for free form text in the Web Content Manager JCR. It is equivalent to the functionality in the Web Content Manager user interface. See REST Query service for web content - Query parameters for details.","title":"New Web Content Text Search REST API"},{"location":"whatsnew/new_cf181/#new-digital-experience-core-configuration-rest-api","text":"The Digital Experience Core Configuration API allows developers to retrieve Digital Experience deployment configuration settings. See Generic reading by using REST services for Web Content Manager for details.","title":"New Digital Experience Core Configuration REST API"},{"location":"whatsnew/new_cf181/#web-developer-toolkit","text":"The Web Developer toolkit for HCL Digital Experience provides the ability to sync themes, content and script portlets (also known as Script Applications). It is available on the HCL Digital Experience GitHub repository. See Web Developer Toolkit for details. Parent topic: Container Update releases 9.5","title":"Web Developer Toolkit"},{"location":"whatsnew/new_cf_95/","text":"Latest Combined CF and 9.5 Container Updates Learn what's new in the HCL Digital Experience Combined Cumulative Fix and HCL Digital Experience Version 9.5 Container Update releases. What's new in CF201? Combined Cumulative Fix (CF201) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF200? Combined Cumulative Fix (CF200) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF199? Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF198? Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF197? Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. What's new in CF196? Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. What's new in CF19? Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. What's new in CF18? 9.5 This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. What's new in CF17? 9.5 Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Container Update releases 9.5 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Beginning with Container Update release CF196, release updates for both on-premises deployments and container platforms are available. Parent topic: What's new in HCL Digital Experience?","title":"Latest Combined CF and 9.5 Container Updates"},{"location":"whatsnew/new_cf_95/#latest-combined-cf-and-95-container-updates","text":"Learn what's new in the HCL Digital Experience Combined Cumulative Fix and HCL Digital Experience Version 9.5 Container Update releases. What's new in CF201? Combined Cumulative Fix (CF201) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF200? Combined Cumulative Fix (CF200) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF199? Combined Cumulative Fix (CF199) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF198? Combined Cumulative Fix (CF198) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on-premises platforms and container deployments are available. What's new in CF197? Combined Cumulative Fix (CF197) includes new features and software fixes for the latest version of HCL Digital Experience. Release updates for both on \u2013 premises platforms and container deployments are available. What's new in CF196? Combined Cumulative Fix (CF196) includes new features and software fixes for the latest version of HCL Digital Experience. Beginning with CF19 and Container Update release CF196, release updates for both on\u2013premises platforms and container deployments will be available. What's new in CF19? Combined Cumulative Fix (CF19) includes new features and software fixes for the latest version of HCL Digital Experience. What's new in CF18? 9.5 This HCL Digital Experience 9.5 CF18 release includes new WCM REST APIs, updated releases of Content Composer and Digital Asset Management Tech Preview releases, Rich Text Editor and JavaServer Faces bridge updates, and more. What's new in CF17? 9.5 Combined Cumulative Fix (CF17) includes new software fixes for the latest version of HCL Digital Experience. Container Update releases 9.5 The HCL Digital Experience 9.5 Cumulative Fix and Container Update releases offer new features and software fixes for the HCL Digital Experience 9.5 container releases. Beginning with Container Update release CF196, release updates for both on-premises deployments and container platforms are available. Parent topic: What's new in HCL Digital Experience?","title":"Latest Combined CF and 9.5 Container Updates"},{"location":"whatsnew/newcf182/","text":"What's new in Container Update CF182? This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. Deploy HCL DX 9.5 Container updates with minimal operations downtime This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details. Optional Digital Asset Management Storage Configuration Settings This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF182?"},{"location":"whatsnew/newcf182/#whats-new-in-container-update-cf182","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. This update adds new guidance to minimize downtime when updating container-based deployments with new artifacts, configuration options for Digital Asset Management storage, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information.","title":"What's new in Container Update CF182?"},{"location":"whatsnew/newcf182/#deploy-hcl-dx-95-container-updates-with-minimal-operations-downtime","text":"This topic provides guidance to update artifacts in HCL Digital Experience 9.5 container deployments while minimizing operations downtime, and notes how processes and tools to support these efforts differ across Kubernetes container-based and non-Kubernetes HCL Digital Experience platform deployments. See Deploying HCL DX 9.5 Container artifact updates with minimal operations downtime for details.","title":"Deploy HCL DX 9.5 Container updates with minimal operations downtime"},{"location":"whatsnew/newcf182/#optional-digital-asset-management-storage-configuration-settings","text":"This topic outlines optional configuration steps to tune Digital Asset Management storage services Storage Class and Volume. See Optional Digital Asset Management Storage Configuration Settings for details. Parent topic: Container Update releases 9.5","title":"Optional Digital Asset Management Storage Configuration Settings"},{"location":"whatsnew/newcf183/","text":"What's new in Container Update CF183? This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS) Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information. Web Content Manager Lock/Unlock API The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information. Content Template Create/Update Option Element Selection API The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information. Search Component Results Display examples A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF183?"},{"location":"whatsnew/newcf183/#whats-new-in-container-update-cf183","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs and Search Query examples, guidance to deploy the Remote Search image on Red Hat OpenShift, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF183. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF183?"},{"location":"whatsnew/newcf183/#deploy-hcl-dx-95-container-cf182-or-higher-to-microsoft-azure-kubernetes-service-aks","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF182 and higher container releases along with Ambassador to Kubernetes, as verified in Microsoft Azure Kubernetes Service (AKS). See the HCL Digital Experience 9.5 Deployment and Deploy HCL Digital Experience 9.5 Container to Microsoft Azure Kubernetes Service (AKS) topics for more information.","title":"Deploy HCL DX 9.5 Container CF182 or higher to Microsoft Azure Kubernetes Service (AKS)"},{"location":"whatsnew/newcf183/#web-content-manager-lockunlock-api","text":"The Web Content Manager Lock/Unlock API lets you lock and unlock WCM content components, authoring templates, and item. It can also extend the WCM Query API. See the Web Content Manager Lock/Unlock AP I topic for more information.","title":"Web Content Manager Lock/Unlock API"},{"location":"whatsnew/newcf183/#content-template-createupdate-option-element-selection-api","text":"The Web Content Manager Create/Update Option Element Selection API lets you create or update an Option Selection Element in a Content Template. See the Create or update an Option Selection Element topic for more information.","title":"Content Template Create/Update Option Element Selection API"},{"location":"whatsnew/newcf183/#search-component-results-display-examples","text":"A search element defines the layout of a form that is used to display search results. See the Search Component Results Display topic for examples of how to design your search results.","title":"Search Component Results Display examples"},{"location":"whatsnew/newcf183/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Remote Search Service Transfer HCL Digital Experience 9.5 Container default database to IBM DB2 Create a web content library Virtual portals Backup and restore Configuration Wizard Combined Cumulative Fix Install Rich text editor toolbar configuration options Data collection and symptom analysis Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf184/","text":"What's new in Container Update CF184? This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Syndication REST APIs The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information. Access the HCL Experience API in HCL DX GitHub The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF184?"},{"location":"whatsnew/newcf184/#whats-new-in-container-update-cf184","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Microsoft Azure Kubernetes Service (AKS), new Web Content Manager REST APIs supporting Syndication options, HCL Digital Experience API published to HCL DX GitHub, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF184?"},{"location":"whatsnew/newcf184/#web-content-manager-syndication-rest-apis","text":"The Web Content Manager Syndication REST APIs let you control syndication processes. See the Web Content Manager Syndication REST APIs topic for more information.","title":"Web Content Manager Syndication REST APIs"},{"location":"whatsnew/newcf184/#access-the-hcl-experience-api-in-hcl-dx-github","text":"The HCL Experience API is a set of OpenAPI-compliant REST APIs available for customers deploying HCL Digital Experience 9.5 containers on supported Kubernetes platforms. It supports the integration and management of HCL Digital Experience content and functionality to any digital channel using any front-end development framework. Developers may also now access this API published to the HCL DX GitHub repository. See the Experience API topic for more information.","title":"Access the HCL Experience API in HCL DX GitHub"},{"location":"whatsnew/newcf184/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: HCL Digital Experience Combined Cumulative Fix (CF) Installation Deploy HCL Digital Experience 9.5 to Red Hat OpenShift Deploy Digital Asset Management, Content Composer and Experience API to Red Hat OpenShift Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf19/","text":"What's new in Container Update CF19? This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information. Hybrid Deployment The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information. Progressive Web Application support Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information. Google Analytics integration Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information. Mobile Preview Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information. DXClient and DXConnect tooling supporting CICD release processes HCL Digital Experience CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Digital Asset Management and Kaltura Integration Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF19?"},{"location":"whatsnew/newcf19/#whats-new-in-container-update-cf19","text":"This HCL Digital Experience 9.5 Container Update release includes updated production releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New additions include additional guidance to deploy to Google Kubernetes Engine (GKE), Hybrid deployment support, Progressive Web Application delivery, Google Analytics integration, Mobile Preview, DXClient and DXConnect tooling supporting CICD release processes, Kaltura video support, new Web Content Manager REST APIs, new HCL Digital Experience \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal. Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF19?"},{"location":"whatsnew/newcf19/#deploy-hcl-digital-experience-95-container-to-google-kubernetes-engine-gke","text":"Learn how to deploy HCL Digital Experience (DX) 9.5 CF19 and higher container release along with Ambassador to Kubernetes, as verified in Google Kubernetes Engine (GKE) . See the Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE) topic for more information.","title":"Deploy HCL Digital Experience 9.5 Container to Google Kubernetes Engine (GKE)"},{"location":"whatsnew/newcf19/#hybrid-deployment","text":"The HCL Digital Experience 9.5 Hybrid deployment and topics deliver capability to deploy and manage HCL Digital Experience 9.5 core Portal Server and Web Content Manager services on premises, and connect to cloud native components Digital Asset Management, Content Composer, Experience API and related services in a production environment. See the Hybrid Deployment topic for more information.","title":"Hybrid Deployment"},{"location":"whatsnew/newcf19/#progressive-web-application-support","text":"Develop support that adds native mobile application experience and performance to your web site using browser-based functionality. See the Progressive Web Application topic for more information.","title":"Progressive Web Application support"},{"location":"whatsnew/newcf19/#google-analytics-integration","text":"Learn how to set up integration of Digital Experience sites with Google Analytics and view the resulting web analytics tracking to assess the effectiveness of your DX site pages with end user audiences. See the Integrate Google Analytics with HCL Digital Experience topic for more information.","title":"Google Analytics integration"},{"location":"whatsnew/newcf19/#mobile-preview","text":"Use the Mobile Preview simulator to view the presentation of Digital Experience site page components on select mobile devices. See the Mobile Preview topic for more information.","title":"Mobile Preview"},{"location":"whatsnew/newcf19/#dxclient-and-dxconnect-tooling-supporting-cicd-release-processes","text":"HCL Digital Experience CF19 and higher includes a DXClient toolset, and DX Connect servlet that provides developers and administrators with an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. Note: The DXClient tool is not supported for use with HCL DX 9.5 deployments in Red Hat OpenShift or supported Kubernetes platforms. Use of the DXClient tool with those platforms will be available in future HCL DX 9.5 update releases. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"DXClient and DXConnect tooling supporting CICD release processes"},{"location":"whatsnew/newcf19/#digital-asset-management-and-kaltura-integration","text":"Learn how to configure Kaltura Video Content Management System integration to accelerate HCL Digital Asset Management rich media integration to HCL Digital Experience site pages and content. See the Configure DAM - Kaltura integration topic for more information.","title":"Digital Asset Management and Kaltura Integration"},{"location":"whatsnew/newcf19/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF19 release: Web content image renditions interactions REST API Web Content Library Locale Query Workflow Comments API Using XML Access to export and import Digital Asset Management assets","title":"New Digital Experience REST APIs"},{"location":"whatsnew/newcf19/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step by step guidance for HCL Digital Experience practitioners presented in several new videos. See the following HCL Digital Experience Help Center topics: Configure the OpenLDAP container image to the HCL DX 9.5 Container Deployment How to manage syndicators and subscribers Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf191/","text":"What's new in Container Update CF191? This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Deploy HCL Digital Experience 9.5 on HCL Solution Factory The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d HCL Digital Experience 9.5 Integration with HCL Commerce HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF191?"},{"location":"whatsnew/newcf191/#whats-new-in-container-update-cf191","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, that may be used for new deployments only. Upgrading from a previous DX 9.5 Container Release to CF191 is not supported. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF191?"},{"location":"whatsnew/newcf191/#deploy-hcl-digital-experience-95-on-hcl-solution-factory","text":"The HCL Solution Factory platform offers the ability for organizations to quickly prototype and test assets and can enable organizations to speed cloud-native adoption. Visit HCL Solution Factory to access HCL Digital Experience 9.5 and other HCL software offerings to quickly assess and test cloud-native strategies to see what best fits adoption plans. View this online tutorial \u201c Deploy HCL Digital Experience in Minutes with HCL SoFy \u201d","title":"Deploy HCL Digital Experience 9.5 on HCL Solution Factory"},{"location":"whatsnew/newcf191/#hcl-digital-experience-95-integration-with-hcl-commerce","text":"HCL Commerce integration with HCL Digital Experience allows content and digital assets managed within HCL Digital Experience to be utilized in any Commerce store. Visit the HCL Commerce Help Center Digital Experience integration for more information and pre-requisites. Parent topic: Container Update releases 9.5","title":"HCL Digital Experience 9.5 Integration with HCL Commerce"},{"location":"whatsnew/newcf192/","text":"What's new in Container Update CF192? This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. HCL Digital Experience 9.5 Docker and Container Initialization Performance Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information. HCL Digital Experience 9.5 Container Core Transaction Logging Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Content Composer Features New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information. HCL Digital Experience 9.5 Integration with HCL Unica Discover Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information. Content Security Policy The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information. New Digital Experience REST APIs New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF192?"},{"location":"whatsnew/newcf192/#whats-new-in-container-update-cf192","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF192?"},{"location":"whatsnew/newcf192/#hcl-digital-experience-95-docker-and-container-initialization-performance","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, DX 9.5 Docker and container initialization performance is improved. See the HCL Digital Experience 9.5 Docker and Container Initialization Performance Help Center topic for more information.","title":"HCL Digital Experience 9.5 Docker and Container Initialization Performance"},{"location":"whatsnew/newcf192/#hcl-digital-experience-95-container-core-transaction-logging","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF192 release, transaction logging for the DX Docker Core image is updated to improve performance. See the Logging and tracing for Containers and new services Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Core Transaction Logging"},{"location":"whatsnew/newcf192/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform,and automate processes in the development and delivery process. New release artifact types supporting Script Application Undeploy and Restore, and Deploy Theme. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf192/#new-content-composer-features","text":"New Content Composer features are added with HCL Digital Experience Container Update CF192, including a new Version Comparison interface and capabilities to View and Filter Workflow comments, and more. See the HCL Content Composer Help Center topic for additional information.","title":"New Content Composer Features"},{"location":"whatsnew/newcf192/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF192, including enhanced crop functionality, Kaltura video player support, thumbnail preview support, asset size filter, Renditions and Versioning support, and more. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"whatsnew/newcf192/#hcl-digital-experience-95-integration-with-hcl-unica-discover","text":"Integration of HCL Digital Experience 9.5 sites with HCL Unica Discover enables DX site managers and marketers to access deep insight analytics and session replay services to assess the effectiveness of DX site pages with end user audiences. See the Integrate HCL Unica Discover with HCL Digital Experience topic for more information.","title":"HCL Digital Experience 9.5 Integration with HCL Unica Discover"},{"location":"whatsnew/newcf192/#content-security-policy","text":"The Content-Security-Policy header is used by modern browsers to enhance the security of HCL Digital Experience site documents or web pages by allowing HCL Digital Experience administrators or developers declare which dynamic resources are allowed to load. With HCL Digital Experience Container Update CF192 and later releases, developers can apply platform support and guidance to update their DX sites to validate trusted sources before rendering pages to end users. See the Content Security Policy Help Center topic for more information.","title":"Content Security Policy"},{"location":"whatsnew/newcf192/#new-digital-experience-rest-apis","text":"New HCL DX APIs are introduced with the HCL DX CF192 Container Update release: Using the WCM Add Comment API Web Content Manager Find Rendering and Script Portlet References Web Content Manager Access Control Filter REST API Clear Theme Cache API Work with Deleted Web Content Items Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"whatsnew/newcf192/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: HCL Digital Experience 9.5 Container Deployment Using the dxctl tool to deploy Digital Experience 9.5 on Red Hat OpenShift Create a WAR-based theme copy on HCL Digital Experience 9.5 Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf193/","text":"What's new in Container Update CF193? This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Enable Presentation of Locales in Friendly URLs Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information. Theme Editor Portlet The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information. HCL Digital Experience 9.5 Container Custom Context Root URL Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information. New Digital Asset Management Features New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information. Change language presented in the HCL Digital Experience Theme Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Digital Experience REST APIs New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF193?"},{"location":"whatsnew/newcf193/#whats-new-in-container-update-cf193","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF193?"},{"location":"whatsnew/newcf193/#enable-presentation-of-locales-in-friendly-urls","text":"Beginning with the HCL Digital Experience 9.5 Container Update CF193 release, enablement is provided to present friendly URLs with locale specific definitions when multi-lingual page versions are requested. This enablement can also improve SEO results when users search for language-specific DX page topics. See the Enabling presentation of locales in friendly URLs topic for more information.","title":"Enable Presentation of Locales in Friendly URLs"},{"location":"whatsnew/newcf193/#theme-editor-portlet","text":"The Theme Editor portlet is a new addition to HCL Digital Experience Container Update CF193 and higher release capabilities. The portlet allows an administrator to edit static theme resources in WebDAV without the use of a WebDAV client or tool. See the Theme Editor Portlet topic for more information.","title":"Theme Editor Portlet"},{"location":"whatsnew/newcf193/#hcl-digital-experience-95-container-custom-context-root-url","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can define the custom context root URLs when deploying your DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience Portal URL when deployed to Container platforms topic for more information.","title":"HCL Digital Experience 9.5 Container Custom Context Root URL"},{"location":"whatsnew/newcf193/#new-digital-asset-management-features","text":"New Digital Asset Management Features are added with HCL Digital Experience Container Update CF193, and include the ability to filter Digital Assets by size. See the HCL Digital Asset Management Help Center topic for additional information.","title":"New Digital Asset Management Features"},{"location":"whatsnew/newcf193/#change-language-presented-in-the-hcl-digital-experience-theme","text":"Beginning with HCL DX 9.5 Container Update CF193 release, you can switch the language presented in your Digital Experience theme. An example is provided in the Woodburn Studio demo site supporting presentations for French, Spanish, and English languages. See the How to switch the languages in the Digital Experience theme topic for more information.","title":"Change language presented in the HCL Digital Experience Theme"},{"location":"whatsnew/newcf193/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Deploy DX Core, Manage Syndicator, and Manage Subscriber tasks are provided in Container Update CF193. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf193/#new-digital-experience-rest-apis","text":"New and updated HCL DX APIs are introduced with the HCL DX CF193 Container Update release: Web Content Manager Multilingual Solution APIs","title":"New Digital Experience REST APIs"},{"location":"whatsnew/newcf193/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/newcf193/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Deploy HCL Digital Experience in Minutes using HCL Solution Factory Using the dxctl tool to Update Digital Experience 9.5 on Red Hat OpenShift How to Upload HCL Digital Experience 9.5 CF Container Images to a Private Repository Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf194/","text":"What's new in Container Update CF194? This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF194?"},{"location":"whatsnew/newcf194/#whats-new-in-container-update-cf194","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. No new features were added in this update. Important note: The default IBM WebSphere Application Server certificate that ships with HCL Digital Experience 9.5 Docker images expires on April 26, 2021. Access to HCL Digital Experience 9.5 container deployments is not adversely affected. However, scripts executed against the DX 9.5 deployed servers, like stopServer or some ConfigEngine tasks, will fail. To address this, HCL Digital Experience 9.5 customers deploying to container platforms can use either of the following options to update the certificate: Apply the HCL Digital Experience 9.5 Container Update CF194, available from the HCL Software Licensing Portal on April 19, 2021. Renew the certificate on your DX 9.5 Container Deployment by following the steps outlined in the following HCL DX Support Knowledge Base article: Manual Steps to Apply the Digital Experience 9.5 Container Deployment Core Certificate Update . Go to the HCL Software Support Site for the list of software fixes, including Container Update release CF194. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF194?"},{"location":"whatsnew/newcf195/","text":"What's new in Container Update CF195? This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Web Content Manager Multilingual Solution Library Export and Import The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information. Web Content Manager Advanced Cache Options New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information. Enhanced Cross Origin Resource Sharing Configuration Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information. Define No Context Root in for HCL Digital Experience 9.5 container deployments Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF195?"},{"location":"whatsnew/newcf195/#whats-new-in-container-update-cf195","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF195?"},{"location":"whatsnew/newcf195/#web-content-manager-multilingual-solution-library-export-and-import","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF195 release, a new export and import capability allows you to support translation of the content of a library by exporting it into a format supported by a translation service and importing the translated content back into the content library using the DXClient tool. See the How to export and import WCM library content using DXClient topic for more information.","title":"Web Content Manager Multilingual Solution Library Export and Import"},{"location":"whatsnew/newcf195/#web-content-manager-advanced-cache-options","text":"New options are available to flush the Web Content Manager Advanced cache, as a result of syndication operations, to help improve performance and reliability. See the Web Content Manager Cache Parameters topic for more information.","title":"Web Content Manager Advanced Cache Options"},{"location":"whatsnew/newcf195/#enhanced-cross-origin-resource-sharing-configuration","text":"Enhanced Cross Origin Resource Sharing Configuration adds new options for HCL Digital Experience administrators to set configuration for CORS using a WP configuration service in the IBM WebSphere Application Server resource environment provider. See the Enhanced Cross Origin Resource Sharing Configuration for more information.","title":"Enhanced Cross Origin Resource Sharing Configuration"},{"location":"whatsnew/newcf195/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. New release artifact types supporting Undeploy Themes, and Export/Import Web Content Manager Library content are provided in Container Update CF195. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf195/#remote-search-configuration-for-hcl-digital-experience-95-deployments-on-kubernetes-platforms","text":"Beginning with HCL DX 9.5 Container Update CF195 release, Remote Search can be configured for deployment on supported Kubernetes platforms. See the Configure Remote Search in Red Hat OpenShift and Kubernetes topic for more information.","title":"Remote Search Configuration for HCL Digital Experience 9.5 deployments on Kubernetes platforms"},{"location":"whatsnew/newcf195/#define-no-context-root-in-for-hcl-digital-experience-95-container-deployments","text":"Beginning with HCL DX 9.5 Container Update CF195 release, administrators can define custom context root URLs, or no context root URL, when deploying HCL DX 9.5 software to the supported container platforms. See the Customizing the Digital Experience URL when deployed to Container platforms topic for more information.","title":"Define No Context Root in for HCL Digital Experience 9.5 container deployments"},{"location":"whatsnew/newcf195/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/newcf195/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Getting Started with DXClient on Red Hat OpenShift using HCL Digital Experience CF194 Understanding the Core Persistent Volumes in HCL Digital Experience Container Update CF194 Create a WebDAV Theme copy using HCL Digital Experience 9.5 Create and apply A/B personalized scenarios with HCL Digital Experience Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf196/","text":"What's new in Container Update CF196? This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information. Deploy HCL DX CF196 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information. Deploy HCL DX 9.5 using Docker Compose Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information. Web Content Manager Multilingual Solution Enhancements The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF196?"},{"location":"whatsnew/newcf196/#whats-new-in-container-update-cf196","text":"This HCL Digital Experience 9.5 Container Update release includes new releases of HCL Digital Experience 9.5 core Portal and Web Content Manager, Content Composer, Digital Asset Management, and Experience API images, and a beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Note: For new capabilities that are available for HCL DX on-premise deployments, see What's new in the CF196 topic.","title":"What's new in Container Update CF196?"},{"location":"whatsnew/newcf196/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196. It is not yet supported for use in production deployments . See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/newcf196/#deploy-hcl-dx-cf196-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations, and is available for use with the Google Kubernetes Engine (GKE) platform with Container Update CF196. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF196 to container platforms using Helm"},{"location":"whatsnew/newcf196/#deploy-hcl-dx-95-using-docker-compose","text":"Beginning with HCL DX 9.5 Container Update CF196, administrators and developers can deploy HCL DX 9.5 using Docker Compose, for non-production use. Docker Compose scripts for HCL DX 9.5, installation, and configuration instructions for non-production use are available in the HCL Software Github page. See the Docker image deployment using Docker Compose topic for more information.","title":"Deploy HCL DX 9.5 using Docker Compose"},{"location":"whatsnew/newcf196/#web-content-manager-multilingual-solution-enhancements","text":"The HCL Web Content Manager Multilingual Solution is a set of tools used to manage translated versions Web Content Manager content for localized and regionalized websites. Beginning with the HCL Digital Experience 9.5 Container Update CF196 release, support is added to import and export multiple libraries to a format supported by a translation service, support a maximum field length, export changed contents from a library, and export to projects. A new option to switch the language in an HCL DX 9.5 mobile view is also available. See the How to export and import WCM library content using DXClient and The Woodburn Studio demo site topics for more information.","title":"Web Content Manager Multilingual Solution Enhancements"},{"location":"whatsnew/newcf196/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include a new DXClient Docker image, and new release artifact types supporting shared libraries, obtain failed syndication reports are provided in Container Update CF196. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf196/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix Help Center topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/newcf196/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Configuring user access permissions to Digital Asset Management assets Deep dive: Progressive Web Applications with HCL DX 9.5 Content Security Policy with HCL DX 9.5 Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf197/","text":"What's new in Container Update CF197? This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF197 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF197?"},{"location":"whatsnew/newcf197/#whats-new-in-container-update-cf197","text":"This HCL Digital Experience 9.5 Container Update and CF197 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management and Experience API images, and an updated beta preview release of Design Studio. New and updated feature references are detailed here. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF197?"},{"location":"whatsnew/newcf197/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New services available with the Container Update CF197 release include ability to render DX site pages and updates using the sample site, Ability to use the page editor to edit elements inline and update metadata, set locations for sites, set html tags for text elements, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF197, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/newcf197/#deploy-hcl-dx-cf197-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators a larger degree of transparency and control in deployment operations. Support for new HCL DX 9.5 CF197 deployments to Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS),and ability to update from HCL DX 9.5 version CF196 to CF197 is supported with the Google Kubernetes Engine (GKE) platform. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF197 to container platforms using Helm"},{"location":"whatsnew/newcf197/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates include new release artifact types supporting, Obtain failed Syndication reports for single or multiple items, and delete Digital Asset Management inactive schema from Persistence are provided in CF197. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf197/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information.","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/newcf197/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in new videos and webinars. See the following HCL Digital Experience Help Center topics: Updating the HCL DX 9.5 Portal & IBM WebSphere Application Server Administrator Secrets in OpenShift and Kubernetes Use Docker Compose to install HCL DX 9.5 with Cloud Components for Developer Use Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf198/","text":"What's new in Container Update CF198? This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Deploy HCL DX CF198 to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New HCL Digital Experience Site Manager Custom Layout Editor Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information. New Experience APIs New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information Rationalized CF release versioning Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service HCL Digital Experience 9.5 Container Platform Support Matrix View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF198?"},{"location":"whatsnew/newcf198/#whats-new-in-container-update-cf198","text":"This HCL Digital Experience 9.5 Container Update and CF198 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, the new Site Manager Custom Layout Editor, HCL Digital Experience Technical Articles, \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF198?"},{"location":"whatsnew/newcf198/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF198 release include the ability to create new DX sites, reading and updating site metadata, accessing site and page UUID and URLs, and client-side logging services. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF198, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/newcf198/#deploy-hcl-dx-cf198-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Support for hybrid deployments is provided, enabling to update from HCL DX 9.5 CF197 to CF198 in the Red Hat OpenShift, Microsoft Azure Elastic Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS) platforms. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX CF198 to container platforms using Helm"},{"location":"whatsnew/newcf198/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and automate processes in the development and delivery process. Updates delivered in CF198 include process definitions to automate select IBM WebSphere Application Server settings during deployment, generate import and export lists of virtual portals, and import and export personalization rules. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf198/#new-hcl-digital-experience-site-manager-custom-layout-editor","text":"Beginning with HCL Digital Experience CF198, a Custom Layout Editor is available for use with HCL DX Site Manager capabilities, reducing custom development requirements for site designers implementing custom layouts in DX site pages. See the Using the Digital Experience Custom Layout Editor topic for more information.","title":"New HCL Digital Experience Site Manager Custom Layout Editor"},{"location":"whatsnew/newcf198/#new-experience-apis","text":"New HCL Experience Web Content Manager REST APIs are available for new menu component update, collection responses conversion utilities, the ability to use the page editor to edit elements inline and update metadata, set locations for sites, set HTML tags for text elements, and more. See the HCL DX Experience API topic for more information","title":"New Experience APIs"},{"location":"whatsnew/newcf198/#rationalized-cf-release-versioning","text":"Beginning with HCL Digital Experience CF196, single versioning is used for both container and on-premise Combined Cumulative Fixes (CFs). This means that fixes for both deployments are included into one CF deliverable. And although the versioning is the same, CFs are packaged separately for on-premises and container deployments. See the CF release versioning and update path topic for more information.","title":"Rationalized CF release versioning"},{"location":"whatsnew/newcf198/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Creating an HCL Digital Experience 9.5 CF196 cluster Step-by-step guide: How to deploy HCL DX Container update CF197 and later to Microsoft Azure Kubernetes Service","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf198/#hcl-digital-experience-95-container-platform-support-matrix","text":"View the latest Kubernetes and OpenShift platforms tested and supported for specific HCL Digital Experience 9.5 Container Update deployments. See the HCL Digital Experience 9.5 Container Platform Support Matrix topic for more information. Parent topic: Container Update releases 9.5","title":"HCL Digital Experience 9.5 Container Platform Support Matrix"},{"location":"whatsnew/newcf199/","text":"What's new in Container Update CF199? This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information. Migrate from HCL DX 9.5 Operator to Helm Deployments Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management Staging New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center. New Experience APIs New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information. New REST APIs to Configure Remote Search Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information. New HCL Digital Experience \u2018How To\u2019 Videos Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF199?"},{"location":"whatsnew/newcf199/#whats-new-in-container-update-cf199","text":"This HCL Digital Experience 9.5 Container Update and CF199 release includes new releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments, updated CICD release process artifacts, \u201cHow To\u201d videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF199?"},{"location":"whatsnew/newcf199/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with the Container Update CF199 release include the ability to select Web Content Manager library assets when creating sites, UI globalization, support for alternate and no context root when defining sites, and more. Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF196, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/newcf199/#migrate-from-hcl-dx-95-operator-to-helm-deployments","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF199, support for migration from Operator-based (dxctl) to Helm-based deployments is provided. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Migrate from HCL DX 9.5 Operator to Helm Deployments"},{"location":"whatsnew/newcf199/#digital-asset-management-staging","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging topic for more information.","title":"Digital Asset Management Staging"},{"location":"whatsnew/newcf199/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export and Import of select IBM WebSphere Application Server Resource Provider settings during deployment, and to create Syndication relationships and credential vault settings. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf199/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with Container update and CF200 release. Refer to the Deprecated features and themes for HCL Digital Experience 9.5 topic in the Help Center.","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"whatsnew/newcf199/#new-experience-apis","text":"New HCL Experience APIs are available for creating, updating and deleting Design Studio (Beta) sites, pages, and containers. New Web Content Manager REST V2 APIs are available for creating content templates, categories, and more. See the HCL Experience API topic for more information.","title":"New Experience APIs"},{"location":"whatsnew/newcf199/#new-rest-apis-to-configure-remote-search","text":"Beginning with HCL Digital Experience CF and Container Update CF199, additional REST services enable administrators and developers to programatically configure remote search in on-premises and container-based Digital Experience deployments. See the Configure Remote Search using REST APIs topic for more information.","title":"New REST APIs to Configure Remote Search"},{"location":"whatsnew/newcf199/#new-hcl-digital-experience-how-to-videos","text":"Take advantage of new step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos from the following HCL Digital Experience Help Center topics: Video: Deploy HCL DX 9.5 Container Update using Helm Video: Experience API V2 Web Content Manager REST APIs Parent topic: Container Update releases 9.5","title":"New HCL Digital Experience \u2018How To\u2019 Videos"},{"location":"whatsnew/newcf200/","text":"What's new in Container Update CF200? This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. Design Studio (Beta) Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF200 include Page creation shortcuts, Delete Site from Overview menu, Define Site base stylesheet, Rename Content container and more. Note: Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF200, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information. Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF200, new servies and updates include support for Sidecars for logging of Remote Search, define central logs location, Incubator section for future DX 9.5 Container Update features, Configuration of labels and annotations, also environment variables for different DX resources, Use of Persistent Volumes for DX 9.5 Core, Digital Asset Management, and Persistence services file storage, migration process for the Core profile from Operator to Helm deployment, and Helm based Version to Version Update process. Important: Beginning with HCL DX Container Update CF200, use of the Operator (dxctl) method of container deployment is not supported. Customers should use Helm deployments and migrate existing Operator-based deployments to Helm. See the HCL DX 9.5 Helm deployment topic for more information. Digital Asset Management New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging Help Center topic for more information. Support to migrate from the old to new Digital Asset Management database in the Helm-based deployments. See the Migrate to new DAM DB in Helm-based deployments Help Center topic for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export or Import of Web Content Manager libraries from source server to target server location, also ability to generate differential reports for DX Server configurations. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New User and Groups REST API Explorer The remote PUMA SPI gives you access to user profiles through REST services. It provides a remote interface for user and group management for the configured HCL DX user repository. Beginning with HCL DX 9.5 Container Update and CF 199, a new API explorer is available that allows developers using the Portal User Interface APIs to explore and test these APIs. See the Help Center topic Remote REST service for PUMA for additional information. New Personalization REST APIs and Explorer Beginning with HCL DX 9.5 Container Update and CF200 , a new API explorer is available that allows developers using the new Digital Experience Personalization REST APIs to explore and test these APIs. See the Digital Experience Personalization Help Center topic for more information. New Search REST APIs Explorer The Digital Experience Search REST API provides developers programmatic access to search indexed Digital Experience content and web pages. Beginning with Container and CF Update CF200, a new Digital Experience Search REST API Explorer allows developers to explore and test the Digital Experience Search REST APIs. See the HCL Digital Experience Search REST API Specification Help Center topic for more information. Access and Deploy HCL Digital Experience 9.5 on HCL Sofy HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. Using HCL SoFy to access and deploy HCL Digital Experience 9.5, and other HCL software offerings and demo packs, you can quickly gain hands-on experience working with these cloud-native solutions. See the Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) Help Center topic for more information. New How-To Video Take advantage of step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos included in HCL Digital Experience Help Center topics. New : Learn how to manage and monitor HCL DX 9.5 Container Deployment Liveliness and Readiness probes. See the Help Center topic: Operations using Helm . End of Support for HCL Digital Experience Deprecated Features The following list of HCL Digital Experience deprecated features will reach end of support beginning with HCL Digital Experience Container update and CF 200. Reference the Help Center topic Deprecated features and themes for HCL Digital Experience 9.5 . Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF200?"},{"location":"whatsnew/newcf200/#whats-new-in-container-update-cf200","text":"This HCL Digital Experience 9.5 Container Update and CF200 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, Design Studio (Beta), new Helm deployment operations for container deployments and migration from Operator deployments, updated CICD release process artifacts, new DX API and Explorer services for Personalization, Search and User and Groups REST APIs, new Digital Experience 9.5 Demo Packs on HCL Sofy, new \u2018How To\u2019 videos, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic.","title":"What's new in Container Update CF200?"},{"location":"whatsnew/newcf200/#design-studio-beta","text":"Design Studio enables content managers and designers to build and style their digital site properties quickly. Available for use with DX 9.5 container-based deployments, Design Studio presents a modern, intuitive, and role-based tool aggregating all needed functions to visually assemble, curate, design, and model pages, content, and applications in DX sites. New features available with Container Update CF200 include Page creation shortcuts, Delete Site from Overview menu, Define Site base stylesheet, Rename Content container and more. Note: Note: Design Studio is provided for beta evaluation with HCL Digital Experience 9.5 Container Update CF200, and includes a sample DX site. It is not yet supported for use in production deployments. See the Design Studio (Beta) Help Center topic for more information.","title":"Design Studio (Beta)"},{"location":"whatsnew/newcf200/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF200, new servies and updates include support for Sidecars for logging of Remote Search, define central logs location, Incubator section for future DX 9.5 Container Update features, Configuration of labels and annotations, also environment variables for different DX resources, Use of Persistent Volumes for DX 9.5 Core, Digital Asset Management, and Persistence services file storage, migration process for the Core profile from Operator to Helm deployment, and Helm based Version to Version Update process. Important: Beginning with HCL DX Container Update CF200, use of the Operator (dxctl) method of container deployment is not supported. Customers should use Helm deployments and migrate existing Operator-based deployments to Helm. See the HCL DX 9.5 Helm deployment topic for more information.","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/newcf200/#digital-asset-management","text":"New Digital Asset Management (DAM) staging support enables administrators to stage and synchronize DAM assets from an authoring environment (source environment/publisher) to multiple rendering environments (target environment/subscriber), using DXClient. See the DAM staging Help Center topic for more information. Support to migrate from the old to new Digital Asset Management database in the Helm-based deployments. See the Migrate to new DAM DB in Helm-based deployments Help Center topic for more information.","title":"Digital Asset Management"},{"location":"whatsnew/newcf200/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include process definitions to automate Export or Import of Web Content Manager libraries from source server to target server location, also ability to generate differential reports for DX Server configurations. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf200/#new-user-and-groups-rest-api-explorer","text":"The remote PUMA SPI gives you access to user profiles through REST services. It provides a remote interface for user and group management for the configured HCL DX user repository. Beginning with HCL DX 9.5 Container Update and CF 199, a new API explorer is available that allows developers using the Portal User Interface APIs to explore and test these APIs. See the Help Center topic Remote REST service for PUMA for additional information.","title":"New User and Groups REST API Explorer"},{"location":"whatsnew/newcf200/#new-personalization-rest-apis-and-explorer","text":"Beginning with HCL DX 9.5 Container Update and CF200 , a new API explorer is available that allows developers using the new Digital Experience Personalization REST APIs to explore and test these APIs. See the Digital Experience Personalization Help Center topic for more information.","title":"New Personalization REST APIs and Explorer"},{"location":"whatsnew/newcf200/#new-search-rest-apis-explorer","text":"The Digital Experience Search REST API provides developers programmatic access to search indexed Digital Experience content and web pages. Beginning with Container and CF Update CF200, a new Digital Experience Search REST API Explorer allows developers to explore and test the Digital Experience Search REST APIs. See the HCL Digital Experience Search REST API Specification Help Center topic for more information.","title":"New Search REST APIs Explorer"},{"location":"whatsnew/newcf200/#access-and-deploy-hcl-digital-experience-95-on-hcl-sofy","text":"HCL SoFy is a next generation software development platform that accelerates deployment and integration of cloud-native products through the application of cloud-centered technologies and practices. Using HCL SoFy to access and deploy HCL Digital Experience 9.5, and other HCL software offerings and demo packs, you can quickly gain hands-on experience working with these cloud-native solutions. See the Deploying HCL Digital Experience 9.5 with HCL Solution Factory (SoFy) Help Center topic for more information.","title":"Access and Deploy HCL Digital Experience 9.5 on HCL Sofy"},{"location":"whatsnew/newcf200/#new-how-to-video","text":"Take advantage of step-by-step guidance for HCL Digital Experience practitioners presented in articles and videos included in HCL Digital Experience Help Center topics. New : Learn how to manage and monitor HCL DX 9.5 Container Deployment Liveliness and Readiness probes. See the Help Center topic: Operations using Helm .","title":"New How-To Video"},{"location":"whatsnew/newcf200/#end-of-support-for-hcl-digital-experience-deprecated-features","text":"The following list of HCL Digital Experience deprecated features will reach end of support beginning with HCL Digital Experience Container update and CF 200. Reference the Help Center topic Deprecated features and themes for HCL Digital Experience 9.5 . Parent topic: Container Update releases 9.5","title":"End of Support for HCL Digital Experience Deprecated Features"},{"location":"whatsnew/newcf201/","text":"What's new in Container Update CF201? This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF201: Deploy HCL DX 9.5 Container Update to container platforms using Helm Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF201, updated capacity requirements are published for Digital Experience components, services, and logging. Options to configure environment values, expanded LDAP configuration guidance, and Prometheus metrics and Grafana visualization dashboard samples are provided for administrators to manage, monitor and optimize their deployments. See the following Help Center topics for more information: HCL DX 9.5 Helm deployment Containerization Requirements and Limitations Additional Helm Tasks Monitor the HCL Digital Experience Container Deployment using Metrics Digital Asset Management New Digital Asset Management (DAM) capability enables developers to use and customize an extensibility plugin, with a customized sample provided use with Digital Asset Management. The DAM Extensibility capability adds support DAM to process user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, while also supporting default and custom renditions. See the Using DAM Extensibility Help Center topic for more information. Script Application Updates New features and updates available for the Script Application include a new configuration task for use to configure Web Content Manager properties used to run Single Page Applications (SPAs) that use React or Angular. Improvements are added to support uploads of minified content for use in production SPAs. An out-of-the-box React profile, \u201cDeferred with React,\u201d is added for use with the Digital Experience 8.5 Theme . See the Script Application Improvements topic in the HCL Digital Experience 9.5 or HCL Digital Experience 8.5 Help center pages, as applicable to your HCL DX deployment, for more information. New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to view and manipulate custom Resource Environment Provider (REP) settings, with examples. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information. New Personalization REST APIs New Personalization REST APIs supporting Personalization Folder operations are available with HCL Digital Experience Container Update and CF201. See the Help Center topic Personalization Folder APIs for more information. Language switcher update Introduced in Container Update CF201, you can disable the language switcher by setting disable.languageSwitcher to true at the root page level in the configuration page. When set to true , the language switcher is hidden from the portal interface. See the Help Center topic Disabling the language switcher for more information. Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy for more information. Parent topic: Container Update releases 9.5","title":"What's new in Container Update CF201?"},{"location":"whatsnew/newcf201/#whats-new-in-container-update-cf201","text":"This HCL Digital Experience 9.5 Container Update and CF201 release includes updated releases of HCL DX core Portal and Web Content Manager, Content Composer, Digital Asset Management, Experience API, and Design Studio (Beta) components. The release also includes updated CICD release process artifacts, new DX Personalization REST APIs, Updated Helm deployment and logging capacity, LDAP, and environment configuration guidance, also metrics and visualization samples, Script Application updates, new React profile for the DX 8.5 sample Theme, extensibility plugin and sample for Digital Asset Management, and more. Go to the HCL Software Support Site and HCL DX Software Fix list for the list of software fixes, including Container Update releases. Product software can be accessed from the HCL Software Licensing Portal . Go to this Step-by-step guide to downloading DX products and accessing Customer Support for more information. The latest Software Requirements and Updates supporting HCL Digital Experience solutions may be accessed from the HCL Support pages, HCL Digital Experience V9.5, V9.0, and V8.5 detailed system requirements topic. The following features and updates are available to customers installing HCL Digital Experience on supported on-premises and container platforms, effective with HCL Digital Experience CF201:","title":"What's new in Container Update CF201?"},{"location":"whatsnew/newcf201/#deploy-hcl-dx-95-container-update-to-container-platforms-using-helm","text":"Beginning with HCL Digital Experience 9.5 Container Update CF196, administrators can deploy HCL DX 9.5 CF196 and later images to supported container platforms using Helm. Using a Helm Chart deployment can provide administrators more transparency and control in deployment operations. Beginning with Container Update CF201, updated capacity requirements are published for Digital Experience components, services, and logging. Options to configure environment values, expanded LDAP configuration guidance, and Prometheus metrics and Grafana visualization dashboard samples are provided for administrators to manage, monitor and optimize their deployments. See the following Help Center topics for more information: HCL DX 9.5 Helm deployment Containerization Requirements and Limitations Additional Helm Tasks Monitor the HCL Digital Experience Container Deployment using Metrics","title":"Deploy HCL DX 9.5 Container Update to container platforms using Helm"},{"location":"whatsnew/newcf201/#digital-asset-management","text":"New Digital Asset Management (DAM) capability enables developers to use and customize an extensibility plugin, with a customized sample provided use with Digital Asset Management. The DAM Extensibility capability adds support DAM to process user-defined custom renditions and transformations for images. This feature can be used to integrate with third-party plug-ins for custom asset processing, for example, to resize, crop, rotate, or other custom operations, while also supporting default and custom renditions. See the Using DAM Extensibility Help Center topic for more information.","title":"Digital Asset Management"},{"location":"whatsnew/newcf201/#script-application-updates","text":"New features and updates available for the Script Application include a new configuration task for use to configure Web Content Manager properties used to run Single Page Applications (SPAs) that use React or Angular. Improvements are added to support uploads of minified content for use in production SPAs. An out-of-the-box React profile, \u201cDeferred with React,\u201d is added for use with the Digital Experience 8.5 Theme . See the Script Application Improvements topic in the HCL Digital Experience 9.5 or HCL Digital Experience 8.5 Help center pages, as applicable to your HCL DX deployment, for more information.","title":"Script Application Updates"},{"location":"whatsnew/newcf201/#new-hcl-digital-experience-95-release-artifacts-supporting-cicd-release-processes","text":"The HCL Digital Experience 9.5 DXClient and DXConnect servlet provides developers and administrators an approach to deploy changes or improvements to the HCL Digital Experience platform, and to automate processes in the development and delivery process. Updates include ability to view and manipulate custom Resource Environment Provider (REP) settings, with examples. See the DXClient and DXConnect tooling supporting CICD release processes topic for more information.","title":"New HCL Digital Experience 9.5 Release Artifacts supporting CICD release processes"},{"location":"whatsnew/newcf201/#new-personalization-rest-apis","text":"New Personalization REST APIs supporting Personalization Folder operations are available with HCL Digital Experience Container Update and CF201. See the Help Center topic Personalization Folder APIs for more information.","title":"New Personalization REST APIs"},{"location":"whatsnew/newcf201/#language-switcher-update","text":"Introduced in Container Update CF201, you can disable the language switcher by setting disable.languageSwitcher to true at the root page level in the configuration page. When set to true , the language switcher is hidden from the portal interface. See the Help Center topic Disabling the language switcher for more information.","title":"Language switcher update"},{"location":"whatsnew/newcf201/#access-the-latest-hcl-digital-experience-95-education-materials-on-hcl-software-academy","text":"The HCL Software Academy offers technical education for the HCL Software portfolio of products, organized by practitioner role. New modules are available for Digital Experience developers and administrators. See the HCL Digital Experience section of the HCL Software Academy for more information. Parent topic: Container Update releases 9.5","title":"Access the latest HCL Digital Experience 9.5 Education Materials on HCL Software Academy"}]}